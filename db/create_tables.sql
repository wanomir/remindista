--
-- PostgreSQL database dump
--

-- Dumped from database version 17.4
-- Dumped by pg_dump version 17.4

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET transaction_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

--
-- Name: data; Type: SCHEMA; Schema: -; Owner: user
--

CREATE SCHEMA data;


ALTER SCHEMA data OWNER TO "user";

SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: reminders; Type: TABLE; Schema: data; Owner: user
--

CREATE TABLE data.reminders (
    id integer NOT NULL,
    user_id integer NOT NULL,
    text text NOT NULL,
    tag character varying(255),
    prompt text,
    frequency interval NOT NULL,
    next_reminder timestamp with time zone NOT NULL,
    is_deleted boolean
);


ALTER TABLE data.reminders OWNER TO "user";

--
-- Name: reminders_id_seq; Type: SEQUENCE; Schema: data; Owner: user
--

CREATE SEQUENCE data.reminders_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE data.reminders_id_seq OWNER TO "user";

--
-- Name: reminders_id_seq; Type: SEQUENCE OWNED BY; Schema: data; Owner: user
--

ALTER SEQUENCE data.reminders_id_seq OWNED BY data.reminders.id;


--
-- Name: users; Type: TABLE; Schema: data; Owner: user
--

CREATE TABLE data.users (
    id integer NOT NULL,
    telegram_id bigint,
    chat_id bigint,
    is_running boolean NOT NULL,
    location character varying(127),
    window_floor time without time zone NOT NULL,
    window_ceil time without time zone NOT NULL,
    is_deleted boolean,
    CONSTRAINT users_check CHECK ((window_ceil > window_floor))
);


ALTER TABLE data.users OWNER TO "user";

--
-- Name: users_id_seq; Type: SEQUENCE; Schema: data; Owner: user
--

CREATE SEQUENCE data.users_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE data.users_id_seq OWNER TO "user";

--
-- Name: users_id_seq; Type: SEQUENCE OWNED BY; Schema: data; Owner: user
--

ALTER SEQUENCE data.users_id_seq OWNED BY data.users.id;


--
-- Name: goose_db_version; Type: TABLE; Schema: public; Owner: user
--

CREATE TABLE public.goose_db_version (
    id integer NOT NULL,
    version_id bigint NOT NULL,
    is_applied boolean NOT NULL,
    tstamp timestamp without time zone DEFAULT now()
);


ALTER TABLE public.goose_db_version OWNER TO "user";

--
-- Name: goose_db_version_id_seq; Type: SEQUENCE; Schema: public; Owner: user
--

CREATE SEQUENCE public.goose_db_version_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.goose_db_version_id_seq OWNER TO "user";

--
-- Name: goose_db_version_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: user
--

ALTER SEQUENCE public.goose_db_version_id_seq OWNED BY public.goose_db_version.id;


--
-- Name: reminders id; Type: DEFAULT; Schema: data; Owner: user
--

ALTER TABLE ONLY data.reminders ALTER COLUMN id SET DEFAULT nextval('data.reminders_id_seq'::regclass);


--
-- Name: users id; Type: DEFAULT; Schema: data; Owner: user
--

ALTER TABLE ONLY data.users ALTER COLUMN id SET DEFAULT nextval('data.users_id_seq'::regclass);


--
-- Name: goose_db_version id; Type: DEFAULT; Schema: public; Owner: user
--

ALTER TABLE ONLY public.goose_db_version ALTER COLUMN id SET DEFAULT nextval('public.goose_db_version_id_seq'::regclass);


--
-- Data for Name: reminders; Type: TABLE DATA; Schema: data; Owner: user
--

COPY data.reminders (id, user_id, text, tag, prompt, frequency, next_reminder, is_deleted) FROM stdin;
498	11	–ß—Ç–æ —Ç–∞–∫–æ–µ –æ—Å?	#–æ—Å	–û—Å	00:30:00	2025-04-18 16:08:10.3713+00	t
496	10	The color of your thoughts	#quotes	The things you think about determine the quality of your mind. Your soul takes on the color of your thoughts.	96:00:00	2025-05-02 16:22:40.709856+00	f
488	10	10	#majorsystem	daisy üåº	48:00:00	2025-04-28 03:43:28.741972+00	f
474	10	Major System mapping	#majorsystem	0 ‚Äî> s, z\n1 ‚Äî> t, d, th\n2 ‚Äî> n\n3 ‚Äî> m\n4 ‚Äî> r\n5 ‚Äî> l\n6 ‚Äî> j, ch, sh\n7 ‚Äî> c, k, g, q, ck\n8 ‚Äî> v, f, ph\n9 ‚Äî> p, b	192:00:00	2025-05-02 15:39:59.593735+00	f
5	10	Quantity is a quality of its own.	#waytosuccess	Familiarity with lots of examples can be of fundamental value for building proficiency.	384:00:00	2025-05-07 10:03:19.863225+00	f
469	10	Restfully pay attention, concentrate, and focus in a relaxed manner.	#mindfulness	When doing that, you automatically activate the frontal lobe, which reduces synaptic firing in the rest of the neocortex.	96:00:00	2025-04-27 18:39:56.634029+00	f
502	11	1. –ö–∞–∫ –∫–æ–º–ø—å—é—Ç–µ—Ä —Ö—Ä–∞–Ω–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é?	#—Ç–µ–æ—Ä–∏—è–±–¥	–≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–∞—Ö –ø–∞–º—è—Ç–∏\n     –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å (RAM - Random Access Memory)\n     –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∂–µ—Å—Ç–∫–∏–π –¥–∏—Å–∫, SSD, —Ñ–ª–µ—à-–ø–∞–º—è—Ç—å)	16:00:00	2025-04-27 15:19:43.366595+00	t
462	10	Minimum Absolute Difference in BST\n\nGiven the root of a Binary Search Tree (BST), return the minimum absolute difference between the values of any two different nodes in the tree.	#leetcode	func getMinimumDifference(root *TreeNode) int {\n  var traverse func(node *TreeNode)\n  minDiff, x := 1<<32, 1<<32\n\n  absDiff := func(a, b int) int {\n    if a < b {\n      return b - a\n    }\n    return a - b\n  }\n\n  traverse = func(node *TreeNode) {\n    if node == nil {\n      return\n    }\n    traverse(node.Left)\n    minDiff = min(minDiff, absDiff(node.Val, x))\n    x = node.Val\n    traverse(node.Right)\n  }\n\n  traverse(root)\n\n  return minDiff\n}	192:00:00	2025-05-04 10:45:22.24987+00	f
497	10	voluntarily cease to keep or claim; give up	#words	to relinquish	48:00:00	2025-04-28 15:14:15.42645+00	f
499	10	deep respect for someone or something	#words	reverence	24:00:00	2025-04-27 11:02:04.86884+00	f
487	10	Clone Graph\n\nGiven a reference of a node in a connected undirected graph, return a deep copy (clone) of the graph. Each node in the graph contains a value (int) and a list (List[Node]) of its neighbors.\n\nclass Node {\n    public int val;\n    public List<Node> neighbors;\n}	#leetcode	func cloneGraph(node *Node) *Node {\n  if node == nil {\n    return nil\n  }\n\n  visited := make(map[*Node]*Node)\n  queue := []*Node{node}\n  visited[node] = &Node{Val: node.Val, Neighbors: make([]*Node, 0, 2)}\n\n  for len(queue) > 0 {\n    curr := queue[0]\n    queue = queue[1:]\n        \n    for _, neighbor := range curr.Neighbors {\n      if _, ok := visited[neighbor]; !ok {\n        visited[neighbor] = &Node{Val: neighbor.Val, Neighbors: make([]*Node, 0, 2)}\n        queue = append(queue, neighbor)\n      }\n      visited[curr].Neighbors = append(visited[curr].Neighbors, visited[neighbor])\n    }\n  }\n\n  return visited[node]\n}	48:00:00	2025-04-28 11:01:22.504853+00	f
31	10	proprietary	#words	private, personal	72:00:00	2025-02-14 15:17:27.957805+00	t
489	10	11	#majorsystem	death	48:00:00	2025-04-29 07:26:59.485691+00	f
433	10	Populating Next Right Pointers in Each Node II\n\nGiven a binary tree\nstruct Node {\n  int val;\n  Node *left;\n  Node *right;\n  Node *next;\n}\n\nPopulate each next pointer to point to its next right node. If there is no next right node, the next pointer should be set to NULL.\n\nInitially, all next pointers are set to NULL.	#leetcode	### Stack\n\\**beats 100% Runtime*\n/**\n * Definition for a Node.\n * type Node struct {\n *     Val int\n *     Left *Node\n *     Right *Node\n *     Next *Node\n * }\n */\n\nfunc connect(root *Node) *Node {\n  if root == nil {\n    return nil\n  }\n\n  level := []*Node{root}\n\n  for len(level) > 0 {\n    nextLevel := make([]*Node, 0, len(level)*2)\n\n    for i, node := range level {\n      if node.Left != nil {\n        nextLevel = append(nextLevel, node.Left)\n      }\n\n      if node.Right != nil {\n        nextLevel = append(nextLevel, node.Right)\n      }\n\n      if i < len(level)-1 {\n        node.Next = level[i+1]\n      } else {\n        node.Next = nil\n      }\n    }\n\n    level = nextLevel\n  }\n\n  return root\n}\n### Tail\n*beats 80% memory*\nfunc connect(root *Node) *Node {\n  curr := root\n\n  for curr != nil {\n    dummy := &Node{}\n    tail := dummy\n\n    for node := curr; node != nil; node = node.Next {\n      if node.Left != nil {\n        tail.Next = node.Left\n        tail = tail.Next\n      }\n\n      if node.Right != nil {\n        tail.Next = node.Right\n        tail = tail.Next\n      }\n    }\n\n    curr = dummy.Next\n  }\n\n  return root\n}	48:00:00	2025-04-27 03:13:52.358249+00	f
503	11	1. DDL (Data Definition Language - –Ø–∑—ã–∫ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –î–∞–Ω–Ω—ã—Ö):\n    - –û–ø–µ—Ä–∞—Ç–æ—Ä—ã DDL –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∏—è, –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏ —É–¥–∞–ª–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:\n        - CREATE: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—ã, –∏–Ω–¥–µ–∫—Ås, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.\n        - ALTER: –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–ª–∏ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤).\n        - DROP: –£–¥–∞–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—ã, –∏–Ω–¥–µ–∫—Å—ã –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.\n            \n2. DML (Data Manipulation Language - –Ø–∑—ã–∫ –ú–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –î–∞–Ω–Ω—ã–º–∏):\n    - –û–ø–µ—Ä–∞—Ç–æ—Ä—ã DML –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π –Ω–∞–¥ –¥–∞–Ω–Ω—ã–º –≤–Ω—É—Ç—Ä–∏ —Ç–∞–±–ª–∏—Ü –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö. –û–Ω–∏ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:\n        - SELECT: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–∞–±–ª–∏—Ü—ã.\n        - INSERT: –í—Å—Ç–∞–≤–∫–∞ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü—É.\n        - UPDATE: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü–µ.\n        - DELETE: –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –∏–∑ —Ç–∞–±–ª–∏—Ü—ã.\n\n3. TCL (Transaction Control Language - –Ø–∑—ã–∫ –£–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏):\n    - –û–ø–µ—Ä–∞—Ç–æ—Ä—ã TCL –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏, –∫–æ—Ç–æ—Ä—ã –ø–æ–∑–≤–æ–ª—è—é—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –≥—Ä—É–ø–ø—ã –æ–ø–µ—Ä–∞—Ü–∏–π –∫–∞–∫ –µ–¥–∏–Ω–æ–µ —Ü–µ–ª–æ–µ. –û–Ω–∏ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:\n       - COMMIT: –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π.\n       - ROLLBACK: –û—Ç–∫–∞—Ç —Ç–µ–∫—É—â–µ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∏ –æ—Ç–º–µ–Ω–∞ –≤—Å–µ—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π.\n       - SAVEPOINT: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—á–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—É—â–µ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ—Ç–∫–∞—Ç–∞ –¥–æ —ç—Ç–æ–π —Ç–æ—á–∫–∏.\n       - SET TRANSACTION: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∏–∑–æ–ª—è—Ü–∏—è –∏ —É—Ä–æ–≤–µ–Ω—å –∏–∑–æ–ª—è—Ü–∏–∏.\n            \n4. DCL (Data Control Language - –Ø–∑—ã–∫ –ö–æ–Ω—Ç—Ä–æ–ª—è –î–æ—Å—Ç—É–ø–∞ –∫ –î–∞–Ω–Ω—ã–º):\n    - –û–ø–µ—Ä–∞—Ç–æ—Ä—ã DCL –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∞–≤–∞–º–∏ –¥–æ—Å—Ç—É–ø–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö. –û–Ω–∏ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:\n        - GRANT: –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞ –∫ –æ–±—ä–µ–∫—Ç–∞–º –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥—Ä—É–≥–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –∏–ª–∏ —Ä–æ–ª—è–º.\n        - REVOKE: –û—Ç–∑—ã–≤ –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ä–∞–Ω–µ–µ —á–µ—Ä–µ–∑ –æ–ø–µ—Ä–∞—Ü–∏—é GRANT.\n\n5. –û–ø–µ—Ä–∞—Ç–æ—Ä—ã —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏:\n    - WHERE: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Å–ª–æ–≤–∏–π.\n    - ORDER BY: –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Å—Ç–æ–ª–±—Ü–∞–º.\n    - GROUP BY: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –æ–¥–Ω–æ–º—É –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Å—Ç–æ–ª–±—Ü–∞–º.\n    - HAVING: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏.\n\n6. –§—É–Ω–∫—Ü–∏–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏:\n    - COUNT(): –ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫.\n    - SUM(): –°—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.\n    - AVG(): –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\n    - MAX(): –ü–æ–∏—Å–∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\n    - MIN(): –ü–æ–∏—Å–∫ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\n\n7. –°–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü:\n    - INNER JOIN: –í–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–≤—É—Ö —Ç–∞–±–ª–∏—Ü –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π.\n    - LEFT JOIN: –õ–µ–≤–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, –≤–∫–ª—é—á–∞—è –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –ø–µ—Ä–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –≤—Ç–æ—Ä–æ–π —Ç–∞–±–ª–∏—Ü—ã.\n    - RIGHT JOIN: –ü—Ä–∞–≤–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, –≤–∫–ª—é—á–∞—è –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –≤—Ç–æ—Ä–æ–π —Ç–∞–±–ª–∏—Ü—ã –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –ø–µ—Ä–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã.\n    - FULL OUTER JOIN: –ü–æ–ª–Ω–æ–µ –≤–Ω–µ—à–Ω–µ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, –≤–∫–ª—é—á–∞—è –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –æ–±–µ–∏—Ö —Ç–∞–±–ª–∏—Ü.\n\n8. –ü–æ–¥–∑–∞–ø—Ä–æ—Å—ã:\n    - –í–ª–æ–∂–µ–Ω–Ω—ã–µ SELECT –∑–∞–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –≤–Ω—É—Ç—Ä–∏ –¥—Ä—É–≥–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.\n\n9. –ò–Ω–¥–µ–∫—Å—ã:\n    - –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤.\n\n10. –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏:\n    - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≥—Ä—É–ø–ø—ã –æ–ø–µ—Ä–∞—Ü–∏–π –ª–∏–±–æ –≤—Å–µ –æ–Ω–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —É—Å–ø–µ—à–Ω–æ, –ª–∏–±–æ –Ω–∏ –æ–¥–Ω–∞.	#sql	1. DDL (Data Definition Language - –Ø–∑—ã–∫ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –î–∞–Ω–Ω—ã—Ö):\n    - –û–ø–µ—Ä–∞—Ç–æ—Ä—ã DDL –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∏—è, –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏ —É–¥–∞–ª–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:\n        - CREATE: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—ã, –∏–Ω–¥–µ–∫—Ås, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.\n        - ALTER: –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–ª–∏ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤).\n        - DROP: –£–¥–∞–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—ã, –∏–Ω–¥–µ–∫—Å—ã –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.\n            \n2. DML (Data Manipulation Language - –Ø–∑—ã–∫ –ú–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –î–∞–Ω–Ω—ã–º–∏):\n    - –û–ø–µ—Ä–∞—Ç–æ—Ä—ã DML –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π –Ω–∞–¥ –¥–∞–Ω–Ω—ã–º –≤–Ω—É—Ç—Ä–∏ —Ç–∞–±–ª–∏—Ü –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö. –û–Ω–∏ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:\n        - SELECT: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–∞–±–ª–∏—Ü—ã.\n        - INSERT: –í—Å—Ç–∞–≤–∫–∞ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü—É.\n        - UPDATE: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü–µ.\n        - DELETE: –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –∏–∑ —Ç–∞–±–ª–∏—Ü—ã.\n\n3. TCL (Transaction Control Language - –Ø–∑—ã–∫ –£–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏):\n    - –û–ø–µ—Ä–∞—Ç–æ—Ä—ã TCL –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏, –∫–æ—Ç–æ—Ä—ã –ø–æ–∑–≤–æ–ª—è—é—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –≥—Ä—É–ø–ø—ã –æ–ø–µ—Ä–∞—Ü–∏–π –∫–∞–∫ –µ–¥–∏–Ω–æ–µ —Ü–µ–ª–æ–µ. –û–Ω–∏ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:\n       - COMMIT: –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π.\n       - ROLLBACK: –û—Ç–∫–∞—Ç —Ç–µ–∫—É—â–µ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∏ –æ—Ç–º–µ–Ω–∞ –≤—Å–µ—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π.\n       - SAVEPOINT: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—á–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—É—â–µ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ—Ç–∫–∞—Ç–∞ –¥–æ —ç—Ç–æ–π —Ç–æ—á–∫–∏.\n       - SET TRANSACTION: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∏–∑–æ–ª—è—Ü–∏—è –∏ —É—Ä–æ–≤–µ–Ω—å –∏–∑–æ–ª—è—Ü–∏–∏.\n            \n4. DCL (Data Control Language - –Ø–∑—ã–∫ –ö–æ–Ω—Ç—Ä–æ–ª—è –î–æ—Å—Ç—É–ø–∞ –∫ –î–∞–Ω–Ω—ã–º):\n    - –û–ø–µ—Ä–∞—Ç–æ—Ä—ã DCL –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∞–≤–∞–º–∏ –¥–æ—Å—Ç—É–ø–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö. –û–Ω–∏ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:\n        - GRANT: –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞ –∫ –æ–±—ä–µ–∫—Ç–∞–º –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥—Ä—É–≥–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –∏–ª–∏ —Ä–æ–ª—è–º.\n        - REVOKE: –û—Ç–∑—ã–≤ –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ä–∞–Ω–µ–µ —á–µ—Ä–µ–∑ –æ–ø–µ—Ä–∞—Ü–∏—é GRANT.\n\n5. –û–ø–µ—Ä–∞—Ç–æ—Ä—ã —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏:\n    - WHERE: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Å–ª–æ–≤–∏–π.\n    - ORDER BY: –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Å—Ç–æ–ª–±—Ü–∞–º.\n    - GROUP BY: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –æ–¥–Ω–æ–º—É –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Å—Ç–æ–ª–±—Ü–∞–º.\n    - HAVING: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏.\n\n6. –§—É–Ω–∫—Ü–∏–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏:\n    - COUNT(): –ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫.\n    - SUM(): –°—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.\n    - AVG(): –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\n    - MAX(): –ü–æ–∏—Å–∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\n    - MIN(): –ü–æ–∏—Å–∫ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\n\n7. –°–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü:\n    - INNER JOIN: –í–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–≤—É—Ö —Ç–∞–±–ª–∏—Ü –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π.\n    - LEFT JOIN: –õ–µ–≤–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, –≤–∫–ª—é—á–∞—è –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –ø–µ—Ä–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –≤—Ç–æ—Ä–æ–π —Ç–∞–±–ª–∏—Ü—ã.\n    - RIGHT JOIN: –ü—Ä–∞–≤–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, –≤–∫–ª—é—á–∞—è –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –≤—Ç–æ—Ä–æ–π —Ç–∞–±–ª–∏—Ü—ã –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –ø–µ—Ä–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã.\n    - FULL OUTER JOIN: –ü–æ–ª–Ω–æ–µ –≤–Ω–µ—à–Ω–µ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, –≤–∫–ª—é—á–∞—è –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –æ–±–µ–∏—Ö —Ç–∞–±–ª–∏—Ü.\n\n8. –ü–æ–¥–∑–∞–ø—Ä–æ—Å—ã:\n    - –í–ª–æ–∂–µ–Ω–Ω—ã–µ SELECT –∑–∞–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –≤–Ω—É—Ç—Ä–∏ –¥—Ä—É–≥–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.\n\n9. –ò–Ω–¥–µ–∫—Å—ã:\n    - –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤.\n\n10. –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏:\n    - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≥—Ä—É–ø–ø—ã –æ–ø–µ—Ä–∞—Ü–∏–π –ª–∏–±–æ –≤—Å–µ –æ–Ω–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —É—Å–ø–µ—à–Ω–æ, –ª–∏–±–æ –Ω–∏ –æ–¥–Ω–∞.	16:00:00	2025-04-27 10:31:12.991518+00	f
254	10	TCL operators	#sql	_Transaction Control Language_\n\n- COMMIT:  \n  Saves all changes made during the current transaction. A transaction is a sequence of SQL statements that are treated as a single unit of work. If any statement in the transaction fails, the entire transaction is rolled back.\n        COMMIT;\n    \n\n- ROLLBACK:  \n  Reverts all changes made during the current transaction. This is used to undo changes made in case of an error or other issue.\n        ROLLBACK;\n    \n\n- SAVEPOINT:  \n  Creates a point within a transaction to which you can roll back. This allows you to undo only part of a transaction.\n        SAVEPOINT my_savepoint;\n    -- ... some SQL statements ...\n    ROLLBACK TO my_savepoint;	192:00:00	2025-04-23 04:29:13.374724+00	t
403	10	Maximum Depth of a Binary Tree\n\nGiven the root of a binary tree, return its maximum depth.\n\nA binary tree's maximum depth is the number of nodes along the longest path from the root node down to the farthest leaf node.	#leetcode	/**\n * type TreeNode struct {\n *     Val int\n *     Left *TreeNode\n *     Right *TreeNode\n * }\n */\nfunc maxDepth(root *TreeNode) int {\n    if root == nil {\n        return 0\n    }\n\n    return max(maxDepth(root.Left)+1, maxDepth(root.Right)+1)\n}	192:00:00	2025-04-27 07:35:01.692703+00	f
490	10	12	#majorsystem	Houdini	48:00:00	2025-04-29 11:27:58.646273+00	f
215	10	Isomorphic Strings\n\nGiven two strings s and t, determine if they are isomorphic.\n\nTwo strings s and t are isomorphic if the characters in s can be replaced to get t.\n\nAll occurrences of a character must be replaced with another character while preserving the order of characters. No two characters may map to the same character, but a character may map to itself.	#leetcode	func isIsomorphic(s string, t string) bool {\n    if len(s) != len(t) {\n        return false\n    } \n\n    pairs := make(map[byte]byte, len(s))\n    paired := make(map[byte]struct{}, len(s))\n\n    for i := 0; i < len(s); i++ {\n        a, b := s[i], t[i]\n\n        if c, ok := pairs[a]; ok {\n            if c != b {\n                return false\n            }\n            continue\n        }\n\n        if _, ok := paired[b]; ok {\n            return false\n        }\n\n        pairs[a] = b\n        paired[b] = struct{}{}\n    }\n\n    return true\n}	192:00:00	2025-05-09 03:46:32.705898+00	f
504	11	12			16:00:00	2025-04-27 05:32:07.399371+00	t
500	10	adjective; making a certain situation or outcome likely or possible	#words	conducive (to); \n"working conditions not conducive to productivity."	24:00:00	2025-04-28 04:52:51.167344+00	f
287	10	TRUNCATE vs DELETE	#sql	TRUNCATE generally runs much faster than DELETE. This is because TRUNCATE removes all rows from a table in one go and releases the space used for data storage without logging each deletion. This streamlined approach makes it both speedy and less resource-intensive. In contrast, DELETE processes rows individually, which can be considerably slower‚Äîespecially when working with large datasets.\n\n2. Transaction Logging:  \n   TRUNCATE doesn‚Äôt record each removed row in the transaction log, meaning its actions cannot be undone. DELETE, however, logs every deletion, allowing you to roll back the operation if needed.\n\n3. Data Handling:   \n   TRUNCATE clears an entire table from the first row onward and cannot be used to remove specific rows based on conditions. It also resets identity counters (such as AUTO_INCREMENT values). DELETE, on the other hand, lets you target and remove specific rows using conditional criteria, and it preserves identity values.\n\n4. Deletion Process:   \n   TRUNCATE typically involves dropping and recreating the table (in many database systems), which can reset counters and indexes as a side effect. DELETE offers greater flexibility by supporting complex conditional deletions, making it suitable for removing only a subset of data.\n\nThe choice between TRUNCATE and DELETE depends on your specific needs. If your goal is to quickly clear all data from a table and free up space, TRUNCATE is more efficient. But if you need to selectively delete data or require the ability to reverse changes, DELETE is the better option.	384:00:00	2025-04-28 14:23:57.317143+00	t
464	10	relating to a person‚Äôs body; especially as opposed to their spirit	#words	corporeal	384:00:00	2025-05-09 17:05:34.71231+00	f
491	10	13	#majorsystem	Tom (from Tom and Jerry)	96:00:00	2025-04-28 17:57:45.402515+00	f
501	10	(of a young person) tending to commit a (minor) crime; failing in one's duty	#words	delinquent	24:00:00	2025-04-27 13:04:34.12056+00	f
505	11	28. –ß—Ç–æ —Ç–∞–∫–æ–µ DDL, DML, TCL, DCL? –ö–∞–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ –Ω–∏—Ö –≤—Ö–æ–¥—è—Ç? –†–∞—Å—Å–∫–∞–∑–∞—Ç—å –ø—Ä–æ –Ω–∏—Ö.	#sql	1. DDL (Data Definition Language - –Ø–∑—ã–∫ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –î–∞–Ω–Ω—ã—Ö):\n    - –û–ø–µ—Ä–∞—Ç–æ—Ä—ã DDL –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∏—è, –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏ —É–¥–∞–ª–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:\n        - CREATE: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—ã, –∏–Ω–¥–µ–∫—Ås, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.\n        - ALTER: –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–ª–∏ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤).\n        - DROP: –£–¥–∞–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—ã, –∏–Ω–¥–µ–∫—Å—ã –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.\n            \n2. DML (Data Manipulation Language - –Ø–∑—ã–∫ –ú–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –î–∞–Ω–Ω—ã–º–∏):\n    - –û–ø–µ—Ä–∞—Ç–æ—Ä—ã DML –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π –Ω–∞–¥ –¥–∞–Ω–Ω—ã–º –≤–Ω—É—Ç—Ä–∏ —Ç–∞–±–ª–∏—Ü –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö. –û–Ω–∏ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:\n        - SELECT: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–∞–±–ª–∏—Ü—ã.\n        - INSERT: –í—Å—Ç–∞–≤–∫–∞ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü—É.\n        - UPDATE: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü–µ.\n        - DELETE: –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –∏–∑ —Ç–∞–±–ª–∏—Ü—ã.\n\n3. TCL (Transaction Control Language - –Ø–∑—ã–∫ –£–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏):\n    - –û–ø–µ—Ä–∞—Ç–æ—Ä—ã TCL –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏, –∫–æ—Ç–æ—Ä—ã –ø–æ–∑–≤–æ–ª—è—é—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –≥—Ä—É–ø–ø—ã –æ–ø–µ—Ä–∞—Ü–∏–π –∫–∞–∫ –µ–¥–∏–Ω–æ–µ —Ü–µ–ª–æ–µ. –û–Ω–∏ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:\n       - COMMIT: –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π.\n       - ROLLBACK: –û—Ç–∫–∞—Ç —Ç–µ–∫—É—â–µ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∏ –æ—Ç–º–µ–Ω–∞ –≤—Å–µ—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π.\n       - SAVEPOINT: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ—á–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—É—â–µ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ—Ç–∫–∞—Ç–∞ –¥–æ —ç—Ç–æ–π —Ç–æ—á–∫–∏.\n       - SET TRANSACTION: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∏–∑–æ–ª—è—Ü–∏—è –∏ —É—Ä–æ–≤–µ–Ω—å –∏–∑–æ–ª—è—Ü–∏–∏.\n            \n4. DCL (Data Control Language - –Ø–∑—ã–∫ –ö–æ–Ω—Ç—Ä–æ–ª—è –î–æ—Å—Ç—É–ø–∞ –∫ –î–∞–Ω–Ω—ã–º):\n    - –û–ø–µ—Ä–∞—Ç–æ—Ä—ã DCL –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∞–≤–∞–º–∏ –¥–æ—Å—Ç—É–ø–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö. –û–Ω–∏ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:\n        - GRANT: –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞ –∫ –æ–±—ä–µ–∫—Ç–∞–º –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥—Ä—É–≥–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –∏–ª–∏ —Ä–æ–ª—è–º.\n        - REVOKE: –û—Ç–∑—ã–≤ –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ä–∞–Ω–µ–µ —á–µ—Ä–µ–∑ –æ–ø–µ—Ä–∞—Ü–∏—é GRANT.\n\n5. –û–ø–µ—Ä–∞—Ç–æ—Ä—ã —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏:\n    - WHERE: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Å–ª–æ–≤–∏–π.\n    - ORDER BY: –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Å—Ç–æ–ª–±—Ü–∞–º.\n    - GROUP BY: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –æ–¥–Ω–æ–º—É –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Å—Ç–æ–ª–±—Ü–∞–º.\n    - HAVING: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏.\n\n6. –§—É–Ω–∫—Ü–∏–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏:\n    - COUNT(): –ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫.\n    - SUM(): –°—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.\n    - AVG(): –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\n    - MAX(): –ü–æ–∏—Å–∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\n    - MIN(): –ü–æ–∏—Å–∫ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.\n\n7. –°–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü:\n    - INNER JOIN: –í–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–≤—É—Ö —Ç–∞–±–ª–∏—Ü –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π.\n    - LEFT JOIN: –õ–µ–≤–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, –≤–∫–ª—é—á–∞—è –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –ø–µ—Ä–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –≤—Ç–æ—Ä–æ–π —Ç–∞–±–ª–∏—Ü—ã.\n    - RIGHT JOIN: –ü—Ä–∞–≤–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, –≤–∫–ª—é—á–∞—è –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –≤—Ç–æ—Ä–æ–π —Ç–∞–±–ª–∏—Ü—ã –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –ø–µ—Ä–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã.\n    - FULL OUTER JOIN: –ü–æ–ª–Ω–æ–µ –≤–Ω–µ—à–Ω–µ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, –≤–∫–ª—é—á–∞—è –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –æ–±–µ–∏—Ö —Ç–∞–±–ª–∏—Ü.\n\n8. –ü–æ–¥–∑–∞–ø—Ä–æ—Å—ã:\n    - –í–ª–æ–∂–µ–Ω–Ω—ã–µ SELECT –∑–∞–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –≤–Ω—É—Ç—Ä–∏ –¥—Ä—É–≥–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.\n\n9. –ò–Ω–¥–µ–∫—Å—ã:\n    - –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤.\n\n10. –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏:\n    - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≥—Ä—É–ø–ø—ã –æ–ø–µ—Ä–∞—Ü–∏–π –ª–∏–±–æ –≤—Å–µ –æ–Ω–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —É—Å–ø–µ—à–Ω–æ, –ª–∏–±–æ –Ω–∏ –æ–¥–Ω–∞.	16:00:00	2025-04-27 06:30:29.661396+00	f
395	10	Security issues: client-server data transmission	#code	When data is exchanged between a client and a server, several potential security risks may arise‚Äîthreats that can be exploited by attackers, malicious software, or other adversaries. Here are some of the most common security challenges and recommended solutions:\n\n1. Eavesdropping  \n    _Issue:_ If transmitted data isn‚Äôt properly secured, an attacker may intercept and listen in on the information being exchanged.  \n    _Solution:_ Protect your data with robust encryption. Using HTTPS with SSL/TLS is the industry standard for secure Internet communications.\n    \n2. Server Impersonation  \n    _Issue:_ An attacker might pose as a legitimate server to trick clients into sending sensitive data.  \n    _Solution:_ Implement strong server authentication, such as using digital certificates or other verification methods, to confirm the server‚Äôs identity.\n    \n3. Client Impersonation  \n    _Issue:_ Similarly, an attacker could mimic a client to gain unauthorized access to server resources.  \n    _Solution:_ Enforce strict client authentication to ensure that only authorized users can access the system.\n    \n4. Cross-Site Scripting (XSS)  \n    _Issue:_ XSS attacks enable attackers to inject malicious scripts into client-side applications, potentially allowing them to access data belonging to other users.  \n    _Solution:_ Always filter and properly escape any data that is sent to the client. Additionally, implement security headers like Content Security Policy (CSP) to further mitigate risks.\n    \n5. Cross-Site Request Forgery (CSRF)  \n    _Issue:_ In a CSRF attack, a legitimate, authenticated user can be tricked into performing unwanted actions on a website without their consent.  \n    _Solution:_ Utilize CSRF tokens and verify them on the server side to ensure that incoming requests are authentic and intentional.\n    \n6. Data Leakage  \n    _Issue:_ Inadequate error handling or poor access control practices can lead to accidental exposure of confidential information.  \n    _Solution:_ Ensure errors are handled gracefully and implement strict access controls to prevent sensitive data from being disclosed.\n    \n7. Code Injection Attacks  \n    _Issue:_ Improper handling of external inputs can leave your system vulnerable to code injection attacks, including SQL injection and command injection.  \n    _Solution:_ Use parameterized queries, along with proper input filtering and data sanitization, to block these types of attacks.\n    \n8. Denial of Service (DoS)  \n    _Issue:_ DoS attacks aim to overwhelm your server with excessive requests, rendering it unavailable to legitimate users.  \n    _Solution:_ Deploy protective measures like rate limiting, traffic throttling, and load balancing to help mitigate the impact of such attacks.\n    \n9. Unprotected Sessions and Tokens  \n    _Issue:_ If session data and authentication tokens are not securely managed, they can be stolen or forged by attackers.  \n    _Solution:_ Store sessions and tokens securely, enforce the use of HTTPS, and refresh these credentials regularly to enhance security.\n    \n10. Lack of Regular Patching  \n    _Issue:_ Failing to apply security patches and updates can leave your systems exposed to known vulnerabilities.  \n    _Solution:_ Keep both server and client software‚Äîalong with any libraries or dependencies‚Äîup to date to safeguard against emerging threats.	96:00:00	2025-03-14 15:02:53.332364+00	t
492	10	14	#majorsystem	Tor	192:00:00	2025-05-01 11:27:18.560903+00	f
301	10	Insert Interval\n\nYou are given an array of non-overlapping intervals intervals where intervals[i] = [starti, endi] represent the start and the end of the i-th interval and intervals is sorted in ascending order by starti. You are also given an interval newInterval = [start, end] that represents the start and end of another interval.\n\nInsert newInterval into intervals such that intervals is still sorted in ascending order by starti and intervals still does not have any overlapping intervals (merge overlapping intervals if necessary).\n\nReturn intervals after the insertion.\n\nNote that you don't need to modify intervals in-place. You can make a new array and return it.	#leetcode	func insert(ivs [][]int, niv []int) [][]int {\n    result := make([][]int, 0, len(ivs))\n    i := 0\n\n    for ;i < len(ivs); i++ {\n        if ivs[i][1] >= niv[0] {\n            break\n        }\n        result = append(result, ivs[i])\n    }\n\n    for ;i < len(ivs); i++ {\n        if ivs[i][0] > niv[1] {\n            break  \n        }\n        niv[0] = min(niv[0], ivs[i][0])\n        niv[1] = max(niv[1], ivs[i][1])\n    }\n\n    result = append(result, niv)\n\n    for ;i < len(ivs); i++ {\n        result = append(result, ivs[i])\n    }\n\n    return result\n}	192:00:00	2025-04-30 20:48:53.669654+00	f
319	10	Reverse Polish Notation\n\nYou are given an array of strings tokens that represents an arithmetic expression in a Reverse Polish Notation.\n\nEvaluate the expression. Return an integer that represents the value of the expression.	#leetcode	/*\n    Push all numbers to stack.\n    When encountering an operator pop last 2 from stack and perform operation.\n    Push result of that operation back to stack and continue.\n    Return number at top of stack when done.\n*/\nfunc evalRPN(tokens []string) int {\n    stack := make([]int, 0, len(tokens)/2) \n\n    for _, tkn := range tokens {\n        n := len(stack)\n\n        switch tkn {\n        case "+":\n            sum := stack[n-2] + stack[n-1]\n            stack = append(stack[:n-2], sum)\n        case "-":\n            diff := stack[n-2]  - stack[n-1]\n            stack = append(stack[:n-2], diff)\n        case "*":\n            prod := stack[n-2] * stack[n-1]\n            stack = append(stack[:n-2], prod)\n        case "/":\n            quot := stack[n-2] / stack[n-1]\n            stack = append(stack[:n-2], quot)\n        default:\n            number, _ := strconv.Atoi(tkn)\n            stack = append(stack, number)\n        }\n    }\n\n    return stack[0]\n}	192:00:00	2025-04-30 18:57:17.096828+00	f
465	10	–æ—Ç—Å–µ–∫, –∫–∞–º–µ—Ä–∞, –æ—Ç–¥–µ–ª–µ–Ω–∏–µ	#words	compartment	192:00:00	2025-05-03 14:37:23.135014+00	f
475	10	00	#majorsystem	seesaw	96:00:00	2025-04-29 10:58:43.158124+00	f
493	10	15	#majorsystem	Towel (from South Park)	48:00:00	2025-04-29 10:36:44.782382+00	f
430	5	–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã SQL	#sql	CREATE TABLE book(\n    book_id INT PRIMARY KEY AUTO_INCREMENT, \n    name_book VARCHAR(30)\n);	64:00:00	2025-05-01 14:11:53.563998+00	f
506	11	4. –ó–∞—á–µ–º –Ω—É–∂–Ω—ã –ë–î? –ü–æ—á–µ–º—É –±—ã –Ω–µ —Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ –≤ –æ–±—ã—á–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö?	#—Ç–µ–æ—Ä–∏—è–±–¥	1. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö: \n    \n    –ë–î –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å—Ä–µ–¥—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö. –û–Ω–∏ *—Ä–∞–∑–¥–µ–ª—è—é—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Ç–∞–±–ª–∏—Ü—ã, –∑–∞–ø–∏—Å–∏ –∏ –ø–æ–ª—è*, —á—Ç–æ *–æ–±–ª–µ–≥—á–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏*.\n    \n2. –ë—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø –∫ –¥–∞–Ω–Ω—ã–º: \n    \n    –ë–î –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç *–º–µ—Ö–∞–Ω–∏–∑–º—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤*, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç *–±—ã—Å—Ç—Ä–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏*.\n    \n3. –ú–Ω–æ–≥–æ–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –¥–æ—Å—Ç—É–ø: \n    \n    –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ë–î –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –ø–æ–¥–¥–µ—Ä–∂–∫—É –º–Ω–æ–≥–æ–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞, —á—Ç–æ *–ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —á–∏—Ç–∞—Ç—å –∏ –∏–∑–º–µ–Ω—è—Ç—å –¥–∞–Ω–Ω—ã–µ.*\n    \n4. –¶–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö: \n    \n    –ë–î –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –º–µ—Ö–∞–Ω–∏–∑–º—ã –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è *–ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏*.\n    \n5. –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å:\n    \n    –ë–î –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —Å—Ä–µ–¥—Å—Ç–≤–∞ *–∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏*, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç *–±–µ–∑–æ–ø–∞—Å–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ –¥–∞–Ω–Ω—ã–º*. –û–Ω–∏ —Ç–∞–∫–∂–µ –º–æ–≥—É—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å *–º–µ—Ö–∞–Ω–∏–∑–º—ã —à–∏—Ñ—Ä–æ–≤–∞–Ω–∏—è* –¥–ª—è –∑–∞—â–∏—Ç—ã –¥–∞–Ω–Ω—ã—Ö.\n    \n6. –†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ: \n    \n    –ë–î –ø–æ–∑–≤–æ–ª—è—é—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å *—Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏ –¥–∞–Ω–Ω—ã—Ö* –∏ *–≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å* –∏—Ö *–≤ —Å–ª—É—á–∞–µ —Å–±–æ–µ–≤ –∏–ª–∏ –ø–æ—Ç–µ—Ä–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏*.\n    \n7. –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å: \n    \n    –ú–Ω–æ–≥–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ë–î –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç *–º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å*, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç *—É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å –æ–±—ä–µ–º* –∏ *–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å* —Å–∏—Å—Ç–µ–º—ã *—Å —Ä–æ—Å—Ç–æ–º –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–∞–≥—Ä—É–∑–∫–∏*.\n    \n8. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏: \n    \n    –ë–î –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —Å—Ä–µ–¥—Å—Ç–≤–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã–º–∏, –≤–∫–ª—é—á–∞—è *–¥–æ–±–∞–≤–ª–µ–Ω–∏–µ, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ, —É–¥–∞–ª–µ–Ω–∏–µ –∏ –ø–æ–∏—Å–∫ –∑–∞–ø–∏—Å–µ–π*.	16:00:00	2025-04-27 12:53:57.841478+00	f
274	10	Database scaling	#code	There are several approaches to scaling applications and infrastructure. The choice of a specific approach depends on the requirements, business case, and architecture of the system. Here are the main scaling approaches:\n\n1. Vertical Scaling:  \n   Vertical scaling, also known as scaling up, involves increasing the power of a single machine or server by upgrading its resources, such as CPU, RAM, or disk space. This method is suitable for small to medium loads, but it has limitations in terms of scalability and can become expensive. Essentially, you are making the existing server stronger.\n    \n2. Horizontal Scaling:   \n   Horizontal scaling, also known as scaling out, involves adding more machines or servers to the existing infrastructure. Each machine handles a portion of the workload. This approach allows for handling larger loads and provides higher availability and fault tolerance. It is commonly used in modern scalable applications and is often preferred for its ability to distribute load and handle failures more gracefully than vertical scaling.\n    \n3. Cloud Scaling:  \n   Cloud platforms like Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) offer the ability to scale resources on demand. You can quickly add or remove servers and services in the cloud based on the current load. This greatly simplifies horizontal scaling as the cloud provider manages the underlying infrastructure. Cloud scaling offers flexibility, cost-efficiency (pay-as-you-go), and frequently includes automated scaling capabilities.\n    \n4. Microservices Architecture:  \n   A microservices' architecture involves dividing an application into small, independent services, each of which can be scaled independently. This approach provides granular control over resources and allows you to scale only the parts of the application that require more capacity. Microservices enhance agility, resilience, and enable technology diversity for different components of the application.\n    \n5. Load Balancing:  \n   Utilizing load balancers enables the distribution of requests across multiple servers or application instances. This improves performance by preventing any single server from being overwhelmed and ensures high availability by automatically routing traffic away from failed servers. Load balancers are a key component in horizontal scaling and can distribute traffic based on various algorithms (e.g., round-robin, least connections).\n    \n6. Caching:  \n   Implementing caching mechanisms can significantly reduce the load on the database and accelerate response times to queries. Caches can be placed at different levels, including the application level (e.g., in-memory caches like Redis or Memcached) and the database level (e.g., query result caching). Effective caching strategies are crucial for improving application performance and reducing latency.\n    \n7. Sharding:  \n   Sharding, also known as database partitioning, involves dividing data into multiple parts (shards) that are stored on different servers or databases. Each shard handles its portion of the workload. This approach is essential for managing and processing massive volumes of data that would be impractical to store and manage on a single server. Sharding improves query performance and scalability for very large datasets.\n    \n8. Code and Query Optimization:  \n   It's not always necessary to increase resources. Sometimes, optimizing code, SQL queries, and data structures can significantly improve performance. This includes techniques like indexing database tables, rewriting inefficient queries, optimizing algorithms, and choosing appropriate data types. Performance optimization should always be considered before resorting to scaling infrastructure as it can often provide substantial gains with minimal cost.\n    \n\nThese scaling approaches are not mutually exclusive and are typically used in combination to create robust and highly scalable systems. The best strategy depends on the specific needs and constraints of each application.\n\nhttps://disk.yandex.ru/i/i4j63n7OCzlquw	96:00:00	2025-03-16 08:42:08.407367+00	t
145	10	The Three Sum\n\nGiven an integer array nums, return all the triplets [nums[i], nums[j], nums[k]] such that i != j, i != k, and j != k, and nums[i] + nums[j] + nums[k] == 0.\n\nNotice that the solution set must not contain duplicate triplets.	#leetcode	func threeSum(nums []int) [][]int {\n  res := make([][]int, 0, len(nums)/3)\n\n  sort.Ints(nums)\n\n    for i := range nums {\n        if i > 0 && nums[i] == nums[i-1] {\n            continue\n        }\n\n        l, r := i+1, len(nums)-1\n        for l < r {\n            total := nums[i] + nums[l] + nums[r]\n\n            if total > 0 {\n                r--\n                continue\n            }\n            \n            if total < 0 {\n                l++\n                continue\n            }  \n\n            res = append(res, []int{nums[i], nums[l], nums[r]})\n            for l < r && nums[l] == nums[l+1] { l++ }\n            for l < r && nums[r] == nums[r-1] { r-- }\n            l++\n            r--\n        }\n    }\n\n  return res\n}	384:00:00	2025-05-05 22:47:17.603969+00	f
252	10	DML opeartors	#sql	_Data Manipulation Language_\n\n- SELECT:  \n  Retrieves data from one or more tables. You can specify which columns to retrieve, which rows to filter, and how to sort the results.\n        SELECT Name, City\n    FROM Customers\n    WHERE City = 'New York'\n    ORDER BY Name;\n    \n\n- INSERT:  \n  Adds new rows of data to a table. You need to specify the table name and the values for each column.\n        INSERT INTO Customers (CustomerID, Name, Address, City)\n    VALUES (1, 'John Doe', '123 Main St', 'Anytown');\n    \n\n- UPDATE:  \n  Modifies existing data in a table. You need to specify the table name, the columns to update, and the new values. You can also use a WHERE clause to filter which rows to update.\n        UPDATE Customers\n    SET City = 'Los Angeles'\n    WHERE CustomerID = 1;\n    \n\n- DELETE:  \n  Removes rows from a table. You can use a WHERE clause to filter which rows to delete. \n        DELETE FROM Customers\n    WHERE CustomerID = 1;	192:00:00	2025-04-26 14:45:42.795982+00	t
455	10	quantum field responds to ‚Ä¶	#quotes	The quantum  field responds not to what we want; it responds to who we are being.	192:00:00	2025-04-27 06:58:08.341528+00	f
218	10	Word Pattern\n\nGiven a pattern and a string s, find if s follows the same pattern.\n\nHere follow means a full match, such that there is a bijection between a letter in pattern and a non-empty word in s. Specifically:\n\n- Each letter in pattern maps to exactly one unique word in s.\n- Each unique word in s maps to exactly one letter in pattern.\n- No two letters map to the same word, and no two words map to the same letter.	#leetcode	func wordPattern(p string, s string) bool {\n    words := strings.Split(s, " ")\n    if len(words) != len(p) {\n        return false\n    }\n\n    mp := make(map[string]byte, len(s)) \n    paired := [26]bool{}\n\n    for i, word := range words {\n        if letter, ok := mp[word]; ok {\n            if letter != p[i] {\n                return false\n            }\n            continue\n        }\n        \n        if paired[p[i]-97] {\n            return false\n        }\n\n        paired[p[i]-97] = true\n        mp[word] = p[i]\n    }\n\n    return true\n}	768:00:00	2025-06-05 22:34:15.453722+00	f
476	10	01	#majorsystem	suit	96:00:00	2025-04-27 17:49:58.896826+00	f
438	10	–Ω–µ–º–æ—â–Ω—ã–π, —Ö–∏–ª—ã–π	#words	feeble	384:00:00	2025-04-29 14:44:15.58822+00	f
494	10	16	#majorsystem	watch ‚åöÔ∏è	48:00:00	2025-04-29 09:28:12.502394+00	f
507	11	13. –ß—Ç–æ —Ç–∞–∫–æ–µ ACID?	#—Ç–µ–æ—Ä–∏—è–±–¥	ACID - —ç—Ç–æ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –æ–∑–Ω–∞—á–∞–µ—Ç *Atomicity, Consistency, Isolation, Durability (–ê—Ç–æ–º–∞—Ä–Ω–æ—Å—Ç—å, –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å, –ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å, –î–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç—å)*. *–≠—Ç–æ —Å–≤–æ–π—Å—Ç–≤–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –≤ —Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö –±–∞–∑–∞—Ö –¥–∞–Ω–Ω—ã—Ö*.\n\n- –ê—Ç–æ–º–∞—Ä–Ω–æ—Å—Ç—å (Atomicity) - —ç—Ç–æ —Å–≤–æ–π—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä–æ–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ *–ª–∏–±–æ –≤—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –±—É–¥—É—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω—ã, –ª–∏–±–æ –Ω–∏ –æ–¥–Ω–∞.*\n- –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å (Consistency) - —ç—Ç–æ —Å–≤–æ–π—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä–æ–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ *—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è –Ω–µ –Ω–∞—Ä—É—à–∞–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö*.\n- –ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å (Isolation) - —ç—Ç–æ —Å–≤–æ–π—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä–æ–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è *–Ω–µ –±—É–¥–µ—Ç –≤–∏–¥–Ω–∞ –¥—Ä—É–≥–∏–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º, –ø–æ–∫–∞ –æ–Ω–∞ –Ω–µ –±—É–¥–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∞.*\n- –î–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç—å (Durability) - —ç—Ç–æ —Å–≤–æ–π—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä–æ–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ *—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ø–æ—Å–ª–µ –µ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è.*	16:00:00	2025-04-27 13:59:26.341893+00	f
510	11	16. –ö–∞–∫–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ CAP –∏–º–µ–µ—Ç –∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –ë–î?	#—Ç–µ–æ—Ä–∏—è–±–¥	–¢–µ–æ—Ä–µ–º–∞ CAP (Consistency, Availability, Partition Tolerance) –∏–º–µ–µ—Ç –ø—Ä—è–º–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º —Å–∏—Å—Ç–µ–º–∞–º –∏, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –±–∞–∑–∞–º –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–∞ —Ç–µ–æ—Ä–µ–º–∞, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –≠—Ä–∏–∫–æ–º –ë—Ä—é–≤–µ—Ä–æ–º (Eric Brewer) –≤ 2000 –≥–æ–¥—É, —É—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç, —á—Ç–æ –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –º–æ–∂–Ω–æ –æ–±–µ—Å–ø–µ—á–∏—Ç—å —Ç–æ–ª—å–∫–æ –¥–≤–∞ –∏–∑ —Ç—Ä–µ—Ö —Å–≤–æ–π—Å—Ç–≤: —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å (Consistency), –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å (Availability) –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—é (Partition Tolerance)	16:00:00	2025-04-27 08:31:00.839774+00	f
477	10	02	#majorsystem	swan	192:00:00	2025-04-30 17:55:30.606009+00	f
437	10	–≤–∏–¥–Ω—ã–π, –±—Ä–æ—Å–∞—é—â–∏–π—Å—è –≤ –≥–ª–∞–∑–∞	#words	conspicuous	192:00:00	2025-05-02 19:49:03.463155+00	f
508	11	14. –ö–∞–∫ —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—é—Ç—Å—è ACID?	#—Ç–µ–æ—Ä–∏—è–±–¥	–†–µ–∞–ª–∏–∑–∞—Ü–∏—è ACID –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö (–°–£–ë–î) –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –∏ —Ç–µ—Ö–Ω–∏–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫:\n\n- –ñ—É—Ä–Ω–∞–ª–∏–∑–∞—Ü–∏—è (Logging): *–ó–∞–ø–∏—Å—å* –≤—Å–µ—Ö *–∏–∑–º–µ–Ω–µ–Ω–∏–π* *–≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –≤ –∂—É—Ä–Ω–∞–ª* –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ *–æ—Ç–∫–∞—Ç–∞* *–≤ —Å–ª—É—á–∞–µ –Ω–µ—É–¥–∞—á–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏*.\n- –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∏ (Locking): –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ –¥–ª—è *–æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è* *–∏–∑–æ–ª—è—Ü–∏–∏* —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –∏ *–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è* *–∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤* *–¥–æ—Å—Ç—É–ø–∞* *–∫ –¥–∞–Ω–Ω—ã–º*.\n- –ú–µ—Ö–∞–Ω–∏–∑–º—ã –æ—Ç–∫–∞—Ç–∞ (Rollback Mechanisms): –ü–æ–∑–≤–æ–ª—è—é—Ç *–æ—Ç–∫–∞—Ç—ã–≤–∞—Ç—å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –ø—Ä–∏ —Å–±–æ—è—Ö –∏–ª–∏ –æ—à–∏–±–∫–∞—Ö*.\n- –ñ—É—Ä–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—è (Logging and Replication): –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –¥*–æ–ª–≥–æ–≤–µ—á–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏*.	16:00:00	2025-04-27 12:18:04.970677+00	f
470	10	An opinion or conclusion formed on the basis of incomplete information	#words	conjecture	48:00:00	2025-04-29 04:34:28.719757+00	f
466	5	–æ–ø–µ—Ä–∞—Ç–æ—Ä—ã –¥–ª—è –≤—ã–±–æ—Ä–∫–∏ –¥–∞–Ω–Ω—ã—Ö sql	#sql	( ) - –Ω–∞–∏–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç, –∑–∞–¥–∞—ë—Ç –ø–æ—Ä—è–¥–æ–∫ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π\n\n    * / - —É–º–Ω–æ–∂–µ–Ω–∏–µ –∏ –¥–µ–ª–µ–Ω–∏–µ (–≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)\n\n    + - - —Å–ª–æ–∂–µ–Ω–∏–µ –∏ –≤—ã—á–∏—Ç–∞–Ω–∏–µ (—Å—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)\n\n    = > < >= <= <> - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π (–≤–æ–∑–≤—Ä–∞—â–∞—é—Ç TRUE/FALSE)\n\n    BETWEEN - –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω (–≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\n\n    IN - –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏—è –≤ —Å–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π (order IN(255,300) –∑–∞–º–µ–Ω—è–µ—Ç order = 255, order =300)\n\n    NOT - –æ—Ç—Ä–∏—Ü–∞–Ω–∏–µ —É—Å–ª–æ–≤–∏—è (–≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)\n\n    AND - –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –ò (–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –¥–æ OR)\n\n    OR - –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –ò–õ–ò (–Ω–∏–∑—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)\n    \n    ORDER BY  -  —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Å—Ç–æ–ª–±—Ü–∞ –æ—Ç –º–µ–Ω—å—à–µ–≥–æ –∫ –±–æ–ª—å—à–µ–º—É (DESC -  –æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É)	04:00:00	2025-04-27 04:10:58.673586+00	f
478	10	03	#majorsystem	Samia (The Agency)	192:00:00	2025-05-03 17:35:42.694684+00	f
495	10	17	#majorsystem	duck ü¶Ü	48:00:00	2025-04-29 06:53:08.224042+00	f
244	10	Two Sum\n\nGiven an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n\nYou may assume that each input would have exactly one solution, and you may not use the same element twice.\n\nYou can return the answer in any order.	#leetcode	func twoSum(nums []int, target int) []int {\n    mp := make(map[int]int, len(nums)) \n\n    for i, v := range nums {\n        mp[v] = i\n    }\n\n    for i, v := range nums {\n        complement := target - v\n        if j, ok := mp[complement]; ok && j != i {\n            return []int{i, j}\n        }\n    }\n\n    return []int{0, 0}\n}	192:00:00	2025-05-07 04:03:56.212594+00	f
471	10	(of a place) not seen or visited by many people; sheltered and private	#words	secluded	96:00:00	2025-05-01 17:56:12.381603+00	f
441	10	–ø–æ—Ä–æ–≥, –ø–æ–¥–æ–∫–æ–Ω–Ω–∏–∫	#words	sill, windowsill	192:00:00	2025-05-01 17:32:28.903706+00	f
399	5	DDD	#ddd	DDD - —Å–ø–æ—Å–æ–± –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –≤–æ–∫—Ä—É–≥ –ø—Ä–µ–¥–µ–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏. –ù–∞—Ü–µ–ª–µ–Ω –Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Ç–∞–∫ –∂–µ –∏   \nDomain - —ç—Ç–æ –ø—Ä–µ–¥–º–µ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å. –í–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫—É, –ø—Ä–∞–≤–∏–ª–∞ –∏ –ø—Ä–æ—Ü–µ—Å—Å—ã, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏. (–±–∏–∑–Ω–µ—Å –∫–æ–¥ –∫–æ—Ç–æ—Ä—ã–π —Ç—ã –±—É–¥–µ—à—å –≤–æ–ø–ª–∞—â–∞—Ç—å)\n### –ö–æ–Ω—Ü–µ–ø—Ü–∏–∏ DDD:\n#### –ï–¥–∏–Ω—ã–π —è–∑—ã–∫ (Ubiquitous Language) \n–≠—Ç–æ –æ–±—â–∏–π —Å–ª–æ–≤–∞—Ä—å —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ –ø–æ–Ω—è—Ç–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –≤—Å–µ–º–∏ —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏ –ø—Ä–æ–µ–∫—Ç–∞ ‚Äî —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º–∏, –∞–Ω–∞–ª–∏—Ç–∏–∫–∞–º–∏ –∏ –±–∏–∑–Ω–µ—Å-—ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏. –¢–∞–∫–æ–π —è–∑—ã–∫ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –µ–¥–∏–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ –∏ –æ–±–ª–µ–≥—á–∞–µ—Ç –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é –≤–Ω—É—Ç—Ä–∏ –∫–æ–º–∞–Ω–¥—ã.\n\n#### –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (Bounded Context)\n–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–æ–º–µ–Ω–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ –Ω–∞ –ø–æ–¥–¥–æ–º–µ–Ω—ã. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –∏ —Å–ø–æ—Å–æ–±—ã –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –º–µ–∂–¥—É –Ω–∏–º–∏\n\n#### –°—É—â–Ω–æ—Å—Ç–∏ (Entities) \n–≠—Ç–æ –æ–±—ä–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–±–ª–∞–¥–∞—é—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å—é –∏ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º. –û–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –æ—Ç–ª–∏—á–∏—Ç—å –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ —Å–∏—Å—Ç–µ–º–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–∫–∞–∑–∞–º–∏ –∫–∞–∂–¥—ã–π –∑–∞–∫–∞–∑ —è–≤–ª—è–µ—Ç—Å—è —Å—É—â–Ω–æ—Å—Ç—å—é, –ø–æ—Å–∫–æ–ª—å–∫—É —É –Ω–µ–≥–æ –µ—Å—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –æ—Ç–ª–∏—á–∞—Ç—å –µ–≥–æ –æ—Ç –¥—Ä—É–≥–∏—Ö –∑–∞–∫–∞–∑–æ–≤, –¥–∞–∂–µ –µ—Å–ª–∏ –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç.\n   \n   \n#### –û–±—ä–µ–∫—Ç—ã-–∑–Ω–∞—á–µ–Ω–∏—è (Value Objects) \n–≠—Ç–æ –æ–±—ä–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ —Å–≤–æ–∏–º–∏ —Å–≤–æ–π—Å—Ç–≤–∞–º–∏. –ü–æ —Å—É—Ç–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –¥–∞–Ω–Ω—ã—Ö. –ú–æ–∂–µ—Ç –±—ã—Ç—å —á–∞—Å—Ç—å—é —Å—É—â–Ω–æ—Å—Ç–∏. –ù–µ –∏–º–µ–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –∞–¥—Ä–µ—Å –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—ä–µ–∫—Ç–æ–º-–∑–Ω–∞—á–µ–Ω–∏–µ–º: –µ—Å–ª–∏ –¥–≤–∞ –∞–¥—Ä–µ—Å–∞ –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —É–ª–∏—Ü—ã, –≥–æ—Ä–æ–¥–∞ –∏ –ø–æ—á—Ç–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞, –æ–Ω–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–º–∏.\n \n#### –ê–≥—Ä–µ–≥–∞—Ç—ã (Aggregates)\n—ç—Ç–æ –≥—Ä—É–ø–ø—ã —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∫–∞–∫ –µ–¥–∏–Ω–æ–µ —Ü–µ–ª–æ–µ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π. –£ –∞–≥—Ä–µ–≥–∞—Ç–∞ –µ—Å—Ç—å –∫–æ—Ä–Ω–µ–≤–∞—è —Å—É—â–Ω–æ—Å—Ç—å (Aggregate Root), —á–µ—Ä–µ–∑ –∫–æ—Ç–æ—Ä—É—é –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è –¥–æ—Å—Ç—É–ø –∫ –¥—Ä—É–≥–∏–º –æ–±—ä–µ–∫—Ç–∞–º –≤–Ω—É—Ç—Ä–∏ –∞–≥—Ä–µ–≥–∞—Ç–∞.\n\n#### –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ (Repositories)\n—ç—Ç–æ –æ–±—ä–µ–∫—Ç—ã –∏–ª–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, –æ—Ç–≤–µ—á–∞—é—â–∏–µ –∑–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–æ–º –∫ –¥–∞–Ω–Ω—ã–º —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –∞–≥—Ä–µ–≥–∞—Ç–æ–≤. –û–Ω–∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—é –Ω–∞–¥ –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –ø–æ–∑–≤–æ–ª—è—è —Ä–∞–±–æ—Ç–∞—Ç—å —Å –∫–æ–ª–ª–µ–∫—Ü–∏—è–º–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∫–∞–∫ —Å –≤ –ø–∞–º—è—Ç–∏.	24:00:00	2025-04-27 13:21:02.319061+00	f
458	10	monotonous routine	#words	humdrum	384:00:00	2025-05-07 17:28:00.070911+00	f
509	11	15. –ß—Ç–æ —Ç–∞–∫–æ–µ CAP?	#—Ç–µ–æ—Ä–∏—è–±–¥	CAP - —ç—Ç–æ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –æ–∑–Ω–∞—á–∞–µ—Ç *Consistency, Availability, Partition tolerance (–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å, –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å, –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—é)*. –≠—Ç–æ *—Å–≤–æ–π—Å—Ç–≤–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º*.\n\n- –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å (Consistency)\n–≠—Ç–æ —Å–≤–æ–π—Å—Ç–≤–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã, –∫–æ—Ç–æ—Ä–æ–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ *–∫–∞–∂–¥—ã–π —É–∑–µ–ª —Å–∏—Å—Ç–µ–º—ã –≤–∏–¥–∏—Ç –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ –¥–∞–Ω–Ω—ã–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ*.\n- –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å (Availability) \n–≠—Ç–æ —Å–≤–æ–π—Å—Ç–≤–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã, –∫–æ—Ç–æ—Ä–æ–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ *–∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å –∫ —Å–∏—Å—Ç–µ–º–µ –±—É–¥–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ*.\n- –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—é (Partition tolerance)\n–≠—Ç–æ —Å–≤–æ–π—Å—Ç–≤–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã, –∫–æ—Ç–æ—Ä–æ–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ *—Å–∏—Å—Ç–µ–º–∞ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–∂–µ –≤ —Å–ª—É—á–∞–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Å–µ—Ç–∏ –Ω–∞ —á–∞—Å—Ç–∏*.	16:00:00	2025-04-27 14:45:36.6218+00	f
479	10	04	#majorsystem	Zorro	96:00:00	2025-04-30 17:26:49.568616+00	f
452	10	Binary Tree Right Side View\n\nGiven the root of a binary tree, imagine yourself standing on the right side of it, return the values of the nodes you can see ordered from top to bottom.	#leetcode	### CPU Efficient\n\nfunc rightSideView(root *TreeNode) []int {\n  if root == nil {\n    return []int{}\n  }\n\n  result := []int{}\n  queue := []*TreeNode{root}\n\n  for len(queue) > 0 {\n    result = append(result, queue[len(queue)-1].Val)\n\n    for _, node := range queue {\n      if node.Left != nil {\n        queue = append(queue, node.Left)\n      }\n      if node.Right != nil {\n        queue = append(queue, node.Right)\n      }\n      queue = queue[1:]\n    }\n  }\n\n  return result\n}\n\n\n### Memory Efficient\n\nfunc rightSideView(root *TreeNode) []int {\n  res := []int{}\n  helper(root, 0, &res)\n  return res\n}\n\nfunc helper(root *TreeNode, depth int, res *[]int) {\n  if root == nil {\n    return\n  }  \n\n  if len(*res)-1 < depth {\n    *res = append(*res, root.Val)\n  } else {\n    (*res)[depth] = root.Val\n  }\n\n  helper(root.Left, depth+1, res)\n  helper(root.Right, depth+1, res)\n}	96:00:00	2025-05-02 15:37:03.807024+00	f
59	10	Gas Station\n\nThere are n gas stations along a circular route, where the amount of gas at the ith station is gas[i].\n\nYou have a car with an unlimited gas tank and it costs cost[i] of gas to travel from the ith station to its next (i + 1)th station. You begin the journey with an empty tank at one of the gas stations.\n\nGiven two integer arrays gas and cost, return the starting gas station's index if you can travel around the circuit once in the clockwise direction, otherwise return -1. If there exists a solution, it is guaranteed to be unique.	#leetcode	func canCompleteCircuit(gas []int, cost []int) int {\n    if sum(gas...) < sum(cost... ) {\n        return -1\n    }\n\n    tank := 0\n    start := 0\n    for i := 0; i < len(gas); i++ {\n        tank += gas[i] - cost[i]\n        if tank < 0 {\n            tank = 0\n            start = i + 1\n        }\n    }\n\n    return start\n}\n\nfunc sum(nums ...int) (rv int) {\n    for _, num := range nums {\n        rv += num\n    }\n    return rv\n}	384:00:00	2025-05-22 16:16:26.2255+00	f
480	10	05	#majorsystem	seal ü¶≠	192:00:00	2025-04-28 02:15:41.121313+00	f
459	10	Go the opposite direction from your impulses	#mindfulness	Too agitated ‚Äî relax, eating disorder ‚Äî starve, escaping reality ‚Äî meditate and be present, afraid ‚Äî face it	24:00:00	2025-04-25 08:21:47.563792+00	t
460	10	Positive affirmations out loud	#mindfulness		24:00:00	2025-04-13 03:45:53.305175+00	t
355	10	Partition List\n\nGiven the head of a linked list and a value x, partition it such that all nodes less than x come before nodes greater than or equal to x.\n\nYou should preserve the original relative order of the nodes in each of the two partitions.	#leetcode	/**\n * Definition for singly-linked list.\n * type ListNode struct {\n *     Val int\n *     Next *ListNode\n * }\n */\nfunc partition(head *ListNode, x int) *ListNode {\n  if head == nil {\n    return head\n  }\n\n  dummyLeft := new(ListNode)\n  dummyRight := new(ListNode)\n\n  currLeft, currRight := dummyLeft, dummyRight\n\n  for curr := head; curr != nil; curr = curr.Next {\n    if curr.Val < x {\n      currLeft.Next = &ListNode{Val: curr.Val}\n      currLeft = currLeft.Next\n      continue\n    }\n\n    currRight.Next = &ListNode{Val: curr.Val}\n    currRight = currRight.Next\n  }\n\n  currLeft.Next = dummyRight.Next\n\n  return dummyLeft.Next\n}	384:00:00	2025-05-16 09:52:50.978563+00	f
481	10	06	#majorsystem	sushi üç£	384:00:00	2025-05-09 03:38:18.271205+00	f
214	10	You need to get your head towards the grindstone and enjoy living in there.	#waytosuccess		72:00:00	2025-04-01 10:32:45.626882+00	t
472	10	accept something reluctantly but without a protest	#words	to acquiesce	96:00:00	2025-05-01 10:55:04.979282+00	f
482	10	07	#majorsystem	sock üß¶	48:00:00	2025-04-28 13:51:41.435236+00	f
409	5	–ö–∞–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥—è–≤–ª—è—é—Ç—Å—è –∫ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞–º		–û—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞–º –≤–∫–ª—é—á–∞—é—Ç:\n\n    –ê–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç—å: –∫–∞–∂–¥—ã–π –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å, —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å, —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ –æ—Ç–¥–µ–ª—å–Ω–æ –æ—Ç –¥—Ä—É–≥–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã.\n\n    –ß—ë—Ç–∫–æ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã: –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞–º–∏ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ —Ö–æ—Ä–æ—à–æ –∑–∞–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ API, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è.\n\n    –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å: –∫–∞–∂–¥—ã–π –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –µ–≥–æ –º–æ–∂–Ω–æ –±—ã–ª–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ, –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞–≥—Ä—É–∑–∫–∏ –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –±–∏–∑–Ω–µ—Å–∞.\n\n    –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Å–±–æ—è–º: –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω—ã —Å —É—á—ë—Ç–æ–º –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ—Ç–∫–∞–∑–æ–≤, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã –≤ —Ü–µ–ª–æ–º.\n\n    –ù–µ–∑–∞–≤–∏—Å–∏–º—ã–π –∂–∏–∑–Ω–µ–Ω–Ω—ã–π —Ü–∏–∫–ª: –∫–∞–∂–¥—ã–π –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –∏ —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞—Ç—å—Å—è –±–µ–∑ –≤–ª–∏—è–Ω–∏—è –Ω–∞ —Ä–∞–±–æ—Ç—É –¥—Ä—É–≥–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤.\n\n    –í—ã–±–æ—Ä —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π: —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ —è–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞, –≤—ã–±–∏—Ä–∞—è –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á.	16:00:00	2025-04-27 04:01:24.215217+00	f
454	5	Event-Driven Architecture  (EDA)	#design development	–°–æ–±—ã—Ç–∏–π–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ - —ç—Ç–æ —à–∞–±–ª–æ–Ω –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –≤ –∫–æ—Ç–æ—Ä–æ–º —Å–∏—Å—Ç–µ–º–∞ —Å—Ç—Ä–æ–∏—Ç—Å—è –≤–æ–∫—Ä—É–≥ —Å–æ–∑–¥–∞–Ω–∏—è, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è, –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏–π.  –í —Ç–∞–∫–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º —á–µ—Ä–µ–∑ —Å–æ–±—ã—Ç–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º —Ä–µ–∞–≥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥—è—â–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏\n\n–°–æ–±—ã—Ç–∏–µ ‚Äì —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏–µ, –∏–Ω–∏—Ü–∏–∏—Ä—É—é—â–µ–µ –ª–∏–±–æ –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ, –ª–∏–±–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.\n–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ EDA\n–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å:\n\n–°–æ–±—ã—Ç–∏–π–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–æ–∑–≤–æ–ª—è—é—Ç –ª–µ–≥–∫–æ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞–º–∏\n–ì–∏–±–∫–æ—Å—Ç—å:\n\n–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã –º–æ–≥—É—Ç –±—ã—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞, —á—Ç–æ —É–ø—Ä–æ—â–∞–µ—Ç –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π\n–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç—å:\n\nEDA –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–æ–±—ã—Ç–∏–π, —á—Ç–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã, —Ç–∞–∫ –∫–∞–∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –º–æ–≥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ\n–†–µ–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å:\n\n–°–∏—Å—Ç–µ–º—ã, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ EDA, –º–æ–≥—É—Ç –±—ã—Å—Ç—Ä–æ —Ä–µ–∞–≥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è —Ç–∞–∫–∏—Ö —Å—Ñ–µ—Ä, –∫–∞–∫ —Ñ–∏–Ω–∞–Ω—Å—ã, IoT –∏ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è\n–¢–æ–ø–æ–ª–æ–≥–∏–∏ —Å–æ–±—ã—Ç–∏–π–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã\nEDA –∏–º–µ–µ—Ç –¥–≤–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–æ–ø–æ–ª–æ–≥–∏–∏:\n\n–¢–æ–ø–æ–ª–æ–≥–∏—è –ø–æ—Å—Ä–µ–¥–Ω–∏–∫–∞ (Mediator):\n\n–í —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –ø–æ—Å—Ä–µ–¥–Ω–∏–∫ –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏. –û–Ω —É–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ—Ç–æ–∫–æ–º —Å–æ–±—ã—Ç–∏–π –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω—ã –æ —Å–æ–±—ã—Ç–∏–∏\n–¢–æ–ø–æ–ª–æ–≥–∏—è –±—Ä–æ–∫–µ—Ä–∞ (Broker):\n\n–ó–¥–µ—Å—å —Å–æ–±—ã—Ç–∏—è –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±—Ä–æ–∫–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –¥–µ–π—Å—Ç–≤—É–µ—Ç –∫–∞–∫ –ø–æ—Å—Ä–µ–¥–Ω–∏–∫ –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è–º–∏ –∏ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è–º–∏ —Å–æ–±—ã—Ç–∏–π. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤	01:00:00	2025-04-27 03:20:54.124557+00	f
161	10	Spiral Matrix\n\nGiven an m x n matrix, return all elements of the matrix in spiral order.	#leetcode	func spiralOrder(mtx [][]int) []int {\n    n, m := len(mtx), len(mtx[0]) // matrix dimensions\n    rv := make([]int, n*m) // return value array\n\n    // row/col indeces and the direction of movement\n    row, col, drow, dcol := 0, 0, 0, 1 \n    top, right, bottom, left := 1, m-1, n-1, 0 // borders\n    \n    for i := 0; i < n*m; i++ {\n        rv[i] = mtx[row][col]\n\n        switch {\n        case dcol == 1 && col == right:\n            drow = 1\n            dcol = 0\n            right--\n\n        case drow == 1 && row == bottom: \n            drow = 0\n            dcol = -1\n            bottom--\n        \n        case dcol == -1 && col == left:\n            drow = -1\n            dcol = 0\n            left++\n\n        case drow == -1 && row == top:\n            drow = 0\n            dcol = 1\n            top++\n        }\n\n        row += drow\n        col += dcol\n    }\n\n    return rv\n}	384:00:00	2025-05-16 09:46:22.247006+00	f
410	10	Go farm	#code		12:00:00	2025-03-15 03:58:19.578966+00	t
402	5	Gitflow	#gitflow	Gitflow ‚Äî —ç—Ç–æ —Ä–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å –≤ Git, –æ—á–µ–Ω—å –∞–∫—Ç–∏–≤–Ω–æ –∑–∞–¥–µ–π—Å—Ç–≤—É—é—â–∏–π –≤–µ—Ç–≤–ª–µ–Ω–∏–µ. Gitflow ‚Äî —ç—Ç–æ –∫–æ–≥–¥–∞ –Ω–æ–≤—ã–π –∫–æ–¥ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è (–º–µ—Ä–¥–∂–∏—Ç—Å—è) –≤ develop-–≤–µ—Ç–∫—É, –∞ –Ω–µ main-–≤–µ—Ç–∫—É; main-–≤–µ—Ç–∫–∞ —Å–ª—É–∂–∏—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –∏—Å—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞.\n–ú–æ–¥–µ–ª—å –≤–µ—Ç–≤–ª–µ–Ω–∏—è: Git-flow –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å—Ç—Ä–æ–≥—É—é –º–æ–¥–µ–ª—å –≤–µ—Ç–≤–ª–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–µ—Ç–∫–∏:\nmaster: –í–µ—Ç–∫–∞ master –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—É—é –∏ –≤—ã–ø—É—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –≤–∞—à–µ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è. –ù–∞ —ç—Ç–æ–π –≤–µ—Ç–∫–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –∏ –≥–æ—Ç–æ–≤—ã–µ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤–µ—Ä—Å–∏–∏.\ndevelop: –í–µ—Ç–∫–∞ develop —Å–ª—É–∂–∏—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤–µ—Ç–∫–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏, –≥–¥–µ –≤—Å–µ –Ω–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –ø–µ—Ä–µ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–µ–π.\nfeature/<–Ω–∞–∑–≤–∞–Ω–∏–µ-—Ñ—É–Ω–∫—Ü–∏–∏>: –í–µ—Ç–∫–∏ feature —Å–æ–∑–¥–∞—é—Ç—Å—è –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π. –ö–∞–∂–¥–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é –≤–µ—Ç–∫—É.\nrelease/<–Ω–æ–º–µ—Ä-–≤–µ—Ä—Å–∏–∏>: –í–µ—Ç–∫–∏ release –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ –≤—ã–ø—É—Å–∫—É –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏. –ó–¥–µ—Å—å –∏—Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–µ –æ—à–∏–±–∫–∏ –∏ –≤–Ω–æ—Å—è—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–µ—Ä–µ–¥ —Ä–µ–ª–∏–∑–æ–º.\nhotfix/<–Ω–æ–º–µ—Ä-–≤–µ—Ä—Å–∏–∏>: –í–µ—Ç–∫–∏ hotfix —Å–æ–∑–¥–∞—é—Ç—Å—è –¥–ª—è –≤–Ω–µ–ø–ª–∞–Ω–æ–≤—ã—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –æ—à–∏–±–æ–∫ –≤ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.	64:00:00	2025-04-29 08:13:46.690022+00	f
473	10	pull (something) hard or suddenly	#words	to tug	96:00:00	2025-04-28 15:05:23.485219+00	f
21	10	Don't just think hard, visualise and break down into small steps.	#leetCode	When working on solution, don't try to simply come up with one. Instead, visualise your algorithm's work and break it down to simple steps and conditions.	96:00:00	2025-03-24 08:57:02.656941+00	t
13	10	Breathe into your belly. Feel like it fills and empties as big but smooth waves.	#mindfulness	Think of what it means to calm down, imagine it.	96:00:00	2025-03-31 04:29:03.782874+00	t
483	10	08	#majorsystem	sieve	48:00:00	2025-04-28 08:55:40.968036+00	f
324	10	Min Stack\n\nDesign a stack that supports push, pop, top, and retrieving the minimum element in constant time.\n\n- MinStack() initializes the stack object.\n- void push(int val) pushes the element val onto the stack.\n- void pop() removes the element on the top of the stack.\n- int top() gets the top element of the stack.\n- int getMin() retrieves the minimum element in the stack.\n\nYou must implement a solution with O(1) time complexity for each function.	#leetcode	/**\n * Your MinStack object will be instantiated and called as such:\n * obj := Constructor();\n * obj.Push(val);\n * obj.Pop();\n * param_3 := obj.Top();\n * param_4 := obj.GetMin();\n */\n\ntype MinStack struct {\n  values []int\n  mins   []int\n}\n\nfunc Constructor() MinStack {\n  return MinStack{}\n}\n\nfunc (this *MinStack) Push(val int) {\n  this.values = append(this.values, val)\n  if len(this.mins) == 0 {\n    this.mins = append(this.mins, val)\n  } else {\n    this.mins = append(this.mins, min(this.mins[len(this.mins)-1], val))\n  }\n}\n\nfunc (this *MinStack) Pop() {\n  if len(this.values) == 0 {\n    return\n  }\n  this.values = this.values[:len(this.values)-1]\n  this.mins = this.mins[:len(this.mins)-1]\n}\n\nfunc (this *MinStack) Top() int {\n  return this.values[len(this.values)-1]\n}\n\nfunc (this *MinStack) GetMin() int {\n  return this.mins[len(this.mins)-1]\n}	192:00:00	2025-05-01 16:50:26.663261+00	f
484	10	09	#majorsystem	soap üßº	384:00:00	2025-05-10 04:18:06.363485+00	f
80	10	User-level threads	#code	User-level threads are typically employed in scenarios where fine control over threading is necessary, but the overhead of kernel threads is not desired. They are also useful in systems that lack native multithreading support, allowing developers to implement threading in a portable way.\n\n### Advantages\n1. Quick and easy to create\n2. Highly portable:   they can be implemented across various operating systems.\n3. No kernel mode privileges required:   context switching can be performed without transitioning to kernel mode.\n\n### Disadvantages \n1. Limited use of multiprocessing: Multithreaded applications may not fully exploit multiple processor cores.\n2. Blocking issues: A blocking operation in one thread can halt the entire process.\n\nhttps://disk.yandex.ru/i/UKuLYIrelmmatg	96:00:00	2025-03-16 07:20:11.625444+00	t
236	10	Programming paradigms	#code	Programming paradigms are fundamental styles or approaches to programming that provide distinct ways to structure and organize code. Each paradigm offers unique principles and techniques for problem-solving, influencing how developers design and implement software.\n\n- Imperative\n    - Procedural\n- OOP\n- Functional\n- Declarative	96:00:00	2025-03-15 17:03:33.233787+00	t
326	10	Add Tow Numbers\n\nYou are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order, and each of their nodes contains a single digit. Add the two numbers and return the sum as a linked list.\n\nYou may assume the two numbers do not contain any leading zero, except the number 0 itself.	#leetcode	/**\n * Definition for singly-linked list.\n * type ListNode struct {\n *     Val int\n *     Next *ListNode\n * }\n */\nfunc addTwoNumbers(l1 *ListNode, l2 *ListNode) *ListNode {\n    dummy := new(ListNode)\n    curr := dummy\n    \n    hold := 0\n    for l1 != nil  l2 != nil  hold != 0 {\n        sum := hold\n\n        if l1 != nil {\n            sum += l1.Val\n            l1 = l1.Next\n        }\n\n        if l2 != nil {\n            sum += l2.Val\n            l2 = l2.Next\n        }\n\n        hold = sum / 10\n        curr.Next = &ListNode{Val: sum % 10}\n        curr = curr.Next\n    }\n\n    return dummy.Next\n}	768:00:00	2025-05-30 14:48:40.307245+00	f
9	10	There is a million ways to be smart.	#waytosuccess	Don't be too hard on yourself and remember your strengths while improving on weaknesses.	72:00:00	2025-03-14 13:30:19.034293+00	t
485	10	Surrounded Regions\n\nYou are given an m x n matrix board containing letters 'X' and 'O', capture regions that are surrounded:\n\nConnect: A cell is connected to adjacent cells horizontally or vertically.\nRegion: To form a region connect every 'O' cell.\nSurround: The region is surrounded with 'X' cells if you can connect the region with 'X' cells and none of the region cells are on the edge of the board.\nTo capture a surrounded region, replace all 'O's with 'X's in-place within the original board. You do not need to return anything.	#leetcode	func solve(board [][]byte)  {\n    var (\n        m = len(board)\n        n = len(board[0])\n        search func(i, j int)\n    )\n\n    if n < 3  m < 3 {\n        return\n    }\n\n    search = func(i, j int) {\n        if i < 0  i >= m  j < 0  j >= n  board[i][j] != 'O' {\n            return\n        }\n\n        board[i][j] = 'E' // for Edge\n        search(i+1, j)\n        search(i-1, j)\n        search(i, j+1)\n        search(i, j-1)\n    }\n\n    for i := range board {\n        for j := range board[i] {\n            if (i == 0  j == 0  i == m-1  j == n-1) && board[i][j] == 'O' {\n                search(i, j)\n            }\n        }\n    }\n\n    for i := range board {\n        for j := range board[i] {\n            if board[i][j] == 'O' {\n                board[i][j] = 'X'\n                continue\n            }\n            if board[i][j] == 'E' {\n                board[i][j] = 'O'\n            }\n        }\n    }\n}	48:00:00	2025-04-28 12:07:36.129893+00	f
19	10	NSDP gives you dopamine	#mindfulness	Non-sleep deep rest practice (aka Yoga Nidra) can increase the baseline level of dopamine by 60%	72:00:00	2025-03-24 08:13:01.339617+00	t
486	10	Number of Islands\n\nGiven an m x n 2D binary grid grid which represents a map of '1's (land) and '0's (water), return the number of islands.\n\nAn island is surrounded by water and is formed by connecting adjacent lands horizontally or vertically. You may assume all four edges of the grid are all surrounded by water.	#leetcode	func numIslands(grid [][]byte) int {\n    var (\n        islands int\n        m = len(grid)\n        n = len(grid[0])\n        outOfBound func(i, j int) bool\n        search func(i, j int)\n    )\n   \n    outOfBound = func(i, j int) bool {\n        return i < 0  i >= m  j < 0 || j >= n\n    }\n   \n    search = func(i, j int) {\n        if outOfBound(i, j) {\n            return\n        }\n        if grid[i][j] != '1' {\n            return\n        }\n        grid[i][j] = '2'\n        search(i+1, j)\n        search(i-1, j)\n        search(i, j+1)\n        search(i, j-1)\n    }\n   \n    for i := 0; i < m; i++ {\n        for j := 0; j < n; j++ {\n            if grid[i][j] == '1' {\n                search(i, j)\n                islands++\n            }\n        }\n    }\n   \n    return islands\n}	48:00:00	2025-04-27 09:11:06.946465+00	f
448	10	—É—â–µ—Ä–±, –≤—Ä–µ–¥	#words	detriment	384:00:00	2025-05-13 14:08:29.353341+00	f
14	10	Tap into maximal awareness.	#mindfulness	Exit your ego and watch yourself from outside. Who is in control of this person? Where do his thoughts come from?	72:00:00	2025-04-28 14:19:59.625388+00	f
17	10	Start every day with cold shower.	#stayhard	Cold shower gives you much prolonged release of dopamine and epinephrine. It also gives you a sense of pride over today's first accomplishment.	144:00:00	2025-03-01 02:07:12.216569+00	t
18	10	"Forward center of mass" without external stimulants.	#mindfulness	You bring yourself to "forward center of mass" state without a huge dose of cofeine or other stimulants. You have control over transition states, just pay attention to them.	72:00:00	2025-03-25 09:59:47.48008+00	t
50	10	Daemon	#code	Daemon is a background process that runs independently of user interaction and provides specific services or functionality to the system or other programs. \n\nDaemons are typically started during the system boot process and continue running in the background, performing tasks such as handling network requests, managing hardware, or performing scheduled operations.	96:00:00	2025-03-20 17:46:23.836455+00	t
6	10	You know exactly what to do, it‚Äôs just sucks doing it.	#stayhard	Pain and suffering will end. You will make it. Your success is well deserved.	96:00:00	2025-04-30 04:40:45.10241+00	f
10	10	Want peace? Go to war with yourself.	#stayhard	Explore and examine the inner side of yourself. Seek out, face, and conquer your daemons. Eliminate your weaknesses.	72:00:00	2025-03-13 13:07:19.656203+00	t
164	10	Lock-free data structures	#code	Lock-free data structures are concurrent data structures designed to allow multiple threads to operate on them without using traditional mutual exclusion locks. \n\nTheir key property is that while individual threads may be delayed, the system as a whole is guaranteed to make progress‚Äîmeaning that at least one thread will always complete its operation within a finite number of steps. \n\nThis is typically achieved by relying on atomic operations such as compare-and-swap (CAS), fetch-and-add, and other hardware-supported atomic primitives.\n\n### Key Characteristics\n1. Non-Blocking:   Unlike traditional lock-based mechanisms, lock-free structures avoid blocking a thread waiting for a lock, reducing risks of deadlock and priority inversion.\n2. Concurrency-Friendly:   They are particularly useful in high-performance and real-time systems where responsiveness and system-wide progress are critical.\n3. Atomicity:   The operations in lock-free structures rely on atomic hardware instructions that ensure correctness even in highly concurrent environments.	96:00:00	2025-03-15 17:00:10.823271+00	t
270	10	Not afraid to be seen looking	#mindfulness	There is nothing wrong with looking at anybody, break this silly habit	48:00:00	2025-04-28 08:45:48.921863+00	f
290	10	Temporary tables	#sql	Temporary tables in SQL are a special type of table designed for short-term data manipulation within a specific database session or transaction. These tables are automatically created and exist only for the duration of the session or transaction in which they are created. Once the session or transaction is completed, the temporary tables are automatically deleted (or "dropped"). Temporary tables are a feature supported by the majority of relational database management systems (RDBMS), including systems like PostgreSQL, MySQL, SQL Server, and Oracle, among others.\n\n### Key Features\n- Store intermediate query results.\n- Boost query speed, especially complex tasks.\n- Isolate data between sessions, preventing conflicts.\n- Reduce locking, improving concurrency.\n- Segment large data for easier processing.\n- Simplify query testing & debugging.\n- Organize schema, avoid clutter. Practical for performance, isolation, and dev tasks.	192:00:00	2025-03-31 03:14:28.180543+00	t
357	10	Load balancing	#code	> Load balancing is the method of distributing workloads across multiple servers or multiple instances of a single server.\n\nLoad balancing addresses several key challenges in distributed systems:\n\n- Workload Distribution: It evenly distributes network traffic or application workload across a pool of servers, ensuring no single server is overwhelmed. This is essentially the core function, ensuring efficient utilization of resources.\n    \n- Fault Tolerance (Resilience): If one server or instance fails or becomes unavailable, the load balancer will automatically stop directing traffic to it. This ensures continuous service availability, even if individual components fail.\n    \n- Scalability: When the demand or load on your application increases, load balancing makes it easy to scale your infrastructure horizontally. You can simply add more servers or instances to the load balancer pool to handle the increased traffic without service disruption.	96:00:00	2025-03-19 16:45:41.690282+00	t
422	10	Every word costs a fortune	#mindfulness	Is it really worth saying? Think twice before opening your mouth.	96:00:00	2025-05-01 15:05:21.966418+00	f
420	10	Never anxious, always aware	#mindfulness		96:00:00	2025-05-02 05:42:00.855948+00	f
414	5	–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö SQL	#sql	*1. –ß–∏—Å–ª–æ–≤—ã–µ —Ç–∏–ø—ã:*\n\n    *–¶–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ:*\n    - SMALLINT: –º–∞–ª—ã–µ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞.\n    - INTEGER –∏–ª–∏ INT: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞.\n    - BIGINT: –±–æ–ª—å—à–∏–µ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞.\n\n    *–ß–∏—Å–ª–∞ —Å –ø–ª–∞–≤–∞—é—â–µ–π –∑–∞–ø—è—Ç–æ–π:*\n    - REAL: —á–∏—Å–ª–∞ —Å –æ–¥–∏–Ω–∞—Ä–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é.\n    - DOUBLE PRECISION: —á–∏—Å–ª–∞ —Å –¥–≤–æ–π–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é.\n\n    *–î–µ—Å—è—Ç–∏—á–Ω—ã–µ —á–∏—Å–ª–∞ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é:*\n    - DECIMAL(p, s) –∏–ª–∏ NUMERIC(p, s): —á–∏—Å–ª–∞ —Å —Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ü–∏—Ñ—Ä, –≥–¥–µ _p_ ‚Äî –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–∏—Ñ—Ä, –∞ _s_ ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–∏—Ñ—Ä –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π.\n\n*2. –°—Ç—Ä–æ–∫–æ–≤—ã–µ —Ç–∏–ø—ã:*\n\n    *–°–∏–º–≤–æ–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã:*\n    - CHAR(n) –∏–ª–∏ CHARACTER(n): —Å—Ç—Ä–æ–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã _n_.\n\n    *–°–∏–º–≤–æ–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã:*\n    - VARCHAR(n) –∏–ª–∏ CHARACTER VARYING(n): —Å—Ç—Ä–æ–∫–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã –¥–æ _n_ —Å–∏–º–≤–æ–ª–æ–≤.\n\n    *–¢–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ:*\n    - TEXT: —Å—Ç—Ä–æ–∫–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª–∏–Ω—ã (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–µ –≤–æ –≤—Å–µ—Ö –°–£–ë–î).\n\n*3. –¢–∏–ø—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞—Ç–æ–π –∏ –≤—Ä–µ–º–µ–Ω–µ–º:*\n\n    - DATE: –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞—Ç—É (–≥–æ–¥, –º–µ—Å—è—Ü, –¥–µ–Ω—å).\n    - TIME: –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≤—Ä–µ–º—è —Å—É—Ç–æ–∫ (—á–∞—Å—ã, –º–∏–Ω—É—Ç—ã, —Å–µ–∫—É–Ω–¥—ã).\n    - TIMESTAMP: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –¥–∞—Ç—ã –∏ –≤—Ä–µ–º–µ–Ω–∏.\n    - INTERVAL: –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫ –≤—Ä–µ–º–µ–Ω–∏.\n\n*4. –î–≤–æ–∏—á–Ω—ã–µ (–±–∏–Ω–∞—Ä–Ω—ã–µ) —Ç–∏–ø—ã:*\n\n    - BINARY(n): –¥–≤–æ–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã _n_.\n    - VARBINARY(n): –¥–≤–æ–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã –¥–æ _n_ –±–∞–π—Ç.\n\n*5. –õ–æ–≥–∏—á–µ—Å–∫–∏–π —Ç–∏–ø:*\n\n    - BOOLEAN: –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è TRUE (–∏—Å—Ç–∏–Ω–∞), FALSE (–ª–æ–∂—å) –∏ UNKNOWN (–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ).\n\n*6. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–∏–ø—ã:*\n\n    - UUID: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä.\n    - ARRAY: –º–∞—Å—Å–∏–≤—ã –∑–Ω–∞—á–µ–Ω–∏–π.\n    - JSON –∏ JSONB: –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.\n    - XML: –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ XML.	64:00:00	2025-04-30 04:35:19.179517+00	f
163	10	Data serialization	#code	Data serialization is the process of converting a data object into a format that can be easily stored or transmitted, typically as a series of bytes. This allows the data to be reconstructed later, making it useful for saving the state of an object or transferring data between different systems.	192:00:00	2025-03-16 22:40:04.003978+00	t
294	10	Indexing with low cardinality	#code	Indexing a column with low cardinality is usually not helpful.  Indexes speed up searches, but with few unique values, the database still has to check many rows.  This negates the index's benefit and adds overhead.  \n\nIndexing *might* help with frequent filtering/sorting or when combined with other filters, but it's often unnecessary and can slow down updates.  Analyze your specific queries to decide.	192:00:00	2025-03-15 11:41:21.91036+00	t
408	5	STAR		–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ STAR –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è—Ö –∏ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫:\n\n    –°–∏—Ç—É–∞—Ü–∏—è (Situation): –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏ –∏–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –≤ –∫–æ—Ç–æ—Ä–æ–º –≤—ã –æ–∫–∞–∑–∞–ª–∏—Å—å.\n    –ó–∞–¥–∞—á–∞ (Task): –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –∏–ª–∏ —Ü–µ–ª–∏, —Å—Ç–æ—è–≤—à–µ–π –ø–µ—Ä–µ–¥ –≤–∞–º–∏ –≤ —ç—Ç–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏.\n    –î–µ–π—Å—Ç–≤–∏–µ (Action): –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –ø—Ä–µ–¥–ø—Ä–∏–Ω—è–ª–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –∏–ª–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏.\n    –†–µ–∑—É–ª—å—Ç–∞—Ç (Result): –æ–ø–∏—Å–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤ –≤–∞—à–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π, –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –≤—ã–≤–æ–¥–æ–≤, —Å–¥–µ–ª–∞–Ω–Ω—ã—Ö –∏–∑ —Å–∏—Ç—É–∞—Ü–∏–∏.	96:00:00	2025-05-02 07:56:51.022853+00	f
15	10	There is zero substitution for hard work.	#waytosuccess	Working smart doesn‚Äôt mean working less. Working hard is the foundation.	96:00:00	2025-04-28 04:01:35.22014+00	f
381	10	Steps to run your application in K8s	#code	To deploy and run a Go application in Kubernetes, you generally need to follow these steps:\n\n1. Create a Docker Image of Your Application: Before deploying to Kubernetes, your Go application needs to be containerized.\n    - Create a Dockerfile for your Go application. This file specifies the base image, copies your application code, builds the Go binary, and defines how to run your application in a container.\n    - Build the Docker image using docker build -t your-dockerhub-username/your-app-name ..\n    - Push the Docker image to a container registry such as Docker Hub, Google Container Registry (GCR), Amazon Elastic Container Registry (ECR), or a private registry using docker push your-dockerhub-username/your-app-name.\n   \n2. Create Kubernetes Manifests: Kubernetes manifests are configuration files (usually in YAML format) that describe the Kubernetes resources required to deploy your application. You'll typically need manifests for:\n    - Deployment: Defines how your application (pods) should be deployed and updated.\n    - Service: Exposes your application to the network, making it accessible within the cluster or externally.\n    - Pods (implicitly created by Deployment): Define the containers that will run your Go application.\n    - ConfigMaps (optional): For managing application configuration separately from the code.\n    - Secrets (optional): For managing sensitive information.\n   \n3. Apply Kubernetes Manifests: Use the kubectl apply command to deploy your application to the Kubernetes cluster.\n    - Example: kubectl apply -f my-app-deployment.yaml (applies the deployment manifest file).\n    - Kubernetes will create the resources defined in your manifests, including deployments, services, and pods, effectively deploying your application.\n   \n4. Monitoring and Logging: Set up monitoring and logging infrastructure to track your application's health, performance, and behavior within Kubernetes.\n    - Popular options include Prometheus for metrics, Grafana for dashboards, and Elasticsearch, Fluentd, and Kibana (EFK stack) or Loki for logging.\n    - Kubernetes itself provides basic monitoring through kubectl get events, kubectl get pods -w, kubectl describe, and kubectl logs.\n\n5. Prepare Environment Settings: Your Go application might require environment configurations, such as environment variables, secrets, or configuration maps.\n    - Define these configurations in your Kubernetes manifests (Deployment, etc.) to be injected into your application containers at runtime.\n\n6. Deploy the Application: After applying the Kubernetes manifests, your application is considered deployed in the cluster.\n    - Kubernetes automatically manages pod lifecycles, scaling based on the deployment configuration, and resource management across the cluster.\n\n7. Testing and Monitoring: After deployment, thoroughly test your application to ensure it functions correctly in the Kubernetes environment.\n    - Set up comprehensive monitoring to track performance metrics, error rates, resource utilization, and identify potential issues or bottlenecks.\n\n8. Updates and Scaling: Kubernetes makes updating and scaling your application straightforward.\n    - Use kubectl set image deployment/my-app your-app-image:new-tag to update the application image.\n    - Use kubectl scale deployment/my-app --replicas=X to adjust the number of application instances (replicas).\n    - Kubernetes rollout mechanisms (e.g., rolling updates) ensure smooth updates with minimal downtime.\n\n9. Backup and Recovery: Implement backup and recovery strategies for your application's persistent data and configuration to ensure resilience and disaster recovery.\n    - Consider strategies for backing up persistent volumes and Kubernetes resource definitions.\n\n10. Support and Maintenance: Continuously support and maintain your application in Kubernetes.\n    - Apply updates to container images, Kubernetes manifests, and cluster configurations as needed.\n    - Address bugs, performance issues, and adapt to changing requirements over time.	96:00:00	2025-03-14 14:41:59.798804+00	t
354	10	Token bucket	#code	There are cases of leaky bucket use with multiple "buckets," where both the size and the rate at which tokens are added to them can vary between individual "buckets." If the first bucket does not have enough tokens to process a request, the second bucket is checked, and so on, but the priority of processing the request is lowered (this is typically used in network interface design, where, for example, the DSCP field value of the processed packet can be changed).\n\nThe key difference from the Leaky Bucket implementation is that tokens can accumulate during system idle time, and a burst load can occur later, where requests will be processed (since there are enough tokens), while the Leaky Bucket will guarantee smooth out the load even during idle time.\n\nhttps://files.bool.dev/site/blog/f2b69526-8bdc-4fe0-9c49-955fc9727fd9/062b8ded-3fa5-4efc-93d7-b53c74ca9501.png)	96:00:00	2025-03-16 18:40:31.235277+00	t
237	10	Imperative programming	#code	Imperative programming is a paradigm where developers write code that explicitly describes the steps a computer must take to achieve a desired state. This approach focuses on _how_ to perform tasks by specifying a sequence of instructions that change the program‚Äôs state. Languages like C and Java are commonly used for imperative programming.\n\nhttps://pictures.s3.yandex.net/resources/Untitled_4_1701722188.png	192:00:00	2025-03-22 10:01:50.881931+00	t
246	10	Spot a race condition in Golang	#code	Race conditions in Go occur when multiple goroutines access shared data concurrently, leading to unpredictable outcomes.  \n\n### How to spot race conditions\n1. The Go race detector (go run -race) is the most effective tool for finding them.  It instruments code and reports conflicting accesses.  \n\n2. Static analysis tools and code review can also help, though they are less reliable.  \n\n3. Testing, especially stress testing, increases the chance of triggering races.  \n\nKey synchronization primitives include mutexes, channels, atomic operations, and wait groups.  Always use the race detector and consider concurrency carefully when writing Go code.	192:00:00	2025-03-15 06:27:33.065294+00	t
245	10	Golang Scheduler	#code	Go's scheduler is a cooperative M:N scheduler. It multiplexes many goroutines (lightweight threads) onto a smaller number of OS threads. \n\nKey components are:\n- Goroutines (G), \n- Machines (M, OS threads), \n- Processors (P, logical processors).  \n\nEach P has a local run queue. Ms execute Gs from Ps' queues, using _work stealing_ to balance load. \n\nContext switching is fast, managed by the runtime.  Blocking system calls don't block all Gs. \n\nThe scheduler enables efficient concurrency and scalability.	96:00:00	2025-03-18 03:54:10.169803+00	t
127	10	forgery	#words	1. The act of forging something, especially the unlawful act of counterfeiting a document or object for the purposes of fraud or deception. \n2. Something that has been forged, especially a document that has been copied or remade to look like the original.\n3. The act of forging metal into shape. \n\nForgery is a white-collar crime that generally consists of the false making or material alteration of a legal instrument with the specific intent to defraud. *Wikipedia	384:00:00	2025-05-09 06:29:45.224717+00	f
456	10	–∑–∞–∫—Ä–µ–ø–ª—è—Ç—å, —É–≤–µ–∫–æ–≤–µ—á–∏–≤–∞—Ç—å	#words	perpetuate	192:00:00	2025-04-30 15:13:36.943659+00	f
419	10	Concurrent map	#code	func NewConcurrentMap() *ConcurrentMap {\n  return &ConcurrentMap{\n    m:  make(map[string]any),\n    rw: &sync.RWMutex{},\n  }\n}\n\nfunc (cm *ConcurrentMap) GetOrCreate(key string, value any) any {\n  cm.rw.RLock()\n  v, ok := cm.m[key]\n  cm.rw.RUnlock()\n\n  if ok {\n    return v\n  }\n\n  cm.rw.Lock()\n  defer cm.rw.Unlock()\n  if v, ok := cm.m[key]; ok {\n    return v\n  }\n\n  cm.m[key] = value\n\n  return value\n}	24:00:00	2025-03-14 17:43:09.417618+00	t
125	10	eavesdropping	#words	1. The habit of lurking about dwelling houses, and other places where persons meet for private intercourse, secretly listening to what is said, and then tattling it abroad. The offense is indictable at common law. \n2. Listening secretly to private conversation of others. \n3. The interception of electronic communication.	384:00:00	2025-04-05 11:13:36.288073+00	t
351	10	Quadree	#code	Let‚Äôs consider this algorithm with an example: finding the nearest restaurants on Yelp or Google Maps.\n\nA Quadtree is a data structure typically used to partition a two-dimensional space by recursively dividing it into four quadrants (grids) until the contents of the grids meet certain criteria (see the image below).\n\nA Quadtree is an in-memory data structure, not a database solution. It operates on each LBS (Location-Based Service) server, and the data structure is created during server startup.\n\n### Complexity\n- Time Complexity:\n  - Find: O(log‚ÇÇN)\n  - Insert: O(log‚ÇÇN)\n  - Search: O(log‚ÇÇN)\n- Space Complexity:\n  - O(k log‚ÇÇN)\n\nhttps://files.bool.dev/site/blog/f2b69526-8bdc-4fe0-9c49-955fc9727fd9/958c9e46-b75e-463d-b533-afad27279881.png	96:00:00	2025-03-15 16:59:37.291813+00	t
162	10	slices.SortStableFunc	#golang	slices.SortStableFunc sorts the slice x while keeping the original order of equal elements, using cmp to compare elements. In many situations, it is more ergonomic and runs faster than the older sort.SliceStable	192:00:00	2025-04-29 16:16:37.90864+00	f
435	10	Flatten Binary Tree to Linked List\n\nGiven the root of a binary tree, flatten the tree into a "linked list":\n- The "linked list" should use the same TreeNode class where the right child pointer points to the next node in the list and the left child pointer is always null.\n- The "linked list" should be in the same order as a pre-order traversal of the binary tree.	#leetcode	func flatten(root *TreeNode) {\n    for curr := root; curr != nil; curr = curr.Right {\n        if curr.Left != nil {\n            pre := curr.Left\n            for pre.Right != nil {\n                pre = pre.Right\n            }\n            pre.Right = curr.Right \n            curr.Right = curr.Left\n            curr.Left = nil\n        }\n    }\n}	192:00:00	2025-05-09 08:13:09.038776+00	f
436	10	Path Sum\n\nGiven the root of a binary tree and an integer targetSum, return true if the tree has a root-to-leaf path such that adding up all the values along the path equals targetSum.\n\nA leaf is a node with no children.	#leetcode	func hasPathSum(node *TreeNode, target int) bool {\n  if node == nil {\n    return false\n  }\n\n  if node.Left == nil && node.Right == nil {\n    return node.Val == target\n  }\n\n  return hasPathSum(node.Left, target-node.Val) ||\n    hasPathSum(node.Right, target-node.Val)\n}	96:00:00	2025-04-29 13:20:01.391861+00	f
133	10	Never hurry	#mindfulness	no prompt	96:00:00	2025-04-28 04:41:33.577411+00	t
463	10	consist of; be made up of	#words	comprise	384:00:00	2025-05-04 04:02:31.26044+00	f
30	10	Product of Array Except Self	#leetCode	func productExceptSelf(nums []int) []int {\n  n := len(nums)\n\n  prefix := make([]int, n)\n  suffix := make([]int, n)\n  result := make([]int, n)\n\n  prefix[0] = 1\n  for i := 1; i < n; i++ {\n    prefix[i] = prefix[i-1] * nums[i-1]\n  }\n\n  suffix[n-1] = 1\n  for i := n - 2; i >= 0; i-- {\n    suffix[i] = suffix[i+1] * nums[i+1]\n  }\n\n  for i := range result {\n    result[i] = prefix[i] * suffix[i]\n  }\n\n  return result\n}	384:00:00	2025-05-13 16:43:27.324191+00	f
157	10	Slice	#code	Slice is a data structure consisting of 3 fields:\n1. Pointer to the first element of the base array;\n2. Length of the slice, which determines location of the last element;\n3. Capacity of the slice, which is the length of the base array;\n\n> When passing a slice to a function, the function receives a copy of the SliceHeader structure for the slice, but not the slice itself. Therefore if the function needs to change slice parameters, the argument must be a pointer to the slice or return a new slice.\n\ntype slice struct {\n    array unsafe.Pointer\n    len   int\n    cap   int\n}	192:00:00	2025-03-14 16:51:58.317719+00	t
24	10	Busy waiting	#code	Continuously testing a variable until some value appears.\n\nA  lock that uses busy waiting is called a spin lock.	192:00:00	2025-03-18 03:56:06.503978+00	t
25	10	H-index\n\nGiven an array of integers citations where citations[i] is the number of citations a researcher received for their ith paper, return the researcher's h-index.\n\nAccording to the definition of h-index on Wikipedia: The h-index is defined as the maximum value of h such that the given researcher has published at least h papers that have each been cited at least h times.	#leetCode	func hIndex(citations []int) int {\n    sort.Ints(citations)\n    \n    n := len(citations)\n    for i, v := range citations {\n        if v >= n - i {\n            return n - i\n        }\n    }\n\n    return 0	768:00:00	2025-05-17 03:59:42.019572+00	f
383	10	CI/CD	#cicd	CI/CD stands for Continuous Integration and Continuous Delivery/Continuous Deployment. It is a methodology and a set of practices used in software development to automate the processes of code integration, testing, and deployment. CI/CD enables developers to deliver new application versions quickly and reliably.\n\nImplementing CI/CD offers several advantages, including improved code quality, faster development cycles, fewer production errors, and more reliable releases. The cornerstone of successful CI/CD implementation is the automation of all stages, from integration to deployment, along with the use of specialized tools to manage the process.\n\nA variety of tools are available for implementing CI/CD pipelines. Popular examples include Jenkins, Travis CI, CircleCI, GitLab CI/CD, and TeamCity, among others. The choice of tools often depends on project requirements, team preferences, and existing infrastructure.	192:00:00	2025-04-05 12:27:38.28606+00	t
32	10	Operating Systems History	#code	1. Early Operating Systems:\n   - 1950s-1960s: Batch processing systems like IBM's OS/360 and UNIVAC's EXEC 8.\n   - 1960s-1970s: Mainframe operating systems like IBM's OS/360 and Unix.\n   - 1970s: Personal computer operating systems like CP/M, Apple DOS (Disc Operating System), and early versions of Microsoft's MS-DOS.\n\n2. Modern Operating Systems:\n   - 1980s: Graphical user interface (GUI) operating systems like Apple's Macintosh OS and Microsoft's Windows.\n   - 1990s: Widespread adoption of Windows and the rise of Unix-based systems like Linux and macOS.\n   - 2000s-present: Continued evolution of Windows, macOS, and Linux, as well as the emergence of mobile operating systems like iOS and Android.	192:00:00	2025-03-14 16:32:42.999354+00	t
190	10	Composition	#code	#composition is a mechanism that allows you to create complex objects by combining simpler objects.\n\nIn Golang, composition is achieved by embedding one structure in the field of another structure. This allows you to create objects consisting of multiple components that can interact with each other.	384:00:00	2025-03-31 04:04:33.444024+00	t
11	10	Jump Game\n\nYou are given an integer array. You are initially positioned at the array's first index, and each element in the array represents your maximum jump length at that position.\n\nReturn true if you can reach the last index, or false otherwise.	#leetCode	## Max Index\nfunc canJump(nums []int) bool {\n  if len(nums) <= 1 {\n    return true\n  }\n\n  max := 0\n  for i := range nums {\n    if i > max {\n      return false\n    }\n\n    if i+nums[i] > max {\n      max = i + nums[i]\n    }\n  }\n\n  return true\n}\n\n## Walk Backwards\nfunc canJump(nums []int) bool {\n  if len(nums) <= 1 {\n    return true\n  }\n\n  needs := 1\n  for i := len(nums) - 2; i > 0; i-- {\n    if nums[i] >= needs {\n      needs = 1\n      continue\n    }\n    needs++\n  }\n\n  return nums[0] >= needs\n}	384:00:00	2025-04-28 15:33:03.606632+00	f
293	10	AutoVacuum	#postgres	Autovacuum is a crucial background process in PostgreSQL that helps maintain the health and efficiency of your database. Think of it as a diligent housekeeper that keeps things tidy and running smoothly. Here's a breakdown of its key functions:\n\n1. Removing obsolete data:\n    - When you delete data in PostgreSQL, it's not immediately erased from the database. Instead, it's marked as "dead" but remains physically present.\n    - Autovacuum steps in to permanently remove these dead rows, freeing up valuable storage space.\n\n2. Reclaiming disk space:\n    - Updates and deletions can leave behind "garbage" in the database, like unused disk space.\n    - Autovacuum cleans up this mess, making the space available for new data.\n\n3. Preventing fragmentation:\n    - Over time, tables and indexes can become fragmented, which can slow down queries.\n    - Autovacuum helps prevent this fragmentation, ensuring that data is stored efficiently.\n\n4. Updating statistics:\n    - PostgreSQL's query planner relies on statistics about the data to make informed decisions about how to execute queries.\n    - Autovacuum keeps these statistics up-to-date, leading to better query performance.\n\nIn essence, autovacuum is like an automatic maintenance worker that ensures your PostgreSQL database remains optimized for performance and space utilization. It runs in the background, continuously working to keep things in order.	192:00:00	2025-05-04 03:10:51.621476+00	t
27	10	Mutex	#code	A mutex (mutual exclusion) is a synchronization primitive used to protect shared resources. It is a shared variable that can be in one of two states: unlocked or  locked. \nOnly 1 bit is required to represent it, but in practice an integer often is used, with 0 meaning unlocked and all other values meaning locked.	384:00:00	2025-04-05 08:15:55.841932+00	t
453	10	lacking any obvious principle or organization	#words	haphazard	768:00:00	2025-06-02 11:03:06.630446+00	f
224	10	Memento	#code	A #behavioral design pattern that allows making snapshots of an object's state and restoring it in the future.\n\nThe #memento pattern lets us save snapshots of an object‚Äôs state. You can use these snapshots to revert the object to the previous state. It‚Äôs handy when you need to implement undo-redo operations on an object. \n\nThe Memento doesn't compromise the internal structure of the object it works with, as well as data kept inside the snapshots.\n\nhttps://disk.yandex.ru/i/ARRYFycewYHw_w	192:00:00	2025-03-21 22:16:05.162174+00	t
12	10	Bring your mind and body to desirable state by exhibiting matching behaviour.	#mindfulness	Big warm smile, eyes wide open, shoulders back, head is up, bounce in your step, etc. \nImagine something exciting is about to happen, or you're going to see the loved one very soon.	96:00:00	2025-03-18 03:54:15.937064+00	t
241	10	Idiom	#code	In programming, an idiom refers to a commonly used pattern or expression that is natural and efficient within a particular programming language. These idioms leverage the unique features and conventions of the language to solve recurring problems or perform frequent tasks in a way that experienced developers find familiar and readable.	96:00:00	2025-03-15 07:20:26.310631+00	t
22	10	Race condition	#code	Situations where two or more processes are reading or writing some shared data and the final result depends on who runs precisely when, are  called race conditions.\n\nMutual exclusion is when reading and writing the shared data by more than one process at the same time is prohibited.\n\nThe part of the program where the shared memory is accessed is called the critical  region or critical section.	192:00:00	2025-03-19 07:59:28.957381+00	t
39	10	Monolithic systems	#code	The monolithic approach is to run the entire operating system as a single program in kernel mode. The operating system is written as a collection of procedures, linked together into a single large executable binary program. Each procedure in the system is free to call any other one, which is very efficient but difficult to develop and maintain. Also, failure in one of the procedures can bring the whole system down.	384:00:00	2025-04-02 06:21:49.938086+00	t
189	10	Polymorphism	#code	#polymorphism is the ability of objects with the same specification to exhibit different behavior.  \n\nIn Golang, polymorphism is achieved through interfaces. An interface defines a set of methods to be implemented by classes, but does not define their specific implementation. This allows interchangeable use of objects of different classes implementing the same interface.	384:00:00	2025-03-28 08:53:27.358401+00	t
340	10	Don‚Äôt allow doubt	#mindfulness	Trust the decision you‚Äôve made	96:00:00	2025-04-27 18:34:06.192207+00	f
202	10	Concurrency vs parallelism	#code	#parallelism is the simultaneous execution of multiple entities of some kind, whereas #concurrency is a way of structuring your components so that they can be executed independently when possible.	96:00:00	2025-03-15 04:12:25.087959+00	t
467	10	Feeling instinct to act? \nMove immediately!	#waytosuccess	Move before fear has a chance to paralyze you. Hesitation is the language of weakness. Action is the language of transformation.	192:00:00	2025-04-28 04:21:29.673206+00	f
457	10	a person who deserts and betrays an organization, country, or set of principles	#words	renegade	384:00:00	2025-05-09 07:29:28.326728+00	f
126	10	tampering	#words	The act of altering something secretly or improperly.	192:00:00	2025-04-27 08:47:22.880288+00	f
175	10	KISS	#code	Keep It Simple, Stupid is a programming principle that calls for creating simple and understandable code. Complexity should be kept at minimum to make the program easier to understand and maintain.\n\nInstead of building complex and overly clever/fancy solutions, we should favor simplicity and clarity. Simple code is easier to test, debug and extend. It also facilitates communication between developers and increases the probability of successful project transfer from one developer to another.	192:00:00	2025-03-22 04:42:25.696397+00	t
223	10	Observer	#code	A #behavioral design pattern that allows some objects to notify other objects about changes in their state.\n\nThe #observer pattern provides a way to subscribe and unsubscribe to and from events for any object that implements a subscriber interface.\n\nhttps://disk.yandex.ru/i/Un_xdxq4gTRd8g	96:00:00	2025-03-16 13:37:05.034572+00	t
120	10	Two sum II\n\nGiven a 1-indexed array of integers numbers that is already sorted in non-decreasing order, find two numbers such that they add up to a specific target number. Let these two numbers be numbers[index1] and numbers[index2] where 1 <= index1 < index2 <= numbers.length.\n\nReturn the indices of the two numbers, index1 and index2, added by one as an integer array [index1, index2] of length 2.\n\nThe tests are generated such that there is exactly one solution. You may not use the same element twice.\n\nYour solution must use only constant extra space.	#leetcode	func twoSum(numbers []int, target int) []int {\n    i, j := 0, len(numbers)-1\n    \n    for i < j {\n        a, b := numbers[i], numbers[j]\n\n        if a + b == target {\n            break\n        }\n\n        if a + b > target {\n            j--\n        } else {\n            i++\n        }\n\n    }\n\n    return []int{i+1, j+1}\n}	384:00:00	2025-05-11 11:27:09.997351+00	f
144	10	Alpha specimen behaviour	#mindfulness	They don‚Äôt respect you, because they don‚Äôt feel your eyes on them. You are overwhelmed with how you look, and they see and feel that. \n\nInstead, your attention must be on how they behave, how they look and feel. This is the type of behaviour of a dominant, alpha specimen. \n\nAll you need to access respect, influence, and power is to practice it.	48:00:00	2025-03-23 15:05:52.128988+00	t
139	10	Docker	#code	Docker is a platform for building, shipping, and running applications in isolated containers, ensuring consistency across different environments.\n\nIts main idea is to create a standard and predictable environment where applications can run independently of the operating system or infrastructure. \n\nDocker is widely used in software development, DevOps, and IT infrastructure management. Thanks to it, you can speed up development and simplify the transfer of applications between environments.	192:00:00	2025-03-25 10:11:46.519231+00	t
16	10	Jump Game II\n\nYou are given a 0-indexed array nums of integers of length n. Each element nums[i] represents the maximum length of a forward jump from index i.\n\nReturn the minimum number of jumps to reach nums[n - 1]. The test cases are generated such that you can reach nums[n - 1].	#leetCode	func jump(nums []int) int {\n    jumps, maxDist, pos := 0, 0, 0\n    for i := 0; i < len(nums)-1; i++ {\n        maxDist = max(maxDist, nums[i]+i)\n        if i == pos {\n            jumps++\n            pos = maxDist\n        }\n    }\n    return jumps\n}	768:00:00	2025-06-02 04:04:58.968982+00	f
393	10	Advanced git commands	#git	- git rebase:   \n  Rebase is used to move, combine, or modify commits in your history, helping you create a cleaner and more understandable commit log. However, be cautious when rebasing public branches, as rewriting history can lead to conflicts.\n\n- git cherry-pick:   \n  This command lets you select and apply a specific commit from one branch to another. It‚Äôs useful when you want to incorporate only a particular change from one branch without merging the entire branch.\n\n- git bisect:   \n  The bisect command helps identify the exact commit that introduced a bug or issue. By marking a known ‚Äúgood‚Äù version and a ‚Äúbad‚Äù version, Git performs a binary search through the commit history to pinpoint the problematic commit.\n\n- git stash:   \n  Stash temporarily saves your uncommitted changes without creating a commit. This is handy when you need to switch to another branch or work on a different task without committing unfinished work.\n\n- git reflog:   \n  Reflog records changes to the tip of branches and can be used to view and recover commits that were deleted or lost. It‚Äôs especially useful for tracking your repository‚Äôs activity and undoing mistakes.\n\n- git filter-branch:   \n  This command lets you perform bulk modifications to your commit history. It‚Äôs often used for tasks like removing sensitive data from your repository history. (Note: For very large repositories or more efficient operations, consider using newer tools like git filter-repo.)\n\n- git submodule:   \n  Git submodules allow you to embed one Git repository within another. This is useful when you want to include external dependencies or libraries as part of your project.\n\n- git worktree:   \n  The worktree command enables you to have multiple working copies of a single repository. This is convenient if you want to work on different branches simultaneously without switching contexts in one working directory.	96:00:00	2025-03-30 12:54:55.346594+00	t
160	10	Hash table	#code	Hash function is a mapping from a set A of any size to a fixed-size set B.\n\nh:  A ‚Üí B\n\nIn other words, a hash function is an algorithm that takes an input of any size and converts it into a fixed-size output, called a #hash value. \n\nThe key characteristics of a good hash function:\n1. Determinism;\n2. Uniform distribution of hash values;\n3. Effectiveness (quick to compute); \n4. Limited range of hash values.\n\nHash table is a data structure that stores on or more key-value pairs and uses a _hash function_ to compute an index into an array of buckets or slots, from which the correct value can be retrieved. 	96:00:00	2025-03-18 14:31:45.343919+00	t
37	10	conformant	#words	—Å–æ–æ—Ç–≤–µ—Å—Ç–≤—É—é—â–∏–π, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π	72:00:00	2025-02-13 06:16:03.904641+00	t
134	10	Container with most water\n\nYou are given an integer array height of length n. There are n vertical lines drawn such that the two endpoints of the ith line are (i, 0) and (i, height[i]).\n\nFind two lines that together with the x-axis form a container, such that the container contains the most water.\n\nReturn the maximum amount of water a container can store.\n\nNotice that you may not slant the container.	#code	func maxArea(height []int) int {\n\n    maximumArea, i, j := 0, 0, len(height)-1\n    for i < j {\n        a, b := height[i], height[j]\n        area := min(a, b) * (j - i)\n\n        maximumArea = max(area, maximumArea)\n\n        if a < b {\n            i++\n        } else {\n            j--\n        }\n    }    \n\n    return maximumArea\n}	384:00:00	2025-03-21 12:02:14.957518+00	t
191	10	Encapsulation	#code	#encapsulation is the principle by which data and the methods that work with that data are combined into a single object.  \n\nIn Golang, encapsulation is achieved through the use of public and private fields and methods. Public fields and methods are accessible from outside the object, while private fields and methods are only accessible inside the object. This allows controlled access to data and hides it from direct interaction.	192:00:00	2025-03-22 14:53:39.627066+00	t
167	10	Futex	#code	A futex (short for ‚Äúfast userspace mutex‚Äù) isn‚Äôt a full-blown library in the usual sense but rather a low-level kernel mechanism that many higher-level synchronization libraries rely on. Essentially, a futex allows a program to perform most locking operations entirely in user space using atomic operations‚Äîand only makes a system call when there‚Äôs contention (i.e. when a thread must block or be woken up). This design minimizes costly context switches in the common, uncontended case.	96:00:00	2025-03-15 13:58:42.515007+00	t
311	10	Relationship types	#sql	Relationships between tables are used to organize and link data across different entities. There are several types of relationships you can utilize, depending on your data structure and project requirements:\n\n1. One-to-One:  Each record in one table corresponds to exactly one record in another table.\n\nTables "Users" and "Profiles". Each user has one profile, and each profile belongs to only one user.\n    \n2. One-to-Many:  One record in one table is linked to multiple records in another table.\n\nTables "Orders" and "Order Items". One order can contain multiple order items.\n    \n3. Many-to-One:  Multiple records in one table are linked to a single record in another table. This is actually the same relationship as One-to-Many, just viewed from the opposite table's perspective. \n\nTables "Products" and "Categories". Multiple products can belong to one category.\n\n4. Many-to-Many:   Multiple records in one table are linked to multiple records in another table. This relationship usually requires a junction table (also known as a linking or associative table) to implement it in a relational database.\n\nTables "Students" and "Courses". Many students can enroll in many courses, and conversely, a course can have many students. A junction table, perhaps named "Enrollments", would typically link Students and Courses.\n    \n5. Self-Referencing:  A table is related to itself.\n\nTable "Employees" with a field "Manager," where each employee can be the manager of other employees.\n    \nCommon applications include organizational hierarchies (employee reporting structures), category hierarchies (parent and child categories), or any structure where entities can be nested within themselves.	96:00:00	2025-03-19 13:06:39.272818+00	t
468	10	think about something carefully, especially making a decision	#words	to ponder	192:00:00	2025-05-01 06:41:42.621035+00	f
313	10	Migrations	#code	A database migration usually refers to the process of modifying a database‚Äôs structure‚Äîits schema‚Äîrather than just moving data from one server or database to another. In modern development, migrations are typically scripted changes that version and evolve your schema over time. This might include adding or renaming columns, creating new tables, updating indexes, or even transforming data to fit new structural requirements.\n\nThat said, the term ‚Äúmigration‚Äù can also describe the broader task of transferring data from one system to another. However, when developers talk about database migrations, they‚Äôre generally referring to schema changes that are applied incrementally and managed through migration tools, ensuring that the database structure stays in sync with the application‚Äôs evolving needs.	96:00:00	2025-03-15 16:10:27.494839+00	t
49	10	Files on Linux	#code	On Linux, a file is a fundamental concept that represents a collection of data or information stored on a storage device (such as a hard drive, SSD, or USB drive). Files are used to store everything from documents and images to executable programs and system configurations. Linux treats almost everything as a file, including hardware devices, directories, and even processes, following the philosophy that ‚Äúeverything is a file.‚Äù\n\n### Key Characteristics\n1. Name: Each file has a name that identifies it within its directory.\n2. Type: Files can be of different types, such as regular files, directories, symbolic links, device files, etc.\n3. Permissions: Files have permissions that determine who can read, write, or execute them.\n4. Metadata: Files have associated metadata, such as size, ownership, timestamps (creation, modification, and access times), and more.\n5. Inode (index node): Each file is associated with an inode, a data structure that stores metadata and pointers to the file's data blocks on the disk.\n\n### Types of Files\n1. Regular Files:\n   - These are the most common type of files, containing data such as text, images, or executable code.\n   - Examples: .txt, .jpg, .sh, .pdf.\n\n2. Directories:\n   - Special files that act as containers for other files and directories.\n   - They store references to the files and subdirectories they contain.\n   - Example: /home, /etc.\n\n3. Symbolic Links (Symlinks):\n   - Files that act as pointers or shortcuts to other files or directories.\n   - Example: A symlink pointing from /home/user/docs to /var/docs.\n\n4. Device Files:\n   - Represent hardware devices, such as hard drives, keyboards, or printers.\n   - Located in the /dev directory.\n   - Types:\n     - Character Device Files: Handle data one character at a time (e.g., keyboards).\n     - Block Device Files: Handle data in blocks (e.g., hard drives).\n\n5. Named Pipes (FIFOs):\n   - Special files used for inter-process communication (IPC).\n   - Data written to a named pipe by one process can be read by another process.\n\n6. Sockets:\n   - Files used for network communication between processes, either on the same machine or across a network.\n\n7. Special Files:\n   - Files that provide access to kernel functionality or system resources.\n   - Examples: /proc (process information), /sys (kernel and system information).\n\n### File Structure\n- Files are organized in a hierarchical directory structure, starting from the root directory (/).\n- Each file has a unique path that describes its location in the directory tree.\n- Example: /home/user/documents/report.txt.\n\n### File Operations\nCommon operations on files include:\n- Creating: touch filename\n- Reading: cat filename\n- Writing: echo "text" > filename\n- Deleting: rm filename\n- Moving/Renaming: mv oldname newname\n- Copying: cp source destination\n- Changing Permissions: chmod permissions filename\n- Changing Ownership: chown user:group filename\n\n### File Metadata\n- You can view file metadata using commands like ls -l, stat, or file.\n- Example: ls -l filename shows permissions, ownership, size, and timestamps.\n\nIn summary, a file on Linux is a versatile entity that can represent data, directories, devices, or even system resources. The ‚Äúeverything is a file‚Äù philosophy simplifies interactions with the system, making it consistent and powerful.	192:00:00	2025-03-18 16:33:37.181323+00	t
95	10	Is Subseqeuence\n\nGiven two strings s and t, return true if s is a subsequence of t, or false otherwise.\n\nA subsequence of a string is a new string that is formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (i.e., "ace" is a subsequence of "abcde" while "aec" is not).	#leetcode	func isSubsequence(s string, t string) bool {\n    if len(t) < len(s) {\n        return false\n    }\n\n    i, j := 0, 0\n    for i < len(s) && j < len(t) {\n        if s[i] == t[j] {\n            i++\n        }\n        j++\n    }\n    \n    return i == len(s)\n}	384:00:00	2025-05-07 18:37:55.3979+00	f
388	10	Release/deployment strategies	#cicd	### Standard Release/Deployment\nThis is a traditional software release where the software is made immediately available to all users upon deployment.\n    \n### Canary Release\nUsed when there's a higher risk of introducing critical issues. The new release is initially rolled out to a small subset of users (e.g., 1%). Developers then gradually increase the user base exposed to the new release while monitoring for issues before a full rollout.\n    \n### Blue-Green Deployment\nThis strategy involves running two identical production environments, ‚Äúblue‚Äù (the current stable version) and ‚Äúgreen‚Äù (the new release). All user traffic is switched from ‚Äúblue‚Äù to ‚Äúgreen‚Äù at once. This approach offers safer deployments and provides a quick rollback option to the previous version (‚Äúblue‚Äù) if problems arise in the new ‚Äúgreen‚Äù environment.\n    \n### Dark Launch\nNew features are released to the production environment without any prior announcement or immediate visibility to users. Features are gradually activated as needed, often using feature flags or toggles. This allows for testing in production with real user load without affecting the majority of users and allows for controlled feature rollouts.	96:00:00	2025-03-27 14:38:06.306856+00	t
143	10	Network protocol	#code	A network protocol is a set of rules that defines the principles of interaction between devices on a network. For sending and receiving information to be successful, all participating devices must accept and follow the protocol's conditions. In the network, their support is built into either the hardware or the software part of the system, or both.	192:00:00	2025-03-21 04:39:21.819995+00	t
56	10	Real-time operating system	#code	If the action absolutely must occur at a certain moment (or within a certain range), we have a hard real-time system. Many of these are found in industrial process control, avionics, military, and similar application areas. These systems must provide absolute guarantees that a certain action will occur by a certain time.\n\nA soft real-time system is one where missing an occasional deadline, while not desirable, is acceptable and does not cause any permanent damage. Digital audio or multimedia systems fall in this category. Smartphones are also soft realtime systems.	8760:00:00	2026-07-31 03:45:04.268178+00	t
57	10	Server operating systems	#code	They run on servers, which  are either very large personal computers, workstations, or even mainframes. They serve multiple users at once over a network and allow the users to share hardware and software resources. \n\nServers can provide print service, file service, database service, or Web service. \n\nTypical server operating systems are Linux, FreeBSD, Solaris, and the Windows Server family.	96:00:00	2025-03-17 11:14:19.225171+00	t
220	10	Template Method	#code	A #behavioral design pattern that allows you to define a skeleton of an algorithm in a base class and let subclasses override the steps without changing the overall algorithm's structure.\n\n#polymorphism \n\nhttps://disk.yandex.ru/i/FngffmM3jR3CfQ	192:00:00	2025-03-18 08:37:18.472899+00	t
28	10	Interview questions asked to learn more about candidate, not to bring them down.	#waytosuccess	We often make the mistake of perceiving an interview as an exam and forgetting that the interviewer is interested in a positive outcome, often even more than we are. The KPI of the recruiter, and sometimes even the team lead, is tied to filling vacancies, and if you‚Äôve made it to the technical interview, all participants are on your side. Use this to your advantage!	96:00:00	2025-02-14 04:44:21.942145+00	t
172	10	Database schema vs ORM model	#code	A database schema is the formal blueprint of a database that defines its structure‚Äîwhat tables exist, what columns each table contains, the data types, constraints (like primary keys, foreign keys, and unique constraints), and the relationships between tables. It‚Äôs essentially the design that tells the database engine how to store and manage the data.\n\nIn contrast, an ORM (Object‚ÄìRelational Mapping) model is an abstraction layer in your application code that represents those database structures as objects (typically classes) in an object‚Äëoriented programming language. The ORM model maps the tables and columns defined in the database schema to class attributes and objects, often incorporating additional behavior (methods, validations, computed properties) that makes it easier for developers to work with data without writing raw SQL.	96:00:00	2025-03-16 04:00:40.157773+00	t
219	10	Group Anagrams\n\nGiven an array of strings strs, group the anagrams together. You can return the answer in any order.	#leetcode	func groupAnagrams(strs []string) [][]string {\n    mp := make(map[string][]string, len(strs))\n    for _, str := range strs {\n        bts := []byte(str)\n        slices.Sort(bts)\n        key := string(bts)\n        mp[key] = append(mp[key], str)\n    }\n\n    rv := make([][]string, 0, len(strs))\n    for _, v := range mp {\n        rv = append(rv, v)\n    }\n\n    return rv\n}	768:00:00	2025-06-13 03:47:08.325074+00	f
316	10	Does the order of fields in a composite PostgreSQL index matter?	#sql	Yes, it does. In PostgreSQL, the order of columns in a composite index is crucial and can significantly impact query performance. The sequence determines how the data is organized within the index, which in turn affects how efficiently the index can be used during query execution.\n\nWhen you create a composite index on multiple columns, the arrangement of these columns matters. For instance, if you have a table with columns "A" and "B" and you build an index ordered as "A, B," the index will first sort by "A" and then by "B." Reversing the order to "B, A" means the sorting will prioritize "B" first.\n\nChoosing the right order depends on your most common queries. If your queries frequently filter or sort by column "A," then an "A, B" index might be more effective. Conversely, if column "B" is more often used in filtering or sorting, a "B, A" index could yield better performance.\n\nMoreover, the ordering is important when using comparison operators such as BETWEEN, >, and <. A well-planned column order in your index can substantially speed up queries that involve these operators.	192:00:00	2025-04-01 13:58:29.837305+00	t
228	10	Chain of Responsibility	#code	A #behavioral design pattern that allows passing requests along the chain of potential handlers until one of them handles the request.\n\nThis pattern allows multiple objects to handle the request without coupling the sender class to the concrete classes of the receivers. The chain can be composed dynamically at runtime with any handler that follows a standard handler interface.\n\nhttps://disk.yandex.ru/i/RGxusmhRvTtNuQ	96:00:00	2025-03-21 04:27:53.21115+00	t
335	10	Kafka semantics	#kafka	### At Most Once \nIn this mode, the producer considers a message ‚Äúdelivered‚Äù the moment it is sent to the broker‚Äîregardless of whether the broker successfully passes it on to the consumer. This means that if the broker fails to forward the message, it might be lost. Although this approach allows for maximum throughput, it does not guarantee that every message will eventually reach its destination.\n\n### At Least Once\nHere, the producer waits for an acknowledgment (ACK) from the broker confirming that the message has been delivered. If the acknowledgment isn‚Äôt received within a set timeframe, the producer resends the message. This ensures that every message will eventually be delivered, but it can also result in the same message being delivered more than once in the case of retries or system failures.\n\n### Exactly Once \nThis mode guarantees that each message is delivered a single time‚Äîno more, no less. To achieve this, the producer sends messages as part of a transaction. The broker only confirms the message once it is safely logged and forwarded to the consumer. While this provides the highest delivery guarantee, it involves additional coordination mechanisms that can reduce throughput.  \n\n> Exactly once delivery is extremely resource intensive and comes with certain risks. For example, you might experience a block during sending because data replication fails due to a faulty replica. Alternatively, even after the message is processed, an unexpected situation might prevent the delivery acknowledgment from reaching the producer, resulting in a duplicate. In practice, pursuing exactly once delivery is not recommended unless absolutely necessary.	96:00:00	2025-03-21 07:24:38.846675+00	t
67	10	What is an operating system	#code	An operating system is a software layer that\n1. provides user programs with abstraction of computer hardware and \n2. manages computer resources such as CPU, memory allocation, I/O and other devices.\n\nOS provides a platform for user programs to run. It coordinates the use of hardware and ensures that all processes get their fair access to system resources. It is the one program that is running all the time.	384:00:00	2025-03-26 05:51:39.615762+00	t
68	10	Operating system services	#code	## Services of Operating System\n\n- Program execution\n- Input Output Operations\n- Communication between Process\n- File Management\n- Memory Management\n- Process Management\n- Security and Privacy\n- Resource Management\n- User Interface\n- Networking\n- Error handling\n- Time Management	192:00:00	2025-03-18 11:05:00.212681+00	t
305	10	VACUUM	#postgres	In PostgreSQL, the VACUUM command is essential for maintaining database health and performance. It reclaims storage occupied by obsolete data, known as "dead tuples," resulting from UPDATE or DELETE operations. Without regular vacuuming, these dead tuples can accumulate, leading to increased disk usage and degraded query performance.\n\n#### Types of Vacuum Operations\n1. VACUUM: This operation marks space occupied by dead tuples as reusable but doesn't immediately reduce the physical size of the table. It's designed to run concurrently with other database operations, minimizing disruption.\n    \n2. VACUUM FULL: This more intensive process compacts the table by removing dead tuples and physically rewriting the table to reduce its size. However, it requires an exclusive lock on the table, making it best suited for maintenance windows due to its impact on database availability.\n    \n3. Autovacuum: PostgreSQL includes an autovacuum daemon that automatically triggers vacuum operations based on specific thresholds. This background process helps maintain database performance without manual intervention.	96:00:00	2025-03-20 10:57:31.458297+00	t
69	10	Length of the last word	#leetcode	func lengthOfLastWord(s string) int {\n  cnt := 0\n  lastWordLen := 0\n  for _, ch := range s {\n    if ch == ' ' {\n      if cnt != 0 {\n        lastWordLen = cnt\n      }\n      cnt = 0\n      continue\n    }\n    cnt++\n  }\n  if cnt != 0 {\n    lastWordLen = cnt\n  }\n  return lastWordLen\n}	384:00:00	2025-05-13 14:30:28.115018+00	f
93	10	netpoller	#code	netpoller is a crucial component of the Go runtime that efficiently handles network I/O operations. It's essentially a mechanism that allows multiple goroutines to concurrently perform network operations without blocking each other.\n\n### How it Works\n1. Asynchronous I/O:   The netpoller operates on an asynchronous model, meaning it doesn't block while waiting for network events.\n\n2. Goroutine Scheduling:   When a goroutine initiates a network operation (like reading or writing data), it registers itself with the netpoller and blocks.\n\n3. Event-Driven:   The netpoller continuously monitors network sockets for events (like data arrival, connection establishment, or errors).\n\n4. Unblocking Goroutines:   When an event occurs, the netpoller unblocks the corresponding goroutine, allowing it to resume its work.\n\n5. Underlying OS Mechanisms:   The netpoller leverages operating system-specific mechanisms like epoll (Linux), kqueue (BSD), or IoCompletionPort (Windows) for efficient event notification.\n\n### Benefits of NetPoller:\n1. High Performance:   Efficiently handles concurrent network I/O operations.\n2. Scalability:   Supports a large number of concurrent connections.\n3. Simplified Programming:   Abstracts away low-level network programming details.\n\nIn essence, the netpoller is responsible for managing network I/O in Go, ensuring that network operations don't hinder the overall performance of the application.\n\nhttps://disk.yandex.ru/i/-QfGTOK-hRrjtg	192:00:00	2025-03-23 14:26:42.976265+00	t
174	10	DRY	#code	Don't Repeat Yourself is a programming principle that involves avoiding code repetition in different parts of a program. Instead, the code should be abstracted and put into reusable components.\n\nThe DRY principle implies that each piece of knowledge or functionality in a programme should have a single, consistent and authoritative representation within the system. If we have repetitive code or logic, we should put it into separate components or functions to avoid duplication and make it easier to maintain the program. This reduces errors, simplifies making changes, and improves code readability.	768:00:00	2025-04-03 04:19:53.888496+00	t
336	10	Kafka broker	#kafka	The broker is responsible for storing your data. All information is saved in a binary format, meaning the broker isn‚Äôt aware of the actual content or structure of the data.\n\nEach logical event type is typically assigned its own topic. For instance, an event for creating an advertisement might be sent to a topic called item.created, while an update event might go to item.changed. You can think of topics as categories for events. At the topic level, you can configure parameters such as:\n\n- Data Volume/Retention: How much data to keep and for how long (using settings like retention.bytes and retention.ms).\n- Redundancy Factor: The replication factor to ensure data is duplicated across brokers.\n- Message Size Limit: The maximum allowed size for a single message (max.message.bytes).\n- In-Sync Replicas: The minimum number of synchronized replicas needed to accept data (min.insync.replicas).\n- Failover Settings: The option to allow failover to an out-of-sync replica, which might lead to potential data loss (unclean.leader.election.enable).\n- And many others: More configuration options can be found in the [Kafka documentation](https://kafka.apache.org/documentation/#topicconfigs).\n\nFurthermore, each topic is split into one or more partitions, which is where the events are ultimately stored. In a cluster with multiple brokers, these partitions are distributed as evenly as possible across the brokers, allowing for scalable read and write operations for each topic.\n\nOn disk, the data for each partition is saved as segment files, which by default are about one gigabyte in size (this can be adjusted with the log.segment.bytes setting). An important detail is that when data is deleted from a partition (triggered by retention policies), the deletion happens at the segment level‚Äîyou can‚Äôt remove a single event, only an entire, inactive segment.	96:00:00	2025-03-30 03:57:35.603902+00	t
222	10	State	#code	A #behavioral design pattern that allows an object to change the behavior depending on its internal state.\n\nThe #state pattern extracts state-related behaviors into separate state classes and forces the original object to delegate the work to one of these classes' instance, instead of acting on its own.\n\nhttps://disk.yandex.ru/i/KkexdMe9OPl09A	96:00:00	2025-03-16 08:17:09.79623+00	t
78	10	Threads pros and cons	#code	### Advantages\n1. Efficiency:   Threads are lightweight and faster to create than processes.\n2. Resource Sharing:   Threads share memory and resources, simplifying data exchange.\n3. Responsiveness:   Threads can improve application responsiveness by handling tasks concurrently.\n\n### Disadvantages\n1. Complexity:  Synchronization and debugging can be challenging.\n2. Scalability:   Too many threads can lead to resource contention and performance degradation.\n3. Stability:   A bug in one thread can crash the entire process.	384:00:00	2025-03-25 16:22:30.224213+00	t
440	10	–º–∞–∑–∞—Ç—å, —Ä–∞–∑–º–∞–∑—ã–≤–∞—Ç—å	#words	smear	384:00:00	2025-05-11 04:42:35.092017+00	f
87	10	Is what you are doing important or essential?	#mindfulness		96:00:00	2025-04-27 12:46:14.907109+00	t
439	10	—Ä–µ–¥–∫–æ (the other one)	#words	seldom	192:00:00	2025-05-03 04:38:40.913973+00	f
165	10	Rotate Image\n\nYou are given an n x n 2D matrix representing an image, rotate the image by 90 degrees (clockwise).\n\nYou have to rotate the image in-place, which means you have to modify the input 2D matrix directly. DO NOT allocate another 2D matrix and do the rotation.	#leetcode	func rotate(m [][]int) {\n  for i := 0; i < len(m); i++ {\n     // transpose the matrix\n    for j := i; j < len(m[0]); j++ {\n      m[i][j], m[j][i] = m[j][i], m[i][j]\n    }\n    // flip each row\n    for j, k := 0, len(m)-1; j < k; j, k = j+1, k-1 {\n      m[i][j], m[i][k] = m[i][k], m[i][j]\n    }\n  }\n}	192:00:00	2025-04-28 04:22:00.578007+00	f
284	10	Aggregate functions	#sql	- COUNT(): Calculates the total number of rows in the result set.\n- SUM(): Adds up all the values in a numeric column from the result set.\n- AVG(): Computes the average of the values in a numeric column.\n- MIN(): Retrieves the smallest value found in a numeric column.\n- MAX(): Retrieves the largest value found in a numeric column.\n- GROUP_CONCAT() (or STRING_AGG()): Joins multiple strings into a single string, using a specified delimiter. (Note: This function isn‚Äôt supported by every database management system.)\n- STDDEV() (or STDEV()): Calculates the standard deviation for the values in a numeric column.\n- VARIANCE(): Determines the variance of the values in a numeric column.\n- FIRST() and LAST(): Return the first and last values, respectively, within a group of rows. (Note: Availability depends on the DBMS.)\n- PERCENTILE_CONT() and PERCENTILE_DISC(): Compute continuous and discrete percentiles, respectively‚Äîthat is, they determine specific percentile values within a dataset.\n- CUME_DIST(): Computes the cumulative distribution of a value within a dataset.\n- ARRAY_AGG() and JSON_AGG(): Aggregate values into an array or JSON object. (Note: Not all systems support these functions.)\n- RANK() and DENSE_RANK(): Assign ranks to rows within an ordered result set. (Again, availability can vary by DBMS.)\n- COUNT(DISTINCT ...): Counts the number of unique values in a column.\n- CORR(): Calculates the correlation coefficient between two numeric columns.\n- COVAR_POP() and COVAR_SAMP(): Compute the population covariance and sample covariance, respectively.	96:00:00	2025-03-23 04:06:21.891608+00	t
255	10	Filtering and sorting operators	#sql	- WHERE:  \n  Filters the results of a query based on specified conditions. You can use comparison operators, logical operators, and other operators to create complex conditions. For example:\n        SELECT *\n    FROM Customers\n    WHERE City = 'New York' AND Name LIKE 'J%';\n    \n\n- ORDER BY:  \n  Sorts the results of a query by one or more columns. You can specify whether to sort in ascending or descending order.\n        SELECT *\n    FROM Customers\n    ORDER BY City ASC, Name DESC;\n    \n\n- GROUP BY:  \n  Groups rows with the same values in one or more columns into a single row. This is often used with aggregate functions to calculate statistics for each group.\n        SELECT City, COUNT(*) AS NumberOfCustomers\n    FROM Customers\n    GROUP BY City\n    ORDER BY NumberOfCustomers DESC;\n    \n\n- HAVING:  \n  Filters the results of a GROUP BY query based on specified conditions. This is similar to WHERE, but _it operates on groups_ rather than individual rows.\n        SELECT City, COUNT(*) AS NumberOfCustomers\n    FROM Customers\n    GROUP BY City\n    HAVING COUNT(*) > 10\n    ORDER BY NumberOfCustomers DESC;	192:00:00	2025-04-05 01:24:32.379125+00	t
132	10	Schenke den Menschen, mit denen du interagierst, deine volle Aufmerksamkeit	#mindfulness	Erkenne ihre Gef√ºhle und Reaktionen, analysiere ihr Verhalten und spiegele ihre Bewegungen wider.	96:00:00	2025-04-30 03:53:08.765613+00	f
196	10	Iterative and Incremental Methodologies	#code	### Iterative Model\nThis approach develops the system through repeated cycles (iterations). In each iteration, the team goes through planning, design, coding, and testing, refining the product based on feedback. It‚Äôs ideal for projects where requirements may evolve over time.  \n\n### Incremental Model\nHere, the software is built in small, manageable pieces (increments). Each increment delivers part of the functionality and is integrated into the overall system. This model allows for early partial releases and gradual expansion of the system[^icm].  \n\n### Spiral Model\nA risk-driven iterative model that combines elements of both Waterfall and iterative approaches. Each cycle (or spiral) involves planning, risk analysis, engineering (including prototyping), and evaluation, making it particularly suitable for large, complex, and high-risk projects.	192:00:00	2025-03-18 17:34:05.664883+00	t
225	10	Mediator	#code	A #behavioral design pattern that reduces coupling between components of a program by making them communicate indirectly, through a special #mediator object.\n\nThe Mediator makes it easy to modify, extend, and reuse individual components because they're no longer dependent on the dozens of other classes.\n\nhttps://disk.yandex.ru/i/Ln05JI6I1TVXvQ	192:00:00	2025-03-17 17:39:38.171663+00	t
58	10	The Internet of Things and embedded operating systems	#code	The IOT (Internet of Things) comprises all the billions of physical objects with sensors and actuators that are increasingly connected to the network, such as fridges, thermostats, security camera‚Äôs motion sensors, and so on.\n\nSuch systems do not accept user-installed software, so the main property which distinguishes such embedded systems from the computers is the certainty that no untrusted software will ever run on it. All software is typically in ROM (Read only memory).	192:00:00	2025-03-27 04:38:41.536903+00	t
152	10	Is valid Sudoku\n\nDetermine if a 9 x 9 Sudoku board is valid. Only the filled cells need to be validated according to the following rules:\n\n- Each row must contain the digits 1-9 without repetition.\n- Each column must contain the digits 1-9 without repetition.\n- Each of the nine 3 x 3 sub-boxes of the grid must contain the digits 1-9 without repetition.	#leetcode	func isValidSudoku(m [][]byte) bool {\n    var rows, cols, sqrs [9][9]bool\n    for i, row := range m {\n        for j, v := range row {\n            if v != 46 {\n                k := int(v)-49\n                if rows[i][k]  cols[j][k]  sqrs[i/3*3 + j/3][k] {\n                    return false\n                }\n                rows[i][k], cols[j][k], sqrs[i/3*3 + j/3][k] = true, true, true\n            }\n        }\n    }\n    return true\n}	384:00:00	2025-05-07 16:54:46.044427+00	f
250	10	Happy Number\n\nA happy number is a number defined by the following process:\n\n- Starting with any positive integer, replace the number by the sum of the squares of its digits.\n- Repeat the process until the number equals 1 (where it will stay), or it loops endlessly in a cycle which does not include 1. \n- Those numbers for which this process ends in 1 are happy.\n\nReturn true if n is a happy number, and false if not.	#leetcode	func isHappy(n int) bool {\n    mp := make(map[int]struct{})\n    for n != 1 {\n        k := 0\n        for n > 0 {\n            res := n%10\n            if res > 0 {\n                k += res*res\n            }\n            n /= 10\n        }\n        if _, ok := mp[k]; ok {\n            return false\n        }\n        mp[k] = struct{}{}\n        n = k\n    } \n    return true\n}	384:00:00	2025-05-01 06:24:23.964331+00	f
142	10	Docker internal network	#code	Docker's internal networking offers several types of networks to suit different communication needs between containers. \n\nThe default bridge network creates a private, internal network on the Docker host, allowing containers to communicate using container names. \n\nThe host network bypasses Docker's internal networking, directly using the host's network stack, offering performance benefits but sacrificing isolation. \n\noverlay networks enable multi-host container communication, crucial for Swarm mode and distributed applications.\n\nOther network plugins and options exist, providing flexibility to tailor container networking to specific application requirements and security considerations.	96:00:00	2025-03-15 14:53:38.703306+00	t
182	10	Words set his mind ablaze with mental imagery	#mindfulness		96:00:00	2025-03-28 13:03:31.440788+00	t
183	10	By the time you get to the end of this sentence, your brain will have physically changed.	#mindfulness	Every sensation that we remember, every thought that we think, transforms our brains by altering the connections within a wast network of neurons.	48:00:00	2025-03-23 06:53:28.295886+00	t
327	10	Merge Two Sorted Lists\n\nYou are given the heads of two sorted linked lists list1 and list2.\n\nMerge the two lists into one sorted list. The list should be made by splicing together the nodes of the first two lists.\n\nReturn the head of the merged linked list.	#leetcode	/**\n * Definition for singly-linked list.\n * type ListNode struct {\n *     Val int\n *     Next *ListNode\n * }\n */\nfunc mergeTwoLists(l1 *ListNode, l2 *ListNode) *ListNode {\n  dummy := new(ListNode)\n  curr := dummy\n\n  for l1 != nil && l2 != nil {\n    val := 0\n    if l1.Val < l2.Val {\n      val = l1.Val\n      l1 = l1.Next\n    } else {\n      val = l2.Val\n      l2 = l2.Next\n    }\n\n    curr.Next = &ListNode{Val: val}\n    curr = curr.Next\n  }\n\n  for l1 != nil {\n    curr.Next = &ListNode{Val: l1.Val}\n    curr = curr.Next\n    l1 = l1.Next\n  }\n\n  for l2 != nil {\n    curr.Next = &ListNode{Val: l2.Val}\n    curr = curr.Next\n    l2 = l2.Next\n  }\n\n  return dummy.Next\n}	6144:00:00	2026-04-27 03:03:01.032461+00	f
51	10	Process	#code	Process refers to a program in execution; it's a running instance of a program. A process has its own address space, which includes the executable code, data, stack, and other resources necessary for executing the program. Each process has a unique identifier known as the Process ID (PID), which allows the operating system to manage and refer to it.\n\nConceptually, each process has its own virtual CPU. In reality, of course, each real CPU switches back and forth from process to process.\n\nProcess in memory is divided in four sections:\n1. Text  ‚Äî is a code segment with program instructions, typically read-only.\n2. Stack ‚Äî contains temporary data, such as function parameters, returns, addresses (pointers ??), and local variables.\n3. Data  ‚Äî contains global variables.\n4. Heap ‚Äî is dynamically allocated to process during its run time.	96:00:00	2025-03-14 16:40:27.381753+00	t
279	10	Working with NULL	#sql	### Checking for NULL \n1. IS NULL  \n   To check if a field contains NULL, use the IS NULL operator in your SQL queries. This operator specifically tests for nullity.\n   \n2. IS NOT NULL  \n   If you need to find records where a field does _not_ contain NULL, use the IS NOT NULL operator. This operator is the negation of IS NULL.\n\n\n3. COALESCE   \n   The COALESCE function is a very useful tool that can be used to replace NULL values with a specific value that you define. COALESCE evaluates its arguments in order and returns the first non-NULL expression.\n\n> To correctly check for NULL, you _must_ use the IS NULL or IS NOT NULL operators. Standard equality or inequality operators will not work for this purpose.\n\n### Using NULL \n1. In Aggregate Functions and GROUP BY:   \n   When working with aggregate functions (like SUM, COUNT, AVG, etc.) and you want to exclude NULL values from your calculations, it's important to handle them appropriately.\n\n2. JOIN Clauses:   \n   When joining tables using the JOIN operator, be aware that NULL values can behave in a particular way that affects the outcome of your queries. In standard JOIN operations (like INNER JOIN), if there is no match between the key values in the joined tables, the rows from both tables will _not_ be included in the final result.\n\n> Critically, NULL values are _not_ considered to match any other value, including another NULL value, in the context of a standard JOIN condition.	192:00:00	2025-03-28 05:30:40.658414+00	t
90	10	poll and epoll	#code	Poll and epoll are system calls used in Unix-like operating systems for efficient handling of multiple I/O operations (aka I/O multiplexing*). They are essential tools for building high-performance network servers and other applications that need to monitor multiple file descriptors (e.g., sockets, pipes, files) for events like data availability, connection requests, or errors.\n\npoll is a portable solution for monitoring multiple file descriptors, suitable for applications with a moderate number of connections.\n\nepoll is a Linux-specific solution that provides significantly better performance for applications with numerous connections.\n\n*multiplexing is a technique that combines multiple signals or data streams into a single signal for transmission over a shared medium. Think of it like sharing a single road for multiple cars.	384:00:00	2025-04-03 04:20:37.68893+00	t
401	10	Be concise in every statement	#mindfulness		24:00:00	2025-04-13 16:50:45.594851+00	t
442	10	Binary Tree Maximum Path Sum\n\nA path in a binary tree is a sequence of nodes where each pair of adjacent nodes in the sequence has an edge connecting them. A node can only appear in the sequence at most once. Note that the path does not need to pass through the root.\n\nThe path sum of a path is the sum of the node's values in the path.\n\nGiven the root of a binary tree, return the maximum path sum of any non-empty path.	#leetcode	func maxPathSum(root *TreeNode) int {\n  var traverse func(*TreeNode) int\n  maxPath := root.Val\n  traverse = func(node *TreeNode) int {\n    if node == nil {\n      return 0\n    }\n\n    l := max(traverse(node.Left), 0)\n    r := max(traverse(node.Right), 0)\n    maxPath = max(maxPath, node.Val+l+r)\n\n    return node.Val + max(l, r)\n  }\n  traverse(root)\n\n  return maxPath\n}	96:00:00	2025-04-29 14:43:05.603102+00	f
400	10	Pause before speaking your mind	#mindfulness	(no prompt)	24:00:00	2025-04-13 03:38:10.393335+00	t
461	10	an inclination or natural tendency to behave in a particular way	#words	propensity	96:00:00	2025-04-27 04:59:19.818335+00	f
304	10	Deadlock	#postgres	A deadlock in PostgreSQL occurs when two or more processes (or transactions) become blocked indefinitely, each waiting for a resource that is held by another process involved in the deadlock.\n\nPostgreSQL is designed to automatically detect deadlock situations and attempt to resolve them. The resolution strategy usually involves terminating one of the transactions involved in the deadlock, effectively breaking the cycle and allowing the other transaction(s) to proceed. This process is often referred to as "deadlock victim selection" and "transaction rollback."\n\nYou can use the pg_stat_activity system view to identify processes that are currently waiting.\nSELECT * FROM pg_stat_activity WHERE waiting = 't';	192:00:00	2025-05-04 15:38:56.495419+00	f
243	10	Domain-Driven Design	#code	Domain-Driven Design is a strategic approach to software development that centers the design process around the business domain, ensuring that the software accurately reflects and serves the needs of the business.	96:00:00	2025-03-14 18:46:12.547179+00	t
230	10	Flyweight	#code	A #structural design pattern that lets you fit more objects into the available amount of RAM by sharing common parts of state between multiple objects instead of keeping all the data in each object.\n\nThe pattern achieves it by sharing parts of object state between multiple objects. In other words, the Flyweight saves RAM by caching the same data used by different objects.	192:00:00	2025-03-20 01:24:42.995141+00	t
186	10	OOP	#code	Object-oriented programming is a programming paradigm that allows code to be organized around objects that represent real or abstract entities and encapsulate data and the methods operating on that data.\n\nAbstraction, inheritance, polymorphism, encapsulation, and composition are the main pillars of object-oriented programming. They allow us to create flexible, modular and extensible systems.\n\nhttps://pictures.s3.yandex.net/resources/PWD-401_4_1701754953.png	384:00:00	2025-04-09 13:47:37.444978+00	t
226	10	Iterator	#code	A #behavioral design pattern that allows sequential traversal through a complex data structure without exposing its internal details.\n\nThe main idea behind the #iterator pattern is to extract the iteration logic of a collection into a different object called iterator. This iterator provides a generic method of iterating over a collection independent of its type.\n\nThanks to the Iterator, clients can go over elements of different collections in a simple fashion using a single iterator interface.\n\nhttps://disk.yandex.ru/i/h1aexHmthXdCCg	192:00:00	2025-03-15 04:07:16.637772+00	t
443	10	On nature and discipline. Rethorica Ad Herennium	#quotes	For in invention nature is never last, education never first; rather the beginnings of things arise from natural talent, and the ends are reached by discipline.	384:00:00	2025-04-28 17:29:04.041985+00	f
432	10	Read faster	#waytosuccess	Reading is a skill. To improve you must get out of your OK plateau	48:00:00	2025-03-28 08:11:25.516492+00	t
431	10	Deliberate practice	#waytosuccess	What separates experts from the rest of us is that they tend to engage in a very directed, highly focused routine, which Ericsson has labeled "deliberate practice." Having studied the best of the best in many different fields, he has found that top achievers tend to follow the same general pattern of development. They develop strategies for consciously keeping out of the autonomous stage while they practice by doing three things; focusing on their technique, staying goal-oriented, and getting constant and immediate feedback on their performance. In other words, they force themselves to stay in the "cognitive phase."	384:00:00	2025-05-12 14:28:46.051684+00	f
271	10	Summary Ranges\n\nYou are given a sorted unique integer array nums.\n\nA range [a,b] is the set of all integers from a to b (inclusive).\n\nReturn the smallest sorted list of ranges that cover all the numbers in the array exactly. That is, each element of nums is covered by exactly one of the ranges, and there is no integer x such that x is in one of the ranges but not in nums.\n\nEach range [a,b] in the list should be output as:\n\n"a->b" if a != b\n"a" if a == b	#leetcode	func summaryRanges(nums []int) []string {\n    ranges := make([]string, 0)\n\n    for s, e := 0, 0; e < len(nums); e++ {\n        s = e\n        for e < len(nums)-1 && nums[e] == nums[e+1] - 1 {\n            e++\n        }\n\n        if s == e {\n            ranges = append(ranges, fmt.Sprintf("%d", nums[s]))\n        } else {\n            ranges = append(ranges, fmt.Sprintf("%d->%d", nums[s], nums[e]))\n        }\n    }\n\n    return ranges\n}	768:00:00	2025-05-27 03:09:02.316341+00	f
200	10	Planning	#code	#planning is the process of defining software goals, specifications, requirements, and system architecture.\n\nPlanning before writing code is an important step in software development. It defines the project goals, specifications, requirements, and system architecture. Planning includes defining functionality, creating diagrams, defining interfaces, and testing.	192:00:00	2025-03-23 11:04:27.521825+00	t
232	10	Decorator	#code	A #structural pattern that allows adding new behaviors to objects dynamically by placing them inside special wrapper objects, called decorators.\n\nUsing decorator, you can wrap objects countless number of times since both target objects and decorators follow the same interface. The resulting object will get a stacking behavior of all wrappers.	192:00:00	2025-03-18 13:11:47.279874+00	t
86	10	TCP handshake	#code	TCP handshake is the process that occurs between a client and a server to establish a TCP connection before data transmission begins. It consists of three steps and is carried out using TCP flags (e.g., SYN, ACK). \n\nHere is a step-by-step description of the TCP handshake process:\n\n1. Step 1: The client sends a connection request packet (SYN) to the server. In this packet, the client specifies a random number called the Initial Sequence Number (ISN), which will be used to sequence subsequent data packets.\n\n2. Step 2: The server receives the SYN packet from the client and sends an acknowledgment (ACK) back to the client. In the response packet, the server sets its own Initial Sequence Number (ISN), which is also a random number. Additionally, the server sends a connection request packet (SYN) to the client.\n\n3. Step 3: The client receives the acknowledgment (ACK) from the server. It verifies whether the acknowledgment matches its Initial Sequence Number and the SYN flag it sent. If the verification is successful, the client sends an acknowledgment (ACK) to the server. At this point, the client and server have completed the connection establishment process.\n\nOnce the TCP handshake is complete, the client and server can begin exchanging data over the established connection. Data is transmitted through TCP segments, each of which has a sequence number for controlling the order of packets and an acknowledgment for confirming receipt.\n\nhttps://disk.yandex.ru/i/60yzn8MEz_P6rA	192:00:00	2025-03-23 08:31:51.993725+00	t
445	10	be friends with failure	#waytosuccess	It is from failure that we learn and grow	96:00:00	2025-04-29 11:25:01.26195+00	f
253	10	DCL operators	#sql	_Data Control Language_ \n \n- GRANT:  \n  Grants permissions to users or roles to access and manipulate data in the database. You can grant permissions to specific objects, such as tables, views, and stored procedures.\n        GRANT SELECT, INSERT ON Customers TO John;\n    \n\n- REVOKE:  \n  Revokes permissions from users or roles.\n        REVOKE SELECT, INSERT ON Customers FROM John;	192:00:00	2025-04-03 03:52:55.39773+00	t
20	10	Control your breath before controlling your mind.	#mindfulness	Before approaching a complex and cognitively demanding task (e.g. LeetCode), first take a pause and focus on your breathing for 2-3 minutes, like a short meditation. Because if you are not able to focus on your breath, how are you going to be focused on the task?	96:00:00	2025-03-22 16:18:27.108917+00	t
317	10	Selectivity in a composite index	#sql	Selectivity in a composite index is a measure of how unique or varied the values in the indexed columns are relative to the size of the table. Generally, more selective indexes are more efficient because they enable the database to quickly narrow down the range of data that needs to be scanned for a query.\n\nThe higher the index‚Äôs selectivity, the fewer rows will match a given value, allowing the index to filter data more effectively. On the other hand, an index with low selectivity corresponds to a large number of rows and is therefore less efficient.\n\nWhen it comes to composite indexes, selectivity depends on the selectivity of each individual column as well as the order in which they appear. If you create a composite index on columns that each have high selectivity, the overall index will be highly selective. For instance, if you build an index on columns "A" and "B" and both are highly selective, the composite index will also exhibit high selectivity.\n\nHowever, the order of columns can significantly affect the index‚Äôs efficiency. If one column is highly selective while the other is not, arranging the highly selective column first (e.g., "A, B") will filter out many rows upfront, making the index more effective than if the order were reversed ("B, A").\n\nIn summary, when designing composite indexes, it‚Äôs crucial to consider the selectivity of the individual columns and the specific needs of your queries to choose the optimal column order and maximize query performance.	192:00:00	2025-03-27 09:47:45.951255+00	t
185	10	Set Matrix Zeros\n\nGiven an m x n integer matrix matrix, if an element is 0, set its entire row and column to 0's.\n\nYou must do it in place.	#leetcode	func setZeroes(mtx [][]int)  {\n    n, m := len(mtx), len(mtx[0])\n\n    rows := make([]bool, n)\n    cols := make([]bool, m)\n\n    for i, row := range mtx {\n        for j, v := range row {\n            if v == 0 {\n                rows[i] = true\n                cols[j] = true\n            }\n        }\n    }\n\n    for i := 0; i < n; i++ {\n        for j := 0; j < m; j++ {\n            if rows[i] || cols[j] {\n                mtx[i][j] = 0\n            }\n        }\n    }\n}	384:00:00	2025-04-28 09:03:21.708949+00	f
320	10	ad-hoc	#words	Adverb, meaning "for this" or "for this purpose only".\n\n1.  For the specific purpose, case, or situation at hand and for no other. "a committee formed ad hoc to address the issue of salaries."\n2.  On the spur of the moment.\n3. For a particular purpose.	384:00:00	2025-05-08 03:14:20.316186+00	f
184	10	Only you decide how this story goes	#mindfulness	(no prompt)	96:00:00	2025-04-28 14:20:39.679881+00	t
444	10	Share your heart only with the ones who deserve it	#mindfulness		192:00:00	2025-05-03 05:39:59.1313+00	f
206	10	Goroutines	#code	#goroutine is a function that executes concurrently with other functions, managed by the Go runtime. Goroutines are designed to be lightweight, starting with a small amount of memory and growing as needed, allowing the creation of thousands or even millions of them within a single program.¬†\n\n### Key Features\n- Lightweight Concurrency: Goroutines are more memory-efficient than traditional threads, starting with a small stack that grows and shrinks as required. This efficiency enables the handling of numerous concurrent tasks without significant overhead.¬†\n\n- Efficient Scheduling: The Go runtime includes a scheduler that manages goroutines, _multiplexing them onto multiple operating system threads_. This design ensures that if one goroutine blocks, others can continue executing, leading to efficient use of system resources.¬†\n\n-  Shared Memory Access: _All goroutines within a program share the same address space_, allowing them to access shared variables. However, to prevent race conditions, access to shared memory must be synchronized, typically using channels or other synchronization primitives.¬†\n\n- Communication via Channels: Go provides channels as a means for goroutines to communicate safely and synchronize their execution. Channels allow the passing of data between goroutines, facilitating coordination without explicit locking.¬†\n\nBy leveraging goroutines, Go enables developers to build scalable and efficient concurrent applications with ease.	96:00:00	2025-03-15 17:18:35.426061+00	t
235	10	Adapter	#code	A #structural design pattern that allows objects with incompatible interfaces to collaborate.\n\nAn adapter wraps one of the objects to hide the complexity of conversion happening behind the scenes. The wrapped object isn‚Äôt even aware of the adapter. For example, you can wrap an object that operates in meters and kilometers with an adapter that converts all of the data to imperial units such as feet and miles.	192:00:00	2025-03-19 11:22:40.554098+00	t
285	10	Constraints	#sql	In SQL, constraints are rules applied to table columns that define which values can be inserted, updated, or deleted. They ensure the integrity of the data and help maintain overall database consistency. Here are some of the most common types of constraints:\n\n- Primary Key Constraint:   \n  This constraint guarantees that the values in a column or a combination of columns are unique, effectively serving as a unique identifier for each row in the table. Every row must have a unique primary key.\n\n- Unique Constraint:   \n  This ensures that all the values in a specified column or group of columns are distinct.\n\n- Foreign Key Constraint:   \n  This type of constraint establishes a relationship between two tables. A foreign key in one table refers to the primary key in another, which helps maintain consistency between related records and prevents the creation of orphaned records (records that reference non-existent data).\n\n- Check Constraint:   \n  A check constraint lets you define a custom condition that values in a column must satisfy. For example, you could specify that an age column must contain only positive numbers.\n\n- Default Constraint:   \n  This constraint assigns a default value to a column when no value is provided during data insertion. It ensures that columns always have a defined value even if one isn‚Äôt explicitly set.\n\n- Delete Constraint:   \n  This defines what should happen when a row that is linked to other data is deleted. For instance, you might set up a rule so that when an order is removed, all associated order items are automatically deleted as well.	96:00:00	2025-03-24 07:37:23.699694+00	t
289	10	VIEW	#sql	SQL database views are like virtual tables. They don't store any data themselves but are based on a query that selects data from one or more underlying tables. They can simplify complex queries, restrict access to sensitive data, and present customized data to specific users.\n\n### Key Features\n- Views are created using the CREATE VIEW statement.\n\n- Views can be queried like regular tables. You can use SELECT, INSERT, UPDATE, and DELETE statements on a view, and the changes will be reflected in the underlying base tables.\n\n- Views can simplify complex queries. If you have a query that involves multiple joins or subqueries, you can create a view that encapsulates that query. Then, you can simply query the view instead of writing the complex query every time.\n\n- Views can be used to restrict access to sensitive data. You can create a view that only shows certain columns or rows from a table. Then, you can grant users access to the view but not to the underlying table.\n\n- Views can be used to present customized data to specific users. You can create different views for different users, each showing only the data that is relevant to that user.	96:00:00	2025-03-25 17:55:28.957159+00	t
217	10	Is Anagram\n\nGiven two strings s and t, return true if t is an anagram of s, and false otherwise.	#leetcode	func isAnagram(s string, t string) bool {\n    mp, total := make([]int, 26), 0\n\n    for _, ch := range s {\n        mp[ch-97]++\n        total++\n    }\n\n    for _, ch := range t {\n        if mp[ch-97] == 0 {\n            return false\n        }\n        mp[ch-97]--\n        total--\n    }\n\n    return total == 0\n}	192:00:00	2025-04-27 03:36:44.277529+00	f
52	10	fork()	#code	fork() is a system call used to create a new process. It creates a complete copy of the parent process, including all its data and resources. The hierarchy of processes is maintained, meaning the parent process is referred to as the parent, and the created process is called the child.\n\nhttps://disk.yandex.ru/i/rdQl2j1d9q0dsA	192:00:00	2025-03-21 12:37:16.429043+00	t
209	10	Ransom Note\n\nGiven two strings ransomNote and magazine, return true if ransomNote can be constructed by using the letters from magazine and false otherwise.\n\nEach letter in magazine can only be used once in ransomNote.	#leetcode	func canConstruct(ransomNote string, magazine string) bool {\n    mp := [26]int{}    \n\n    for _, v := range magazine {\n        mp[v-97]++\n    }\n\n    for _, v := range ransomNote {\n        if mp[v-97] == 0 {\n            return false\n        }\n        mp[v-97]--\n    }\n\n    return true\n}	384:00:00	2025-05-12 14:38:36.957484+00	f
151	10	Minimum size subarray sum\n\nGiven an array of positive integers nums and a positive integer target, return the minimal length of a subarray whose sum is greater than or equal to target. If there is no such subarray, return 0 instead.	#leetcode	func minSubArrayLen(target int, nums []int) int {\n    minLen := len(nums)+1\n    l, acc := 0, 0\n\n    for r := 0; r < len(nums); r++ {\n        acc += nums[r]\n\n        for acc >= target {\n            minLen = min(minLen, r-l+1)\n            \n            acc -= nums[l]\n            l++\n\n        }\n    }\n\n    if minLen == len(nums)+1 {\n        return 0\n    }\n\n    return minLen\n}	384:00:00	2025-05-14 12:21:41.29888+00	f
306	10	MVCC	#postgres	Multiversion Concurrency Control (MVCC) is a fundamental process in PostgreSQL that allows database users to read and modify data concurrently without blocking each other. It is primarily employed to prevent locking issues that can arise when multiple transactions attempt to access the same data simultaneously.\n\nIn essence, MVCC is a mechanism that enables multiple users to access and manipulate a database concurrently while ensuring transaction isolation. This concept is crucial for maintaining data integrity. Instead of relying on locks to manage concurrent access, MVCC provides each transaction with a _snapshot_ or version of the database as it existed at the moment the transaction began. This approach effectively eliminates the typical reader-writer or writer-writer conflicts that traditional locking mechanisms would encounter.	96:00:00	2025-03-26 04:32:05.80946+00	t
292	10	TIMESTAMP	#sql	In SQL, the TIMESTAMP data type is used to store both date and time information, typically in the format YYYY-MM-DD HH:MI:SS. This allows for precise recording of events, such as when a record is created or modified. The TIMESTAMP data type is essential for applications requiring accurate tracking of temporal data.\n\nThe handling of time zones with TIMESTAMP varies between SQL database systems. Some systems, like PostgreSQL, offer a TIMESTAMPTZ (timestamp with time zone) data type that includes time zone information, ensuring accurate time representation across different regions.\n\nUse functions like CURRENT_TIMESTAMP or NOW() to obtain the current date and time.\n \n      SELECT CURRENT_TIMESTAMP;\n \n\nThe EXTRACT function can retrieve specific parts of a timestamp, such as the year, month, day, hour, etc.\n      SELECT EXTRACT(YEAR FROM EventTime) AS EventYear\n      FROM Events;	192:00:00	2025-04-03 02:48:04.292171+00	t
337	10	Zookeeper	#kafka	Zookeeper serves as both a metadata repository and a coordinator. It \n1. verifies whether brokers are active (you can check this in Zookeeper via the zookeeper-shell command ls /brokers/ids), \n2. identifies which broker is acting as the controller (using get /controller), and\n3. confirms if partitions are synchronized with their replicas (via get /brokers/topics/topic_name/partitions/partition_number/state). \n\nProducers and consumers initially contact Zookeeper to learn which brokers host which topics and partitions. When a topic is set with a replication factor greater than one, Zookeeper indicates which partitions are the leaders‚Äîthese are the partitions where data is written and from which data is read. In the event of a broker failure, Zookeeper records information about the new leader partitions (starting with version 1.1.0, this update happens asynchronously, which is important).\n\nIn earlier versions of Kafka, the Zookeeper was also responsible for storing offsets. However, offsets are now maintained in a special topic called consumer_offsets on the broker (although Zookeeper can still be used for this purpose).\n\nIn short, losing the data stored in Zookeeper is akin to turning your valuable information into a pumpkin‚Äîonce that information is gone, it becomes extremely challenging to determine what data to retrieve and from where.	96:00:00	2025-03-31 16:01:39.345411+00	t
8	10	Are you focused on the essential?	#waytosuccess	Don't waste your attention on anything that doesn't bring you closer to your goals. \n\nUse boredom, make it your superpower.	96:00:00	2025-04-28 13:06:39.37836+00	f
386	10	CI/CD build stage	#cicd	### What Happens in the Build Stage?\nDuring the build stage, an executable artifact is created. This could be a binary file, a container image (like Docker), or an executable file (like an .exe). The primary goal of the build stage is to verify that the application can be compiled or packaged without errors. It's a crucial step in ensuring the code is in a deployable state.\n\n### Build Duration Considerations\nThe time required for a build process varies depending on the project's size and complexity. Build times can range from a few seconds to several hours. It is a best practice to continuously optimize the CI/CD pipeline to reduce build times, as this directly impacts team productivity and the speed of feedback loops.	96:00:00	2025-03-23 04:04:42.895633+00	t
187	10	Abstraction	#code	#abstraction is the process of extracting common characteristics of objects and representing them as classes or interfaces. It allows us to hide implementation details and focus on the important aspects of an object.  \n\nIn Golang, abstraction is achieved by defining interfaces that describe a set of methods but do not contain their implementation. This allows us to separate the interface and its implementation, providing flexibility and the ability to replace system components.	96:00:00	2025-03-19 17:11:03.109539+00	t
128	10	TLS/SSL	#code	SSL (Secure Sockets Layer) and its successor, TLS (Transport Layer Security), are cryptographic protocols that provide secure communication between a client and a server. They are essential for protecting the confidentiality and integrity of data transmitted across a network.\n\nSSL/TLS protocols establish an encrypted channel, preventing eavesdropping, tampering, and message forgery. The client and server negotiate a secure connection using cryptographic algorithms. This involves exchanging digital certificates to verify identities and agreeing on encryption keys. Once the secure connection is established, all data exchanged is encrypted.\n\nSSL/TLS is widely used to secure web browsing (HTTPS), email communication (SMTPS, IMAPS, POP3S), file transfer (FTPS), and other network applications where security is critical.\n\nSSL/TLS relies on digital certificates issued by Certificate Authorities (CAs) to verify the identity of the server. These certificates contain information about the server's domain name and public key.	96:00:00	2025-03-16 10:15:38.303151+00	t
434	10	One has to hurt, to go through a period of stress, a period of self-doubt, a period of confusion. And then out of that mess can flow the richest tapestries.	#waytosuccess		192:00:00	2025-04-28 07:54:52.449673+00	f
446	10	Lowest Common Ancestor of a Binary Tree\n\nGiven a binary tree, find the lowest common ancestor (LCA) of two given nodes in the tree.\n\nAccording to the definition of LCA on Wikipedia: ‚ÄúThe lowest common ancestor is defined between two nodes p and q as the lowest node in T that has both p and q as descendants (where we allow a node to be a descendant of itself).‚Äù	#leetcode	func lowestCommonAncestor(node, p, q *TreeNode) *TreeNode {\n  switch {\n  case node == nil:\n    return nil\n  case node == p:\n    return p\n  case node == q:\n    return q\n  }\n\n  l := lowestCommonAncestor(node.Left, p, q)\n  r := lowestCommonAncestor(node.Right, p, q)\n\n  if l != nil && r != nil {\n    return node\n  }\n\n  if l != nil {\n    return l\n  } else {\n    return r\n  }\n}	192:00:00	2025-04-29 12:04:32.73232+00	f
288	10	Stored procedures	#sql	A Stored Procedure is a pre-compiled set of SQL instructions that can be invoked and executed as a single unit directly within a database. Stored procedures are typically created and stored within DBMS for repeated use.\n\nHere are several key concepts and advantages of using stored procedures:\n\n1. Pre-compilation:  \n   Stored procedures are compiled once, either when they are initially created or when they are modified. \n\n2. Reusability:  \n   Stored procedures can be called from various parts of an application, and even from different applications altogether. \n\n3. Reduced Network Traffic:  \n   Because a stored procedure executes on the database server itself, only the results of the procedure are transmitted over the network, rather than the entire set of SQL instructions. \n\n4. Enhanced Security:  \n   Stored procedures can be configured with specific access permissions. This enables database administrators to control and restrict access to sensitive data and manage database security more effectively. \n\n5. Minimized Code Duplication:  \n   Stored procedures can encapsulate common logic and functionalities that are used repeatedly. \n\n6. Transaction Management:  \n   Stored procedures are capable of performing complex operations within the scope of a single transaction. This ensures data integrity by maintaining ACID (Atomicity, Consistency, Isolation, Durability) properties, especially when dealing with operations that must either all succeed or all fail together.\n\n\nStored procedures are frequently employed for performing a wide range of database operations, including inserting, updating, and deleting data, as well as data aggregation, data validation, and much more.	192:00:00	2025-04-06 04:27:37.848166+00	t
98	10	Everything is a file	#code	In Linux, every entity, including devices, files, and processes, is represented as part of the file system and is handled using file operations. This concept is known as "everything is a file."\n\nThe Linux file system is organized in a hierarchical structure, starting from the root directory ("/"). Directories and files exist within this structure, and each has its own path. \n\nAll devices, including hard drives, printers, network interfaces, and others, are also represented as files in special directories called "devices." \n\nAdditionally, processes in Linux are also treated as files. Information about processes is accessible through special files in the "/proc" directory. Each process has its own directory named according to its process identifier (PID). These files provide information about the process's state, open file descriptors, resource usage, and other process attributes. \n\nThe "everything is a file" concept in Linux provides a unified and consistent interface for interacting with various system entities, making it more flexible and convenient for administration and development.	96:00:00	2025-03-21 16:32:08.541091+00	t
247	10	RAM vs persistant storage	#code	Computers use different types of memory to store information. These can be broadly classified into two main categories: Random Access Memory (RAM) and persistent storage (such as hard drives, SSDs, and flash drives).\n\nRAM is fast but volatile (data lost when power off), used for active tasks. Persistent storage is slower but non-volatile, for long-term data like files and OS. RAM has smaller capacity than persistent storage. They work together: RAM for speed, persistent storage for safekeeping.\n\nKey differences:\n- Access time and write speed (nanoseconds/microseconds);\n- Data retention (volatile/nonvolatile);\n- Storage capacity (GBs/TBs);\n- Usage (temporary/long-term);\n- Lifespan (unlimited/limited);	192:00:00	2025-03-21 04:12:36.539817+00	t
447	10	a person lives three lives	#quotes	A person lives three lives.\nThe first ends with the loss of na√Øvet√©, the second,¬†with the loss of innocence and the third... with the loss of life itself.\nIt's inevitable that we go through all three stages.	384:00:00	2025-05-09 07:33:03.553055+00	f
23	10	Clock interrupt	#code	A clock interrupt is a signal generated by a hardware timer (or clock) in a computer system that interrupts the normal execution of a program to allow the operating system to perform certain tasks.\n\nIt is a critical mechanism in operating systems that allows for multitasking, process scheduling, and time management. It enables the operating system to maintain control over the CPU and ensure that all processes receive fair access to system resources.	192:00:00	2025-03-17 10:14:30.423134+00	t
138	10	Popular Linux container technologies	#code	1. Docker:   The most widely used container platform, offering a user-friendly interface and vast ecosystem of tools.\n\n2. LXC (Linux Containers):   An alternative container technology with a focus on security and resource control.\n\n3. Kubernetes:   An open-source system for automating deployment, scaling, and management of containerized applications.	192:00:00	2025-03-16 04:31:44.683324+00	t
411	10	Invert Tree\n\nGiven the root of a binary tree, invert the tree, and return its root.	#leetcode	func invertTree(node *TreeNode) *TreeNode {\n    if node == nil {\n        return nil\n    }\n\n    right := invertTree(node.Right)    \n    left := invertTree(node.Left)\n\n    node.Right, node.Left = left, right\n\n    return node\n}	384:00:00	2025-05-08 16:57:17.182751+00	f
46	10	Linux file system	#code	A file system on Linux is a method for storing and organizing files and directories on a storage device. It acts as an interface between the operating system and the physical storage, allowing users and applications to access data. A file system is a combination of software and data structures that manage how data is stored and retrieved. \n\nKey functions of a file system include:\n1. File Management: Organizes files into directories for easy navigation and access.\n2. Metadata Storage: Stores information about files, such as their names, sizes, permissions, and creation/modification dates.\n3. Access Control: Manages permissions to ensure only authorized users or applications can access or modify files.\n4. Storage Allocation: Allocates space on the storage device efficiently to store files and directories.\n5. Data Integrity: Ensures data is stored reliably and can be recovered in case of errors or crashes.\n\nCommon file systems include:\n- NTFS (Windows)\n- FAT32 and exFAT (cross-platform)\n- HFS+ and APFS (macOS)\n- ext4, XFS, and Btrfs (Linux)	96:00:00	2025-03-16 12:02:36.935587+00	t
404	10	Same Tree\n\nGiven the roots of two binary trees p and q, write a function to check if they are the same or not.\n\nTwo binary trees are considered the same if they are structurally identical, and the nodes have the same value.	#leetcode	func isSameTree(p *TreeNode, q *TreeNode) bool {\n  if p == nil && q == nil {\n    return true\n  }\n\n  if (p == nil && q != nil) || (p != nil && q == nil) {\n    return false\n  }\n\n  return p.Val == q.Val &&\n    isSameTree(p.Left, q.Left) &&\n    isSameTree(p.Right, q.Right)\n}	192:00:00	2025-04-29 10:59:00.967689+00	f
35	10	Operating system structure	#code	Modern operating systems can be described by using the following hierarchy:\n\n1. Kernel: The core of the operating system, responsible for managing system resources, memory, and processes.\n \n2. User Interface: Provides the graphical or command-line interface for users to interact with the system.\n\n3. System Services: Provide additional functionality and support for applications, such as file management, networking, security, and device drivers.\n\n4. Applications: End-user software that runs on top of the operating system, utilizing its services and resources.\n\nThe kernel provides the fundamental system functionality, and higher-level components build upon it to offer a complete user experience.	192:00:00	2025-03-20 03:59:05.771559+00	t
54	10	Thread	#code	Thread is the smallest unit of execution within a process. Threads allow a program to perform multiple tasks concurrently, sharing the same address space and resources as the parent process. This makes threads lightweight compared to processes, as they do not require separate memory allocations or context switches for most operations.\n\nThese are the basic components of the Operating System Thread.\n- Stack Space:   Stores local variables, function calls, and return addresses specific to the thread.\n- Register Set:   Hold temporary data and intermediate results for the thread‚Äôs execution.\n- Program Counter:   Tracks the current instruction being executed by the thread.	192:00:00	2025-03-15 12:28:16.268233+00	t
379	10	K8s	#code	Kubernetes is a modern, open-source platform designed to automate the deployment, scaling, and management of containerized applications. It provides orchestration and management for containers, such as Docker containers, in a distributed environment, across a cluster of machines. Kubernetes simplifies and automates container operations, ensuring high availability, scalability, and fault tolerance for your applications.\n\n### Key Features\n- Container Orchestration: Kubernetes manages the entire lifecycle of containers, including deployment, scaling, updates, and health monitoring.\n\n- Application Scaling: You can easily scale your applications up or down based on current load and demand, dynamically adjusting resources.\n\n- Configuration Management: Kubernetes allows you to manage application configurations and sensitive information (secrets) separately from the application code itself, improving security and manageability.\n\n- High Availability: Kubernetes ensures fault tolerance and high availability for your applications by distributing them across a cluster and automatically restarting or rescheduling containers in case of failures.\n\n- Service Discovery and Load Balancing: Kubernetes automatically discovers services within the cluster and manages load balancing across them, ensuring efficient traffic distribution.\n\n- Automated Deployment and Updates: Kubernetes streamlines and automates application deployments and updates, enabling continuous delivery practices.\n\n- Isolated and Secure Containers: Kubernetes provides container isolation and secure access control, enhancing the security of your application environment.	96:00:00	2025-03-15 07:01:01.768892+00	t
42	10	Client-server model	#code	The essence is in the presence of client processes and server processes. \n\nCommunication between clients and servers is often done by message passing. \n\nTo obtain a service, a client process constructs a message saying what it wants and sends it to the appropriate service. \n\nThe service then does the work and sends back the answer. \n\nIf the client and server happen to run on the same machine, certain optimizations are possible, but conceptually, we are still talking about message passing here.	192:00:00	2025-03-17 12:31:58.268875+00	t
315	10	Bulk insert	#postgres	‚ÄúBulk Insert‚Äù refers to the process of inserting a massive amount of data into a database using a single query or operation. This technique significantly improves performance when dealing with extensive data sets. In PostgreSQL, bulk insertion is typically achieved with the COPY command.\n\nThe COPY command is a special PostgreSQL feature designed for bulk data insertion. Instead of issuing many individual INSERT statements, you can prepare your data in a CSV (or similar) file and then use COPY to load the data quickly into your table. This method is one of the most efficient ways to handle large volumes of data.\n\nCOPY is built for high-volume data import or export from files or other sources. It can insert thousands or even millions of rows in one go, making it far more efficient than executing numerous individual INSERT commands.	192:00:00	2025-03-30 03:07:57.367845+00	t
450	10	MS numbers to words mapping	#memo	0   s, z\n1   t, d, th\n2   n\n3   m\n4   r\n5   l\n6   j, ch, sh\n7   c, k, g, q, ck\n8   v, f, ph\n9   p, b	24:00:00	2025-03-26 06:01:31.434223+00	t
449	10	Doctor names in clinic 8	#miscellaneous	–°–æ—ç–ª–º–∞ –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–Ω–∞\n–ù–∞–¥–µ–∂–¥–∞ –ê–Ω–¥—Ä–µ–µ–≤–Ω–∞ \n–ù–∏–∫–æ–ª–∞–π –ú–∏—Ö–∞–π–ª–æ–≤–∏—á	384:00:00	2025-05-05 10:57:14.29786+00	f
198	10	Integrative and emerging approaches	#code	### Lean\nInspired by lean manufacturing, Lean software development focuses on maximizing value by eliminating waste, fostering rapid delivery, and encouraging continuous improvement.  \n\n### DevOps\nNot strictly a development methodology but a cultural and technical movement, DevOps integrates software development with IT operations. It emphasizes continuous integration, delivery (CI/CD), automation, and collaboration to accelerate deployment and improve product quality.	192:00:00	2025-03-15 07:57:48.97687+00	t
286	10	Surrogate keys	#code	A surrogate key is an artificially created identifier used in a database to uniquely identify a record. Unlike natural keys, which are derived from actual business data (such as a Social Security Number or email address), surrogate keys have no business meaning and are generated solely for the purpose of facilitating a unique, stable reference for each record.\n\nSurrogate keys are a common best practice used to avoid the complications that arise from using natural keys, especially when the natural keys might change over time or are composed of multiple columns.	192:00:00	2025-03-20 17:51:38.150009+00	t
300	10	Merge Intervals\n\nGiven an array of intervals where intervals[i] = [starti, endi], merge all overlapping intervals, and return an array of the non-overlapping intervals that cover all the intervals in the input.	#leetcode	func merge(intervals [][]int) [][]int {\n    slices.SortFunc(intervals, func(a, b []int) int {\n        value := cmp.Compare(a[0], b[0])\n        if value == 0 {\n            return cmp.Compare(a[1], b[1])\n        }\n        return value\n    })\n\n    result := make([][]int, 0, len(intervals))\n    result = append(result, intervals[0])\n\n    for _, interval := range intervals[1:] {\n        last := result[len(result)-1]\n        if last[1] < interval[0] {\n            result = append(result, interval)\n            continue\n        }\n        \n        // last is a slice, so we can update it's array value inplace\n        if last[1] < interval[1] {\n            last[1] = interval[1]\n        }\n\n    }\n\n    return result\n}	192:00:00	2025-05-05 04:25:49.858071+00	f
216	10	UML diagram	#code	A Unified Modeling Language diagram is like a blueprint for software systems. It uses simple visuals to show how a system works and how different parts connect, making it easier for everyone involved to understand and discuss the system. \n\nThere are two main types of UML diagrams:\n\n### Structure diagrams\nThese show the static parts of a system‚Äîlike the system‚Äôs building blocks and how they‚Äôre arranged.\n\nExamples include:\n- Class Diagram:   Illustrates the system‚Äôs classes (like templates for objects), their properties, and how they relate to each other.\n\n- Component Diagram:   Shows the different parts or components of a system and how they depend on each other.\n\n- Deployment Diagram:   Depicts where the system‚Äôs software components are physically located on hardware.\n\n### Behavior Diagrams\nThese focus on the dynamic aspects‚Äîhow the system behaves and how its parts interact over time. \n\nExamples include:\n- Use Case Diagram:   Highlights the system‚Äôs functions from an end-user perspective, showing who uses the system and what they can do.\n\n- Sequence Diagram:   Details the order of interactions between objects in a system for a specific task.\n\n- Activity Diagram:   Represents the flow of activities or steps in a process, similar to a flowchart.\n  \nBy using these diagrams, teams can visualize and plan software systems more effectively, ensuring everyone has a clear understanding of how the system should function and how its parts fit together.	96:00:00	2025-03-17 04:21:13.477297+00	t
82	10	System calls	#code	System calls are a fundamental interface between an application and the operating system (OS). They provide a way for programs to request services from the OS kernel, allowing applications to perform operations that require higher privileges than those available in user mode. System calls enable applications to interact with hardware and access system resources in a controlled manner.\n\nIn other words, system calls allow user-level applications to request services such as file operations, process control, memory management, and communication with devices. They serve as a bridge between user applications and the kernel.\n\nhttps://disk.yandex.ru/i/CA5fsbbL9sIkqg	192:00:00	2025-03-22 03:14:14.805523+00	t
314	10	Migrations optimization	#code	Optimizing your database migrations is a critical task during application development. Poorly planned or unoptimized migrations can lead to extended downtimes and performance issues. Here are several recommendations for streamlining your migration process:\n\n- Break Migrations into Small Steps;\n- Optimize Indexes;\n- Leverage Asynchronous Migrations;\n- Refine Your Queries;\n- Set Execution Time Limits;\n- Utilize Monitoring Tools;\n- Test Migrations in a staging or development environment;\n- Plan for Rollbacks;      \n- Migrate Gradually;  \n- Keep a Detailed Migration Log;\n- Prioritize Backup and Data Security;	192:00:00	2025-03-26 04:30:19.702989+00	t
451	10	—Å–æ–∑–µ—Ä—Ü–∞—Ç—å, –æ–±–¥—É–º—ã–≤–∞—Ç—å, –Ω–∞–º–µ—Ä–µ–≤–∞—Ç—å—Å—è	#words	contemplate	384:00:00	2025-05-11 18:32:34.77903+00	f
325	10	Linked List Cycle\n\nGiven head, the head of a linked list, determine if the linked list has a cycle in it.\n\nThere is a cycle in a linked list if there is some node in the list that can be reached again by continuously following the next pointer. Internally, pos is used to denote the index of the node that tail's next pointer is connected to. Note that pos is not passed as a parameter.\n\nReturn true if there is a cycle in the linked list. Otherwise, return false.	#leetcode	/**\n * Definition for singly-linked list.\n * type ListNode struct {\n *     Val int\n *     Next *ListNode\n * }\n */\nfunc hasCycle(head *ListNode) bool {\n    if head == nil || head.Next == nil {\n        return false\n    }\n\n    slow := head\n    fast := head.Next\n\n    for fast.Next != nil && fast.Next.Next != nil {\n        if slow == fast {\n            return true\n        }\n        slow = slow.Next\n        fast = fast.Next.Next\n    }\n\n    return false\n}	192:00:00	2025-05-04 12:56:20.424332+00	f
339	10	Kafka consumer	#kafka	A Consumer is responsible for receiving data from Apache Kafka. Returning to our previous example, a moderation service might act as the Consumer. In this scenario, the service subscribes to the advertisement topic and, when a new ad is posted, it retrieves and examines it to ensure compliance with specified policies.\n\nKafka keeps track of which events the Consumer has already processed (using the internal __consumer__offsets topic), ensuring that once an event is successfully read, it won‚Äôt be delivered again. However, if you enable the option `enable.auto.commit = true`‚Äîthus delegating the tracking of the Consumer‚Äôs position in the topic entirely to Kafka‚Äîthere‚Äôs a risk of data loss. In production environments, the Consumer‚Äôs offset is typically managed manually, with developers explicitly determining when a read event should be committed.\n\nWhen a single Consumer can‚Äôt handle the volume of incoming events (for example, when there is an extremely high rate of new messages), you can deploy multiple Consumers grouped together into a Consumer Group. A Consumer Group functions similarly to a single Consumer but distributes the workload among its members, allowing each one to process a portion of the messages and thereby scaling up the overall throughput.	192:00:00	2025-03-31 03:25:17.368641+00	t
367	10	Promethues	#code	Prometheus is a tool specifically designed for collecting metrics. It is a powerful system providing robust capabilities for collecting, storing, and analyzing metric data, essential for monitoring the performance of applications and infrastructure.\n\nHere are some key features of Prometheus:\n\n- Multidimensional Metrics: Prometheus uses multidimensional metrics, which are essentially key-value pairs for each metric. This allows for flexible and efficient filtering and aggregation of data.\n\n- Data Collection via HTTP: Prometheus gathers data by scraping metrics endpoints over HTTP.  This means you instrument your applications to expose metrics at a specific URL, and Prometheus periodically requests this data. \n\n- Powerful Query Language (PromQL): Prometheus provides PromQL, a specialized query language for analyzing and filtering metric data. It enables the creation of complex queries to retrieve the exact information needed for dashboards, alerting, and analysis.  \n\n- Data Storage: Prometheus stores collected data in its own time-series database, optimized for fast metric retrieval. This database is designed for efficient storage and querying of time-stamped data, which is ideal for monitoring.\n\n- Alerting: Prometheus can be configured to trigger alerts based on predefined rules. This allows for rapid response to critical issues. Alert rules in Prometheus define conditions that, when met, generate alerts that can be routed to Alertmanager.	96:00:00	2025-03-20 03:54:25.697466+00	t
394	10	Trunk-Based Development (TBD)	#code	Trunk-based development (TBD) is a branching strategy where all developers integrate their code into a single shared branch‚Äîoften called the trunk, mainline, or main. This model minimizes merge conflicts and supports continuous integration (CI) and continuous delivery (CD) by ensuring the trunk is always in a deployable state. \n\nDevelopers commit small, incremental changes frequently, and pull requests are used for quick code reviews before merging. \n\nRelease branches are created only for deployments and are short-lived, while ongoing development remains on the trunk.\n\nTechniques like Branch by Abstraction help manage larger, longer-term changes without diverging from the trunk. \n\nMajor companies such as Google, Facebook, Spotify, and Atlassian use TBD to enhance collaboration, accelerate release cycles, and maintain high-quality code, making it a vital practice in modern agile and DevOps environments.	96:00:00	2025-03-15 15:33:21.845821+00	t
213	10	Singleton	#code	A #creational design pattern, which ensures that only one object of its kind exists and provides a single point of access to it for any other code.\n\nSingleton has almost the same pros and cons as global variables. Although they‚Äôre super-handy, they break the modularity of your code.\n\nYou can‚Äôt just use a class that depends on a Singleton in some other context, without carrying over the Singleton to the other context. Most of the time, this limitation comes up during the creation of unit tests.\n\nUsually, a singleton instance is created when the struct is first initialized. To make this happen, we define the getInstance method on the struct. This method will be responsible for creating and returning the singleton instance. Once created, the same singleton instance will be returned every time the getInstance is called.	384:00:00	2025-03-28 03:15:36.295231+00	t
207	10	Design patterns	#code	> Design patterns are typical solutions to commonly occurring problems in software design. They are like blueprints that you can customize to solve a recurring design problem in your code. It is a general concept for solving a particular problem.\n\nTo put it simply, a pattern is a general approach for solving a problem, which you will have to adjust to the specifics of your program.\n\nAll patterns can be categorized by their _intent_, or purpose. Here are the three main groups of patterns:\n1. Creational patterns provide object creation mechanisms that increase flexibility and reuse of existing code.\n    \n2. Structural patterns explain how to assemble objects and classes into larger structures, while keeping these structures flexible and efficient.\n    \n3. Behavioral patterns take care of effective communication and the assignment of responsibilities between objects.	96:00:00	2025-03-20 13:13:43.167357+00	t
55	10	Mainframe operating systems	#code	High-end operating systems for room-sized computers with thousands of hard discs and hight I/O capacity. It can be found in corporate data centers or research facilities and mostly used for large-scale commerce sites, banking, airline reservations, and servers for B2B transactions.\n\nMainframe OS are oriented towards processing many jobs at ones with high I/O capacity. They typically offer 3 kinds of services: batch, transaction processing, and timesharing. Batch system does routine jobs, transaction-processing system handles large number of small requests, time-sharing system allows multiple remote users to run jobs.	384:00:00	2025-03-23 16:20:37.698589+00	t
233	10	Composite	#code	A #structural design pattern that lets you compose objects into tree structures and then work with these structures as if they were individual objects.\n\nThe Composite became a pretty popular solution for the most problems that require building a tree structure. The Composite‚Äôs great feature is the ability to run methods recursively over the whole tree structure and sum up the results.	384:00:00	2025-04-07 08:19:52.403525+00	t
192	10	Object constructor	#code	Object #constructor is a special method that is used to initialize a new object. It defines initial values of object fields and can perform other necessary operations.\n\nIn Golang, object constructors are implemented as functions. Constructors help in solving the problem with uninitialized fields of an object (if we don't define an object constructor, then when we create a new object instance, all its fields will have default values, which in some cases, may lead to errors in the program operation). Constructors also help to discard global variables, which is also a good practice.\n\nGo has also a practice of using #idiomatic constructors that return a pointer to an object rather than the object itself. This is done for ease of use and is consistent with the principles of the language.	384:00:00	2025-04-01 10:58:25.17774+00	t
193	10	Functional options	#code	type Object struct {  \n    field1 string  \n    field2 int  \n}  \n  \ntype Option func(*Object)  \n  \nfunc WithField1(value string) Option {  \n    return func(obj *Object) {  \n       obj.field1 = value  \n    }  \n}  \n  \nfunc WithField2(value int) Option {  \n    return func(obj *Object) {  \n       obj.field2 = value  \n    }  \n}  \n  \nfunc NewObject(options ...Option) *Object {  \n    obj := &Object{  \n       field1: "default value",  \n       field2: 0,  \n    }  \n    for _, option := range options {  \n       option(obj)  \n    }  \n    return obj  \n}	192:00:00	2025-03-15 04:04:31.173328+00	t
282	10	SQL operators	#sql	### EXISTS\nChecks whether a subquery returns any rows. It is used to test for the presence of rows that meet certain criteria, returning true if the subquery finds any matching records.\n\nSELECT * FROM customers\nWHERE EXISTS (\n    SELECT 1 FROM orders\n    WHERE orders.customer_id = customers.id\n);\n\n\n### IN\nis used to compare a column‚Äôs value against a set of specific values or the result of a subquery. It returns TRUE if the column‚Äôs value matches any one of the provided values.  \n\nSELECT * FROM products WHERE category_id IN (1, 2, 3);\n\n\n### BETWEEN\nchecks whether a column‚Äôs value falls within a specified range, including both endpoints. It‚Äôs most commonly applied to numeric or date data.  \n\nSELECT * FROM sales WHERE sale_amount BETWEEN 1000 AND 5000;\n\n\n### LIKE\nis used to find strings that match a specific pattern or substring. It supports wildcard characters‚Äî% for any sequence of characters and _ for a single character‚Äîto allow more flexible pattern matching.  \n\nSELECT * FROM customers WHERE last_name LIKE 'Smith%';\n	192:00:00	2025-03-22 11:15:27.334578+00	t
412	10	Create array from a slice	#code	// scipt the obvious way with a cycle\n\n// use copy()\nslice := []int{1, 2, 3, 4, 5}\nvar arr [5]int\ncopy(arr[:], slice)\n\n// use unsafe pointer\nslice = []int{6, 7, 8, 9, 10}\narrPtr := (*[5]int)(unsafe.Pointer(&slice[0]))\narr = *arrPtr	96:00:00	2025-03-15 18:44:27.203999+00	t
283	10	MERGE	#sql	The MERGE operator‚Äîalso known as UPSERT or ON DUPLICATE KEY UPDATE in some database systems‚Äîis used to either insert a new record or update an existing one based on a given condition. \n\nSpecifically, it allows you to:\n1. Insert: Add a new record if no record with the specified key exists.\n2. Update: Modify an existing record with new data if a record with the specified key is already present.\n\nThis functionality is particularly useful for efficiently handling situations where you want to insert data only if it isn‚Äôt already there, or update it if it is, making MERGE a powerful tool for managing duplicates in tables.\n\nHowever, it‚Äôs important to note that the MERGE operator is not part of the SQL standard and its syntax can vary between different database management systems (DBMSs). For example, PostgreSQL and MySQL do not support MERGE directly; instead, they use alternative methods like INSERT ... ON CONFLICT DO UPDATE in PostgreSQL or INSERT ON DUPLICATE KEY UPDATE in MySQL.	96:00:00	2025-03-29 16:31:25.466283+00	t
150	10	Cache coherence	#code	Cache coherence is a property of multiprocessor systems that ensures all caches containing copies of the same data remain consistent with each other. In multiprocessor systems, each processor typically has its own cache to speed up memory access. However, this can lead to problems when multiple processors have copies of the same data in their caches, and each processor can modify its local copy independently of the other copies.\n\nCache coherence ensures the correct behavior of the system in such cases. It guarantees that all caches see the latest data changes and that all read and write operations are performed in the correct order, considering the interaction between processors.	192:00:00	2025-03-21 05:03:47.552409+00	t
91	10	poll	#code	poll is a system call that waits for events on a set of file descriptors, i.e. iteratively checks a list of file descriptors for new events.\n\n### Main features\n1, Monitors multiple file descriptors for read, write, and error conditions.\n2. More flexible than select, supporting a wider range of events.\n3. Portable across Unix-like systems.\n\n### Key distinctions from epoll\n1. Less efficient for numerous file descriptors due to its linear time complexity (O(n)).\n2. Uses a single structure to manage file descriptors and events.\n\n### Use case examples\n1. Network servers with a moderate number of connections.\n2. Applications requiring portability across different Unix systems.\n3. Commonly used in older systems or when the number of file descriptors is relatively small.\n\nhttps://disk.yandex.ru/i/-yMISgToN0py2Q	192:00:00	2025-03-21 09:19:44.098293+00	t
333	10	Copy List With Random Pointer\n\nA linked list of length n is given such that each node contains an additional random pointer, which could point to any node in the list, or null.\n\nConstruct a deep copy of the list. The deep copy should consist of exactly n brand new nodes, where each new node has its value set to the value of its corresponding original node. Both the next and random pointer of the new nodes should point to new nodes in the copied list such that the pointers in the original list and copied list represent the same list state. None of the pointers in the new list should point to nodes in the original list.	#leetcode	/**\n * Definition for a Node.\n * type Node struct {\n *     Val int\n *     Next *Node\n *     Random *Node\n * }\n */\n\nfunc copyRandomList(head *Node) *Node {\n    mp := make(map[*Node]*Node)\n\n    curr := head\n    for curr != nil {\n        mp[curr] = &Node{Val: curr.Val}\n        curr = curr.Next\n    }\n\n    curr = head\n    for curr != nil {\n        mp[curr].Next = mp[curr.Next] \n        mp[curr].Random = mp[curr.Random]\n        curr = curr.Next\n    }\n\n    return mp[head] \n}	384:00:00	2025-05-11 04:44:10.346665+00	f
342	10	Remove Nth Node From the List\n\nGiven the head of a linked list, remove the nth node from the end of the list and return its head.	#leetcode	/**\n * Definition for singly-linked list.\n * type ListNode struct {\n *     Val int\n *     Next *ListNode\n * }\n */\nfunc removeNthFromEnd(head *ListNode, n int) *ListNode {\n  slow, fast := head, head\n\n  for i := 0; i < n; i++ {\n    fast = fast.Next\n  }\n\n  if fast == nil {\n    return head.Next\n  }\n\n  for fast.Next != nil {\n    fast = fast.Next\n    slow = slow.Next\n  }\n\n  slow.Next = slow.Next.Next\n\n  return head\n}	96:00:00	2025-05-01 16:49:02.439178+00	f
346	10	Rotate List\n\nGiven the head of a linked list, rotate the list to the right by k places.	#leetcode	/**\n * Definition for singly-linked list.\n * type ListNode struct {\n *     Val int\n *     Next *ListNode\n * }\n */\nfunc rotateRight(head *ListNode, k int) *ListNode {\n    if head == nil || k == 0 {\n        return head\n    }\n\n    curr, n := head, 1\n  for curr.Next != nil {\n        curr = curr.Next\n        n++\n  }\n\n    if k % n == 0 {\n        return head\n    }\n\n    curr.Next = head\n    for i := 0; i < n-k%n; i++ {\n        curr = curr.Next\n    }\n\n    head = curr.Next\n    curr.Next = nil\n\n    return head\n}	384:00:00	2025-05-09 14:40:00.222592+00	f
204	10	Context	#code	In Go, the context package is essential for managing deadlines, cancellation signals, and request-scoped data across API boundaries and between goroutines. It provides a standardized way to handle operations that need to be canceled or have timeouts, ensuring efficient resource management and responsiveness in concurrent programs.\n\n### Main Use Cases of context\n1. Cancellation Signals: When an operation becomes redundant or exceeds its necessity, such as when a user aborts an action, the context package allows propagation of cancellation signals to terminate associated goroutines, preventing resource leaks.¬†\n\n2. Timeouts and Deadlines: To prevent operations from running indefinitely, contexts can enforce time limits. By setting timeouts or specific deadlines, functions can automatically cancel operations that exceed the allotted time, enhancing system reliability.¬†\n\n3. Passing Request-Scoped Data: Contexts facilitate the transfer of data specific to a request, such as authentication tokens or trace IDs, through various layers of an application without altering function signatures. This ensures consistent access to necessary data across different components.¬†\n\n\n### Types of Contexts\n- Background Context: Obtained using context.Background(), this is the default, empty context, often used at the application‚Äôs top level or in tests.\n\n- TODO Context: Created with context.TODO(), it indicates that the context‚Äôs specifics are undecided or will be determined later.\n\n- Cancelable Context: Generated via context.WithCancel(parentContext), it returns a derived context and a cancellation function. Invoking this function cancels the context, signaling all operations using it to terminate promptly.\n\n- Timeout Context: Created with context.WithTimeout(parentContext, duration), it sets a timeout for operations. If the operation doesn‚Äôt complete within the specified duration, the context is automatically canceled.\n\n- Deadline Context: Using context.WithDeadline(parentContext, deadline), this context is canceled when the specified time is reached, regardless of operation completion.\n\n- Value Context: Established with context.WithValue(parentContext, key, value), it allows associating arbitrary data with the context, enabling the passing of request-scoped values through the call chain.\n\nBy leveraging the context package, Go developers can build applications that are more robust, responsive, and easier to manage, especially in environments requiring high concurrency and precise control over operations.	96:00:00	2025-03-15 17:31:39.777965+00	t
61	10	0.0.0.0 interface: a misconception	#code	There's no such thing as a "0.0.0.0 interface". The IP address 0.0.0.0 has specific meanings in different networking contexts, but it doesn't represent an interface. Common Interpretations of 0.0.0.0:\n\n1. Default Route: In routing tables, 0.0.0.0 often indicates the default gateway. This means any traffic destined for a network not explicitly defined in the routing table will be forwarded to this gateway.   \n\n2. Unspecified Source Address: In some network protocols, 0.0.0.0 can be used as a placeholder for the source address when it's unknown or not yet assigned. For instance, during the DHCP discovery process.\n    \n3. Invalid Address: In most cases, 0.0.0.0 is considered an invalid address for general network communication.\n\nTo summarize: While 0.0.0.0 is a significant IP address in networking, it doesn't correlate to an interface. Interfaces have specific names (like eth0, wlan0, etc.) and IP addresses assigned to them.	96:00:00	2025-03-17 11:50:22.404985+00	t
303	10	EXPLAIN	#postgres	### EXPLAIN\nThe EXPLAIN command is used to obtain the execution plan of an SQL query without actually executing it. Running EXPLAIN before an SQL query allows you to see the plan that _would be_ chosen for execution, but without performing the actual data retrieval or modification. This is invaluable for query optimization, identifying potential bottlenecks, and assessing query performance.\n\nThe result of this command will be a textual or tree-structured representation of the query execution plan. This plan details the steps PostgreSQL would take to execute the query, such as sequential scans, index scans, join types (e.g., hash join, merge join, nested loop), and more.\n\n### EXPLAIN ANALYZE\nWhen ANALYZE is added to EXPLAIN PostgreSQL actually executes the query and measures its performance.  The output from EXPLAIN ANALYZE includes the same textual or tree-structured execution plan as EXPLAIN, but it is enhanced with additional information about the actual runtime and statistical data for each operation in the plan.\n\nThis includes metrics like:\n- Actual execution time for each node in the plan.\n- Number of rows produced by each node.\n- Startup cost and total cost estimates (from the planner).\n- Information about buffers used (shared buffer hits, reads, etc.).	96:00:00	2025-03-22 10:32:51.450188+00	t
85	10	TCP layers	#code	TCP/IP model is often described in terms of four layers:\n\n- Application Layer   includes protocols for specific applications, such as HTTP (Hypertext Transfer Protocol), FTP (File Transfer Protocol), and SMTP (Simple Mail Transfer Protocol).\n\n- Transport Layer   includes TCP and UDP (User Datagram Protocol), which is used for applications that require faster, connectionless communication.\n\n- Internet Layer   is primarily concerned with IP and routing.\n\n- Link Layer   deals with the physical network hardware and protocols used to connect devices.	192:00:00	2025-03-16 12:03:55.020209+00	t
281	10	WHERE vs HAVING	#sql	Both WHERE and HAVING are SQL clauses used for filtering data, but they serve different roles and operate at different stages of a query:\n\n### WHERE\n- Filters rows before any grouping or aggregation takes place.\n- Applies the condition to each individual row in the table, returning only those rows that meet the criteria.\n- Works with the columns in the original table data before any aggregate functions or GROUP BY operations are applied.\n\n### HAVING\n- Filters groups after the data has been grouped or aggregated.\n- Applies conditions to each group (for example, groups created by a GROUP BY clause) and returns only those groups that satisfy the condition.\n- Is used to filter on calculated values such as sums, averages, and other aggregates.\n\nIn essence, use WHERE to filter individual rows at the start of the query, and use HAVING to filter groups after aggregation has occurred.	96:00:00	2025-03-24 12:40:40.228074+00	t
266	10	Transaction management levels	#code	Transaction management is a crucial aspect of designing and developing database management systems (DBMS). Several approaches to transaction management exist, and the choice of approach depends on the requirements and characteristics of a specific system. Here are some common approaches:\n\n### Local Transactions\nLocal transactions apply only to a single database or resource. This approach is used in systems where all data operations are performed within a single DBMS.\n\nExample: Operations within a single database, where transactions are managed using SQL commands.\n\n### Distributed Transactions\nDistributed transactions apply to multiple databases or resources, which may reside on different servers. This approach is used in distributed systems and applications.\n\nExample: Transferring money between bank accounts on different servers, where each server has its own DBMS.\n\n### Application-Level Transaction Management\nTransaction management is performed at the application level, rather than at the database level. The application explicitly starts, commits, and rolls back transactions.\n\nExample: Using DBMS APIs in application code to manage transactions.\n\n### Object-Level Transaction Management\nTransaction management is performed at the level of objects or data models. Each object can have its own transaction, and they can be nested.\n\nExample: Using object-oriented databases or ORM (Object-Relational Mapping) with transaction support.\n\n### Message-Level Transaction Management\nTransactions are managed at the message level in asynchronous systems. Each message can be part of a transaction, and they can be atomic.\n\nExample: Using messages in message queues, such as Apache Kafka or RabbitMQ, with transaction support.\n\n***\n\nIt's important to note that these approaches are not mutually exclusive, and a system may use a combination of them. The choice of approach depends on various factors, such as the complexity of the system, the need for data consistency, and the performance requirements.\n\nhttps://disk.yandex.ru/i/RX95zKFUic3AgQ	96:00:00	2025-03-20 10:20:38.114629+00	t
424	10	Construct Binary Tree from Inorder and Postorder Traversal\n\nGiven two integer arrays inorder and postorder where inorder is the inorder traversal of a binary tree and postorder is the postorder traversal of the same tree, construct and return the binary tree.	#leetcode	func buildTree(inorder []int, postorder []int) *TreeNode {\n  n := len(postorder)\n\n  if n == 0 {\n    return nil\n  }\n\n  i := slices.Index(inorder, postorder[n-1])\n  return &TreeNode{\n    Val:   postorder[n-1],\n    Left:  buildTree(inorder[:i], postorder[:i]),\n    Right: buildTree(inorder[i+1:], postorder[i:n-1]),\n  }\n}	96:00:00	2025-05-01 14:58:53.050192+00	f
405	10	JWT tokens pair auth steps	#code	1. Authorization request;\n  - Set ‚ÄúAccess-Control-Allow-Credentials‚Äù header;\n  - Read request payload;\n  - Validate user agains the database;\n  - Validate password against database;\n  - Generate a pair of tokens (auth, refresh);\n  - Generate and set refresh cookie;\n  - Return a pair of tokens.\n  \n2. Refresh token request:\n  - Set ‚ÄúAccess-Control-Allow-Credentials‚Äù header;\n  - Iterate over request cookies until cookie name matches;\n  - Parse and validate refresh token;\n  - Validate user against the database;\n  - Generate a pair of tokens (auth, refresh);\n  - Generate and set refresh cookie;\n  - Return a pair of tokens.	48:00:00	2025-03-15 10:53:20.05789+00	t
273	10	Why don't index every column?	#code	While indexes speed up query execution, they also consume storage space and increase overhead during data modifications. Therefore, indexing every column is not advisable as it can lead to excessive memory consumption by the database.\n\nFurthermore, every time a new record is inserted, indexes need to be updated or rebuilt. This index maintenance process consumes both time and processor resources, thus increasing server load.\n\nFor example, rebuilding a B-tree index, commonly used in databases like PostgreSQL, has a time complexity of O(log n), where 'n' is the number of records in the table. To illustrate, if a table contains 1,000 records, index rebuilding might take approximately 10 units of time (in arbitrary units for simplicity). If the table grows to 10,000 records, rebuilding could take around 100 units of time. This demonstrates that as the number of records in a table increases, the time required for index maintenance grows logarithmically, but still contributes to overall server load, especially with frequent write operations. Excessive indexing amplifies this overhead.	192:00:00	2025-03-25 03:47:10.219688+00	t
425	10	sync.Mutex	#golang	Golang‚Äôs sync.Mutex is a highly optimized, low‚Äêlevel mutual exclusion primitive designed to be fast in the uncontended case while still handling contention efficiently. Under the hood, it uses a mix of atomic operations and OS-assisted blocking mechanisms.\n\nThe Mutex is defined roughly as follows:\ntype Mutex struct {\n    state int32  // holds bits that indicate lock status and contention\n    sema  uint32 // used as an index for the runtime semaphore (futex-like mechanism)\n}\n\nIn the uncontended case, acquiring a sync.Mutex involves only an atomic compare-and-swap operation. This is a fast, user-space operation that does not require a system call or context switch. The lock is acquired and released without any kernel involvement.\n\nHowever, when there is contention (i.e., when another goroutine already holds the lock), the goroutine that cannot acquire the lock will eventually enter a slow path. This slow path involves:\n\n- Marking the mutex as contended.\n- Calling into runtime functions (like runtime_Semacquire) which use OS-level primitives (e.g., futex on Linux).\n- This call typically leads to a system call, and if the goroutine needs to wait, it will be suspended‚Äîtriggering a context switch so that another goroutine can run.\n\nThus, no system call or context switch is necessary in the common uncontended scenario, but under contention, system calls and context switches become involved to efficiently block and wake waiting goroutines.	24:00:00	2025-03-15 09:54:47.943072+00	t
385	10	Local vs cloud CI/CD platforms	#cicd	### Local CI Server\nA self-hosted CI server is managed like any other server. This means it requires installation, configuration, and ongoing maintenance, including updates and security patches. \n\n> Failure of a local CI server can block testing, development, and deployment processes, potentially causing significant disruptions.\n    \n### Cloud CI Server\nCloud-based CI servers are offered as a service, eliminating the need for user-side maintenance. The cloud provider handles all server maintenance and infrastructure management. Users can typically start using the service immediately with minimal setup. Cloud platforms offer scalability, and service level agreements (SLAs) often guarantee server uptime and reliability.	96:00:00	2025-03-17 10:17:17.338284+00	t
66	10	Process lifecycle	#code	Process Lifecycle.\n1. Creation ‚Äî a process is created using the fork() system call, which creates a copy of the parent process.\n2. Execution ‚Äî the new process can replace its program code using the exec() system call, loading a new program into memory.\n3. Termination ‚Äîa process ends when it completes its task, encounters an error, or is terminated by a signal (e.g., kill command).\n\nExample.\nWhen you run a command like ls in the terminal:\n1. The shell (e.g., bash) forks a new process.\n2. The new process executes the ls program using exec().\n3. The ls process runs, displays the directory contents, and then terminates.	384:00:00	2025-03-15 17:50:02.583531+00	t
179	10	Liskov substitution	#code	> If, for each object, 01 of type S there is an object 02 of type T such that for all programs P defined in terms of T, the behavior of P is unchanged when 01 is substituted for 02, then S is a subtype of T.\n\nIn layman's terms, two types are _substitutable_ if their exhibited behavior follows exactly the same contract, thereby making it _impossible_ for _callers_ to distinguish between them. This means that the subclass must follow the same interface and contract as the superclass, and must not add new constraints or modify existing ones.\n\nEach Go type defines an implicit interface consisting of all the methods it implements. It allows the Go compiler to pass an object instance as a substitute to a function or method that expects a particular interface as its argument.\n> If it looks like a duck, and it quacks like a duck, then it's a duck.	192:00:00	2025-03-26 16:50:54.329107+00	t
64	10	Types of processes	#code	1. Foreground Processes:\n  - Run interactively in the terminal.\n  - Require user input and display output directly to the terminal.\n\n2. Background Processes:\n  - Run independently of the terminal.\n  - Can be started by appending & to a command (e.g., sleep 100 &).\n\n3. Daemons:\n  - Background processes that provide system services (e.g., sshd, httpd).\n  - Typically started during system boot and run indefinitely.	192:00:00	2025-03-16 03:38:02.006069+00	t
65	10	Process hierarchy	#code	- Processes are organized in a tree-like hierarchy. \n- The first process‚Äîinit()* or systemd‚Äîhas a PID of 1 and is the ancestor of all other processes.\n- Child processes inherit certain attributes from their parent processes.\n\n*init() is the first process that is started during the system boot. When the init process terminates, the system is rebooted or shut down. The PID (Process ID) of the init process is always equal to 1.	192:00:00	2025-03-15 09:54:16.014606+00	t
365	10	Cache invalidation	#code	Cache invalidation is the process of removing or marking data in the cache as no longer valid or ‚Äústale.‚Äù This is typically triggered when the original data source (like a database) is updated.\n\nImportantly, cache invalidation is not usually an automatic process. Developers must implement logic within their applications to detect data changes and then invalidate the corresponding cache entries. This often involves:\n\n- Manual Invalidation: Explicitly removing cache entries when data is updated.\n- Time-Based Expiration (TTL ‚Äî Time To Live): Setting an expiry time for cached data. After this time, the cache entry is considered stale and will be refreshed on the next request.\n- Event-Based Invalidation: Using mechanisms (like database triggers or message queues) to signal cache invalidation when data changes occur.\n\nChoosing the right invalidation strategy depends on the application's specific requirements for data consistency, performance, and complexity.	96:00:00	2025-03-20 15:58:39.411867+00	t
263	10	2PC	#code	Two-Phase Commit is a classic algorithm for managing transactions in distributed systems. Its primary goal is to guarantee the atomicity (the ‚Äúall or nothing‚Äù principle) of a transaction, even if it spans multiple distributed resources or nodes. \n\n### 2PC Phases\n\n1. Prepare Phase:\n    - A #coordinator, typically a central component, initiates the process. It sends a ‚Äúprepare-to-commit‚Äù request to all #participants in the transaction (resources or nodes involved in the transaction).\n    - Each participant, upon receiving the prepare request:\n        - Performs its part of the transaction (e.g., necessary database operations).\n        - Guarantees that it can either commit the changes or roll them back if required. To ensure this, the participant typically writes intentions to a transaction log and sets locks on the involved resources to prevent them from being modified by other transactions.\n        - Sends a response back to the coordinator. The response can be:\n            - ‚ÄúVote Commit‚Äù: The participant is ready to commit its part of the transaction.\n            - ‚ÄúVote Abort‚Äù: The participant cannot successfully complete its part of the transaction (e.g., due to an error, data consistency violation, service failure).\n\n2. Commit Phase:  \n    After the coordinator has received responses from all participants, it decides on the transaction's outcome:\n    \n    - Successful Commit: If all participants responded with ‚ÄúVote Commit,‚Äù the coordinator sends a ‚ÄúCommit‚Äù command to all participants. Participants, upon receiving the command, permanently commit their changes, release the locks, and respond to the coordinator with successful completion.\n    - Rollback: If at least one participant responded with ‚ÄúVote Abort‚Äù or did not respond within a specified timeout period, the coordinator decides to abort the entire transaction. It sends a ‚ÄúRollback‚Äù command to all participants. Participants, upon receiving the command, undo all changes made within the transaction, release the locks, and respond to the coordinator with the rollback confirmation.	192:00:00	2025-03-18 04:03:25.170031+00	t
387	10	Importance of security in CI/CD	#cicd	Security is paramount in CI/CD environments. These platforms often have access to sensitive information, such as API keys, repository credentials, database access details, and even usernames and passwords. A misconfigured CI/CD server can be a significant security vulnerability, making it an attractive target for malicious actors. Exploits can range from deploying compromised software to public access, selling proprietary software, or blackmailing owners and users. Robust CI/CD platforms should incorporate proven mechanisms for securely storing sensitive data and strictly control access to logs and repositories.	192:00:00	2025-05-07 11:40:03.984135+00	f
318	10	Filtered indexes	#postgres	Filtered indexes are a specialized type of index available in some relational databases, including PostgreSQL. They let you build an index only on a portion of the data in a table‚Äîspecifically, the rows that meet a certain condition (i.e., pass a filter). This approach is especially useful when you want to reduce the size of your index and boost query performance by indexing just the necessary rows.\n\n### Advantages\n- Space Efficiency: Since filtered indexes include only the rows that satisfy a specified condition, they require less disk space.\n- Improved Performance: A smaller index means the database can process it more quickly, thereby speeding up queries that rely on it.\n- Reduced Maintenance: With less data to manage, tasks such as rebuilding the index can be completed faster and with lower overhead.\n\n### Example\nCREATE INDEX idx_filtered_example\nON your_table (column_name)\nWHERE condition;\n\nA practical use for a filtered index might be when you want to create an index only for active records in a table or for entries that meet a specific criterion. However, remember that filtered indexes are not universally beneficial; they work best when you are sure that a particular subset of data is queried frequently and you aim to enhance the performance of those queries.	96:00:00	2025-03-21 14:46:11.946019+00	t
210	10	Abstract factory	#code	A #creational design pattern that provides an interface for creating families of related or dependent objects without specifying their concrete classes.\n\nThis pattern allows a system to be independent of the way its objects are created, composed, and represented. It is particularly useful when a system should support multiple product families but only use objects from one family at a time.	192:00:00	2025-03-17 17:43:49.875125+00	t
63	10	Process states	#code	1. New ‚Äî Newly Created Process (or) being-created process.\n2. Ready ‚Äî After the creation process moves to the Ready state, i.e. the process is ready for execution.\n3. Running ‚Äî Currently running process in CPU (only one process at a time can be under execution in a single processor).\n4. Wait (or Block) ‚Äî When a process requests I/O access.\n5. Complete (or Terminated) ‚Äî The process completed its execution.\n6. Suspended Ready ‚Äî When the ready queue becomes full, some processes are moved to a suspended ready state\n7. Suspended Block ‚Äî When the waiting queue becomes full.\n\nhttps://media.geeksforgeeks.org/wp-content/uploads/20250122155935521530/state.webp	384:00:00	2025-03-23 06:09:20.438351+00	t
158	10	Golang inteface	#code	An interface is a type that specifies a collection of method signatures.\n\nIt acts like a contract, outlining the methods that any type implementing the interface must provide. Think of it as a blueprint that defines the functionalities required, without specifying how those functionalities are implemented.\n\n### Key Points\n- Method signatures:  ¬†An interface defines methods by specifying their names, parameter types, and return types.\n\n- Implementation:  ¬†Any type (struct, int, string, etc.) can implement an interface by providing the defined methods.  A type is said to "satisfy" the interface if it implements all the methods with the exact signatures.\n\n- Benefits:  ¬†Interfaces promote code reusability by allowing functions to work with any type that implements the interface. This makes code more flexible and generic.\n\n> In Go, interface implementation is _implicit_.\n\nhttps://disk.yandex.ru/i/Gf0R8cl_g0QlNg	192:00:00	2025-03-23 06:44:03.959437+00	t
137	10	Benefits of using Linux containers	#code	1. Portability:   Containers run consistently across different environments since they rely only on the host kernel.\n\n2. Resource Efficiency:   Containers share the host kernel, making them much faster to start and consume fewer resources compared to VMs.\n\n3. Isolation:   Applications in containers are isolated from each other and the host system, improving security and stability.\n\n4. Scalability:   Containers can be easily scaled up or down to meet changing demands.	96:00:00	2025-03-20 11:15:01.809914+00	t
221	10	Strategy	#code	Is a #behavioral design pattern that turns a set of behaviors into objects and makes them interchangeable inside original context object.\n\nThe original object, called #context, holds a reference to a #strategy object. The context delegates executing the behavior to the linked strategy object. In order to change the way the context performs its work, other objects may replace the currently linked strategy object with another one.\n\nhttps://disk.yandex.ru/i/fSfAuUff_AtdXw	384:00:00	2025-04-01 11:02:06.576803+00	t
62	10	Process attributes	#code	Process Attributes help the OS to manage and regulate how processes are carried out. PA are stored in a data structure called process control block or task control block.\n\nKey process attributes:\n1. Process ID ‚Äî a unique number assigned to each process, so the operating system can identify it.\n2. Process State ‚Äî running, waiting, ready to execute, and others.\n3. CPU Scheduling Info ‚Äî processes' priority and other data that helps OS to decide which process to run next.\n4. I/O ‚Äî information about I/O devices the process is running.\n5. Accounting ‚Äî tracks how long the process has run, the amount of CPU time used, and other resource usage data.\n6. Memory Management ‚Äî details about the memory space allocated to the process, including where it is loaded in memory and the structure of its memory layout (stack, heap, etc.).\n\nhttps://media.geeksforgeeks.org/wp-content/uploads/20230702175558/Add-a-subheading(1).png	192:00:00	2025-03-25 04:36:52.375269+00	t
77	10	Thread synchronization	#code	Since threads share memory, proper synchronization is critical to avoid issues like race conditions. \n\nCommon synchronization mechanisms include:\n1. Mutexes:   Ensure only one thread can access a shared resource at a time.\n2. Condition Variables:  Allow threads to wait for specific conditions to be met.\n3. Semaphores:   Control access to a shared resource by a limited number of threads.	384:00:00	2025-03-30 11:29:27.690676+00	t
176	10	YAGNI	#code	You Ain't Gonna Need It is a design principle that suggests not to include functionality that is not currently required. Avoid building complex and redundant solutions based on assumptions about future needs.\n\nOften developers tend to add features assuming that they may be needed in the future. However, this can lead to redundancy and code complexity. Instead, we should focus on current requirements and add functionality only when needed. This helps to avoid unnecessary complexity, speed up development and improve programm support.	384:00:00	2025-03-27 10:45:23.314968+00	t
168	10	Firewall	#code	A firewall is a security system that monitors and controls the flow of network traffic between trusted and untrusted networks. In simple terms, it acts as a barrier‚Äîallowing legitimate traffic to pass while blocking unauthorized or potentially harmful data packets.\n\n### Key Points:\n- Traffic Filtering:   Firewalls use a set of preconfigured security rules to decide whether to allow or block specific network traffic.\n- Deployment Options:   They can be implemented as hardware appliances, software programs, or cloud-based solutions.\n- Types of Firewalls:   Basic types include packet-filtering firewalls and stateful inspection firewalls, while advanced options such as Next-Generation Firewalls (NGFW) offer deep packet inspection, application-level awareness, and integrated intrusion prevention.\n\n- Purpose:   Firewalls protect networks from external threats like hackers and malware, and can also help segment internal networks to minimize damage if a breach occurs.\n\nThese systems form the first line of defense in a comprehensive cybersecurity strategy, ensuring that only trusted data enters your network.\n\n[^1]: https://www.cisco.com/site/us/en/learn/topics/security/what-is-a-firewall.html\n[^2]: https://www.techtarget.com/searchsecurity/definition/firewall	192:00:00	2025-03-19 08:33:00.419159+00	t
259	10	ACID	#code	ACID is an acronym that stands for #atomicity, #consistency, #isolation, and #durability ‚Äî fundamental properties of database transactions in (RDBMS. \nLet's break down each property:\n\n- Atomicity:  \n  This property ensures that a transaction is treated as a single, indivisible unit of work. Either all operations within a transaction are successfully completed, or if any part fails, the entire transaction is rolled back, and no changes are applied to the database.\n    \n- Consistency:  \n  Consistency ensures that a transaction brings the database from one valid state to another valid state. In other words, a transaction must not violate any defined database constraints, rules, or integrity conditions.\n    \n- Isolation:  \n  Isolation ensures that concurrent transactions are executed in such a way that the intermediate state of one transaction is not visible to other transactions until it is successfully completed (committed).\n    \n- Durability:  \n  Durability ensures that once a transaction has been committed, the changes made by that transaction are permanent and will survive even system failures such as power outages or crashes.	96:00:00	2025-03-19 10:09:23.967044+00	t
177	10	Single responsibility	#code	> In any well-designed system, objects should only have a single responsibility.\n\nThe SRP principle states that a class should have only one reason to change‚Äîthat is, it should focus on one specific task. Consider this simple example:\n\nYou have an object Order with ProcessOrder method that processes the order (business logic) and creates a log record. Here, logging logic should not be implemented directly in the ProcessOrder, but rather embedded as a separate Logger object into Order. This way, changes in one object will not effect the other.	384:00:00	2025-03-17 04:06:19.547019+00	t
129	10	TLS vs SSL	#code	SSL\n- Older protocol: While still commonly used in everyday language, SSL is an older protocol for encrypting data transmitted over the internet.¬†\n- Security concerns: Due to vulnerabilities, SSL is now considered insecure. ¬†\n\nTLS\n- Successor to SSL: TLS is the modern, secure protocol for encrypting data transmitted over the internet. ¬†\n- Enhanced security: It offers stronger encryption, authentication, and data integrity compared to SSL. ¬†\n- Commonly used: TLS is the standard for securing online communications.	192:00:00	2025-03-15 08:48:20.2179+00	t
398	10	CRFS Attacks	#code	CSRF (Cross-Site Request Forgery) attacks exploit authenticated user sessions to execute unauthorized actions on web applications. Attackers trick users into unknowingly sending requests, leveraging browser mechanisms like cookies.  \n\n### Prevention Methods\n1. CSRF Tokens:  Unique, session-bound tokens included in requests that modify data. The server validates the token before processing actions, preventing unauthorized requests.  \n\n2. Referer Header Checks:  Servers verify request origins, but this method is unreliable due to browser privacy settings and potential spoofing.  \n\n3. SameSite Cookies:  Setting SameSite=Strict or Lax restricts cookie transmission to same-site requests, mitigating CSRF risks.  \n\n4. Double-Submit Cookie Technique:  A random value is sent both as a cookie and in the request body. If they don't match, the request is rejected.  \n\n5. Origin Header Validation:  Ensures requests originate from a trusted domain but is not foolproof due to potential DNS manipulation.  \n\n6. Least Privilege & Access Control:  Restricting user permissions limits the potential damage of a successful CSRF attack.  \n\nA layered approach combining these strategies enhances protection against CSRF vulnerabilities.	96:00:00	2025-03-16 08:20:51.365487+00	t
140	10	Docker container	#code	A Docker container is a lightweight, standalone, and executable package that includes everything needed to run an application: code, runtime, system tools, libraries, and settings. Containers isolate applications from each other and the underlying infrastructure, ensuring consistent performance across different environments. \n\nContainers are working instances that can change their state: you can change something in the file system, start a new process, etc. They are created from Docker images and provide a portable and efficient way to deploy applications.	192:00:00	2025-03-21 06:21:22.834786+00	t
280	10	JOINs	#sql	### Types of JOINs\n1. INNER JOIN  \n   Shows you the intersection of the datasets from both tables.\n\n2. LEFT JOIN (or LEFT OUTER JOIN)  \n   If there is no matching row in the right table, columns from the right table will contain NULL values in the result set.\n\n3. RIGHT JOIN (or RIGHT OUTER JOIN). \n   A RIGHT JOIN is similar to a LEFT JOIN, but it behaves in the opposite way.\n\n4. FULL JOIN (or FULL OUTER JOIN)  \n   If there is no match in one of the tables, the columns originating from the table without a match will contain NULL values. \n\n5. CROSS JOIN (or CARTESIAN JOIN):  \n   A CROSS JOIN (also known as a Cartesian Join) combines every row from the left table with every row from the right table. This results in a Cartesian product with x * y rows.  \n   \n   > This type of JOIN is rarely used in production queries unless you specifically need every possible combination of rows, as it can generate very large result sets, potentially impacting performance.\n\n### JOINs vs. Subqueries\n\nThe choice between using JOINs or subqueries depends on the specific task and performance considerations:\n\n- JOINs are more efficient for combining large datasets from multiple tables, especially when the columns involved in the join are properly indexed.\n    \n- Subqueries (queries nested within another query) can sometimes make complex logic more readable and easier to understand, especially for breaking down a problem into smaller, manageable steps. However, poorly written subqueries can lead to performance bottlenecks, particularly correlated subqueries (subqueries that depend on the outer query).	192:00:00	2025-03-27 09:12:56.831426+00	t
41	10	Hybrid kernel	#code	A hybrid kernel is a type of operating system kernel that combines elements of both monolithic kernels and microkernels. It aims to take advantage of the strengths of both architectures while minimizing their weaknesses.\n\nLike a microkernel, a hybrid kernel can support a modular design where certain services (like device drivers, file systems, and network protocols) can run in user space rather than kernel space. This can enhance system stability and security, as faults in these services are less likely to crash the entire system.\n\nHybrid kernels often include some core services (like process management and memory management) in kernel space, similar to monolithic kernels. This can lead to better performance because these services can communicate more quickly without the overhead of context switching between user space and kernel space.\n\nSome well-known operating systems that use a hybrid kernel architecture include Windows NT (and its successors like Windows 10) and macOS. These systems incorporate features of both monolithic and microkernel designs.	384:00:00	2025-03-29 05:39:59.223528+00	t
369	10	Alertmanager	#code	Alertmanager is a component of the Prometheus monitoring system. It's responsible for managing alerts and notifications based on metric data and alert rules defined in Prometheus. Alertmanager handles de-duplicating, grouping, and routing alerts.\n\nKey functions and features of Alertmanager include:\n\n- Integration with Prometheus: Alertmanager directly integrates with Prometheus, receiving alerts from it based on the alert rules defined in Prometheus' configuration.  \n\n- Alert Grouping: Alertmanager can group alerts related to the same event or issue into a single notification. This prevents alert fatigue by reducing the number of similar alerts flooding notification channels. \n\n- Filtering and Processing Alerts: Alertmanager provides flexible options for configuring alert filtering rules. You can define which alerts to send, which to ignore, and even manipulate the text of alert messages.  \n\n- Integration with Various Notification Channels: Alertmanager supports various notification channels, such as email, Slack, PagerDuty, webhooks, and others.\n\n- Flexible Alert Rule Configuration: Alertmanager allows defining alert rules in configuration files. These rules can include conditions that trigger alerts, as well as settings for alert text and format. \n\n- High Availability (HA): Alertmanager can be deployed in a high-availability mode to ensure reliable alert delivery. HA deployments ensure that alerts are not missed even if one Alertmanager instance fails.	96:00:00	2025-03-19 04:27:37.3088+00	t
181	10	Dependency inversion	#code	> Hight-level modules should not depend on low-level modules. Both should depend on abstractions. Abstractions should not depend on details. Details should not depend on abstractions.\n\nThe DIP (dependency inversion principle) essentially summarizes the rest of the SOLID principles. Dependency should be based on abstractions, not on specific cases. That is, classes should not depend on concrete implementations of other classes, but on abstract interfaces. This reduces code dependency and increases code flexibility and maintainability.	384:00:00	2025-04-05 12:15:10.385223+00	t
130	10	HTTPS	#code	Hypertext Transfer Protocol Secure:\n- Secure version of HTTP: HTTPS is the secure version of HTTP, the protocol used for transferring data on the web. ¬†\n- Uses TLS: HTTPS uses TLS to encrypt communication between a web server and a web browser. ¬†\n- Indication of security: Websites using HTTPS typically have a padlock icon in the address bar, indicating a secure connection. ¬†\n\nWhy is HTTPS important?\n- Data privacy: Protects sensitive information like passwords, credit card numbers, and personal data from being intercepted.\n- Website credibility: Builds trust with users as it indicates a secure website. ¬†\n- Search engine ranking: Google and other search engines favor HTTPS websites in search results.	192:00:00	2025-03-20 07:11:58.028078+00	t
88	10	UDP	#code	User Datagram Protocol is a lightweight, connectionless *transport layer* protocol used for transmitting data over a network. Unlike TCP, UDP does not establish a connection before sending data and does not guarantee delivery, order, or error correction. It is designed for speed and efficiency, making it suitable for applications where low latency is more important than reliability.\n\n### Key Features of UDP:\n1. Connectionless: No handshake or connection setup is required before sending data.\n2. Low Overhead: Minimal protocol overhead due to the lack of error checking, sequencing, or acknowledgment mechanisms.\n3. Unreliable Delivery: Does not guarantee that packets will arrive at their destination or in the correct order.\n4. No Congestion Control: Does not adjust transmission rates based on network conditions.\n5. Supports Multicast and Broadcast: Can send data to multiple recipients simultaneously.\n\n### Use Case Examples:\n1. Real-Time Applications:  \n   - Video streaming (e.g., YouTube, Netflix)\n   - Voice over IP (VoIP) (e.g., Skype, Zoom)\n   - Online gaming\n2. DNS (Domain Name System):  \n   - Resolving domain names to IP addresses.\n3. IoT (Internet of Things):  \n   - Sending sensor data or status updates.\n4. Broadcast/Multicast Services:  \n   - Live broadcasting (e.g., sports events, news).\n5. Simple Query-Response Protocols:  \n   - SNMP (Simple Network Management Protocol) for network monitoring.\n6. Time-Sensitive Applications:  \n   - NTP (Network Time Protocol) for synchronizing clocks.\n\nUDP is ideal for scenarios where speed and efficiency are prioritized over reliability, and where occasional packet loss is acceptable.	96:00:00	2025-03-18 11:15:54.5118+00	t
297	10	Debugging slow queries	#code	Slow queries hinder application performance. Debugging them involves several key steps.\n\n1. First, use database monitoring tools like EXPLAIN to analyze query execution plans and identify bottlenecks.  \n2. Enable query logging to track execution times. \n3. Profile application code (especially ORM usage) to pinpoint slow queries within your application.\n4. Manually analyze slow queries, focusing on indexes, data retrieval, joins, and filters.\n5. Optimize indexes for efficient data access. \n6. Review database and application architecture for potential improvements like table restructuring or caching. 7. Limit data retrieval using techniques like LIMIT clauses. 8. Fine-tune DBMS configuration parameters like buffer sizes. \n9. Implement caching for frequently used queries. \n10. Finally, continuously test and monitor database performance to proactively address issues.  \n\nThese steps will help you effectively diagnose and optimize slow queries, ensuring a responsive system.	96:00:00	2025-03-15 17:40:15.17806+00	t
302	10	Query planner	#postgres	In PostgreSQL, the query planner is a crucial component responsible for optimizing and planning the execution of SQL queries. Its main task is to select the most efficient execution plan for a given query, taking into account table structures, indexes, data volume, and other parameters. The PostgreSQL planner ensures that queries are executed as quickly and efficiently as possible.\n\nHere is how it works:\n1. SQL Query Analysis:  \n   When a client sends an SQL query to the PostgreSQL server, the planner first analyzes this query. It determines the query's structure, identifies which tables and columns are involved, understands filtering conditions, and so on.\n\n2. Execution Plan Generation:  \n   Based on the query analysis, the planner generates various possible execution plans. Each plan represents a method for executing the query. This can include choices about index usage, the order of table joins, sorting methods, filtering techniques, and more.\n\n3. Plan Comparison and Selection:  \n   The planner estimates the cost of each plan, using statistics about tables and indexes. It then selects the plan with the lowest estimated cost as the most optimal execution plan. Optimization may involve choosing specific indexes, deciding on the order of join operations, and filtering data efficiently.\n\n4. Query Execution:  \n   After selecting the optimal plan, PostgreSQL proceeds to execute the SQL query according to the chosen execution plan. Data is retrieved, filtered, and sorted as specified in the plan.\n\n5. Execution Monitoring:  \n   During query execution, PostgreSQL monitors its progress. It can dynamically adjust the execution plan on-the-fly if it detects that the current plan is not as efficient as initially predicted. This is a form of adaptive query optimization.\n\n6. Query Completion:  \n   Upon completion of the query execution, the result is sent back to the client.\n\nThe PostgreSQL query planner plays a pivotal role in ensuring database performance. It ensures optimal utilization of indexes and server resources, allowing for efficient processing of queries, even with large volumes of data.\n\nPostgreSQL provides the commands EXPLAIN and EXPLAIN ANALYZE, which are used to analyze SQL query execution and query plans.	192:00:00	2025-04-01 08:44:39.822975+00	t
240	10	Declarative programming	#code	Declarative programming focuses on _what_ the program should accomplish without explicitly detailing _how_ to achieve it. This paradigm expresses the logic of computation without describing its control flow. Examples include SQL for database queries and HTML for webpage structure.\n\nhttps://pictures.s3.yandex.net/resources/Untitled_5_1701722211.png	192:00:00	2025-03-23 16:07:29.865629+00	t
407	10	Distributed DNS	#code	Distributed DNS offers an alternative to the traditional hierarchical Domain Name System by replacing centralized control with a decentralized, resilient, and scalable network for resolving domain names. Instead of relying on a tree-like structure‚Äîstarting with root servers, then TLD servers, and finally authoritative name servers‚Äîdistributed DNS leverages technologies such as peer-to-peer networking, distributed hash tables (DHTs), and blockchain to store and retrieve DNS records.\n\nIn a traditional DNS setup, while many servers contribute to redundancy, the reliance on a few central points (like root and TLD servers) creates vulnerabilities. Compromise or failure at these key junctures could disrupt significant parts of the Internet and open the door to censorship. \n\nDistributed DNS, by contrast, spreads the resolution process across numerous independent nodes. For instance, in DHT-based systems, nodes store and look up DNS records using consistent hashing, ensuring that even if some nodes fail, others can continue to provide the necessary information. Similarly, blockchain-based DNS uses a tamper-proof ledger to record domain registrations and DNS data, with consensus mechanisms ensuring the integrity of the data. Peer-to-peer implementations also allow nodes to directly share and cache records, further enhancing resilience and reducing latency bottlenecks.\n\nHowever, these decentralized approaches come with challenges. They must address issues of consistency, ensuring that all nodes have up-to-date records; security, protecting against threats like Sybil and routing attacks; and performance, as distributed networks can introduce higher latencies compared to traditional systems. Governance and interoperability with the existing DNS infrastructure also pose hurdles, as any new system must balance decentralization with practical integration and user adoption.\n\nIn summary, distributed DNS aims to recreate the core function of mapping domain names to IP addresses while mitigating central points of failure and censorship risks, offering a robust foundation for a more secure Internet infrastructure.	48:00:00	2025-03-17 10:22:09.021856+00	t
307	10	Improve database performance	#code	Improving database performance is crucial for web applications and other systems. Below are several key strategies and techniques to help you accelerate database operations:\n\n1. Indexing:  \n    Create proper indexes on the columns used in your queries. Effective indexing allows the database to quickly locate records, significantly boosting query execution speed.\n    \n2. Query Optimization:  \n    Review and refine your SQL queries. This might involve reducing the total number of queries, selecting more efficient operators, or restructuring queries with subqueries where appropriate.\n    \n3. Caching:  \n    Implement caching mechanisms to store results of frequently run queries in memory. This reduces the load on the database and speeds up data retrieval.\n    \n4. Scaling:  \n    Consider scaling your database architecture. Horizontal scaling (adding more servers) or vertical scaling (upgrading the existing server‚Äôs resources) can provide the additional capacity needed for better performance.\n    \n5. Application-Level Caching:  \n    In addition to database caching, use application-level caches to store rarely changing data. This might include caching HTML pages, images, or other static resources.\n    \n6. Using NoSQL:  \n    In scenarios where a rigid data schema isn‚Äôt required, NoSQL databases can sometimes offer a more efficient alternative by providing faster data access under certain conditions.\n    \n7. Indexed Views:  \n    For queries that are executed frequently, creating indexed views can greatly reduce response times by pre-computing and storing the results.\n    \n8. Data Volume Management:  \n    Ensure your database is free of unnecessary data. Removing outdated records and archiving older data can help improve query performance by reducing data clutter.\n    \n9. Optimizing Network Requests:  \n    In a client-server architecture, minimize both the number and size of network requests between the client and the server to improve overall response times.\n    \n10. Utilizing Caching Databases:  \n    Explore the use of dedicated caching systems like Redis or Memcached. These systems can store frequently accessed data and relieve pressure on your primary database.\n    \n11. Monitoring and Profiling:  \n    Use monitoring and profiling tools to identify slow queries and performance bottlenecks. This enables targeted optimizations where they are needed most.\n    \n12. Infrastructure Optimization:  \n    Finally, ensure that your entire infrastructure‚Äîincluding the database server and network components‚Äîis configured and optimized for high performance.	96:00:00	2025-03-16 12:10:54.621091+00	t
208	10	Factory Method	#code	> A #creational design pattern that provides an interface for creating objects in a superclass, allowing subclasses to alter the type of objects that will be created.\n \nThis pattern promotes loose coupling by eliminating the need to bind application-specific classes into the code. Instead, the code interacts with a general interface, and the subclasses handle the instantiation.\n\nIn Go, which lacks traditional object-oriented features like classes and inheritance, the Factory Method pattern can be implemented using interfaces and factory functions. \n\nThis approach allows for the creation of objects without specifying their exact concrete types.	192:00:00	2025-03-19 04:03:11.989332+00	t
40	10	Microkernel	#code	The basic idea behind the microkernel design is to achieve high reliability by splitting the operating system up into small, well-defined modules, only one of which‚Äîthe microkernel‚Äîruns in kernel mode and the rest run as relatively powerless ordinary user processes. Consequently (and unlike a monolithic design), a buggy audio driver cannot accidentally write on the SSD or disk.\n\nCommon desktop operating systems do not use microkernels (they use hybrid kernels). However, they are dominant in real-time, industrial, avionics, and military applications that are mission critical and have very high reliability requirements.	192:00:00	2025-03-19 12:01:40.560124+00	t
81	10	Kernel-level threads	#code	Kernel Threads are the units of execution within an operating system process. The OS kernel is responsible for generating, scheduling, and overseeing kernel-level threads since it controls them directly. \n\nEach kernel-level thread has its own context, including information about the thread‚Äôs status, such as its name, group, and priority.\n\n### Advantages\n1. True parallelism:   Kernel threads allow real parallel execution in multithreading.\n2. Execution continuity:   Other threads can continue to run even if one is blocked.\n3. Access to system resources:   Kernel threads have direct access to system-level features, including I/O operations.\n\n### Disadvantages \n1. Management overhead:   Kernel threads take more time to create and manage.\n2. Kernel mode switching:   Requires mode switching to the kernel, adding overhead.\n\nhttps://disk.yandex.ru/i/kZpHzmImuzwXDA	192:00:00	2025-03-20 04:21:10.854358+00	t
269	10	PostreSQL index data structures	#code	PostgreSQL uses a variety of data structures to implement its different index types. The specific data structure depends heavily on the type of index you choose to create. Here's a breakdown of the primary data structures used for the common index types in PostgreSQL:\n\nPostgreSQL indexes use various structures:\n- B-tree:  (default) Balanced tree, general purpose.\n- Hash:  Hash table, equality searches.\n- GiST:  Framework (R-trees), complex data/ops.\n- GIN:  Inverted index, composite values (arrays, text).\n- SP-GiST:  Space-partitioned trees, spatial data.\n- BRIN:  Block range summary, ordered data, small index.\n\nChoose based on data & queries.\nhttps://disk.yandex.ru/i/nZ5npONpjb2hjA	192:00:00	2025-03-19 13:05:59.845506+00	t
239	10	Functional programming	#code	Functional programming treats computation as the evaluation of mathematical functions without changing state or mutable data. It emphasizes the use of pure functions, immutability, and higher-order functions. Languages such as Haskell, Lisp, and Scala are known for their functional programming capabilities.\n\n> Such functions are called #pure, they do only calculations and nothing more.\n\nhttps://pictures.s3.yandex.net/resources/Untitled_6_1701754903.png	96:00:00	2025-03-18 08:18:27.872592+00	t
429	10	Spot a race condition	#golang	Race conditions arise when two or more goroutines access shared data concurrently without proper synchronization, causing the final program state to depend on the non-deterministic order of execution. This makes them hard to reproduce and diagnose.\n\n### Go Race Detector\nThe most effective way to detect these issues is using Go‚Äôs built-in race detector. Run your program or tests with the -race flag:  \ngo run -race your_program.go\ngo test -race your_package\n  \nThis tool instruments your code at compile time and monitors memory accesses at runtime. When a race is detected, it prints a detailed report showing the exact locations in the code where conflicting accesses occur (including goroutine IDs, file paths, and line numbers). Note that the detector only catches races that occur during that particular execution and might slow down large or complex programs.\n\n### Static Analysis Tools\nTools like go vet can analyze your code statically to flag some basic concurrency issues. While these tools can help by pointing out potential problems without executing the code, they are less comprehensive, may generate false positives, and often miss more subtle race conditions.\n\n### Code Review and Manual Inspection\nBecause automatic tools can‚Äôt catch every scenario, thorough code reviews are crucial. Look for shared variables accessed by multiple goroutines, unsynchronized access, and dependencies on the order of operations. A careful manual inspection can uncover issues that automated tools might overlook.\n\n### Testing Strategies\nWhile testing can‚Äôt prove the absence of races, it can increase your confidence. Stress testing your code under heavy load and employing fuzz testing (using commands like go test -fuzz) can force uncommon interleavings to surface hidden race conditions.\n\nUsing a combination of dynamic detection (race detector), static analysis, careful code reviews, and rigorous testing offers a practical, multi-layered approach to identifying and mitigating race conditions in Go programs.	96:00:00	2025-03-21 16:14:02.165941+00	t
33	10	List of modern operating systems	#code	1. Windows. Proprietary operating system, primarily used on personal computers and servers. Current versions include Windows 10, Windows 11, Windows Server, etc.\n\n2. Unix. A family of multi-tasking, multi-user operating systems developed at Bell Labs in the 1970s. Served as the foundation for many modern operating systems, including Linux, Solaris, and macOS.\n\n3. Linux. An open-source, Unix-like operating system developed by Linus Torvalds and the open-source community. Highly customizable and available in many different distributions (distros), such as Ubuntu, Fedora, Arch Linux, etc. Used in a wide range of devices, from personal computers to servers, supercomputers, and embedded systems.\n\n4. Minix. A Unix-like operating system developed by Andrew Tanenbaum as a teaching tool for operating system principles. Served as an inspiration for Linus Torvalds to create the Linux kernel. The current version of MINIX,  called MINIX 3, is now focused on being an extremely reliable and secure operating system.\n\n5. Solaris. A Unix-based operating system developed by Sun Microsystems (now owned by Oracle). Primarily used on enterprise-level servers and workstations.\n\n6. macOS. The operating system for Apple's Macintosh computers. Based on the Unix-like Darwin core, which is derived from the BSD (Berkeley Software Distribution) branch of Unix.\n\n7. OpenBSD, FreeBSD. BSD-based, open-source operating systems that are descendants of the original BSD Unix. Focused on security, stability, and standards compliance.\n\n8. Fedora. A community-driven Linux distribution sponsored by Red Hat. Serves as the upstream for Red Hat Enterprise Linux (RHEL).\n\n9. Arch Linux. A highly customizable, rolling-release Linux distribution known for its minimalist approach and extensive documentation.\n\n10. Android. A mobile operating system based on the Linux kernel, developed by Google. Used primarily on smartphones and tablets.\n\n11. Ubuntu. A popular, user-friendly Linux distribution based on Debian. Sponsored by Canonical and known for its regular release cycle and long-term support versions	192:00:00	2025-03-18 08:03:26.078945+00	t
251	10	DDL operators	#sql	_Data Definition Language_\n\n- CREATE TABLE:  \n  Creates a new table in the database. You need to specify the table name and the columns it will contain, including their data types and any constraints.\n        CREATE TABLE Customers (\n        CustomerID INT PRIMARY KEY,\n        Name VARCHAR(255),\n        Address VARCHAR(255),\n        City VARCHAR(255)\n    );\n    \n\n- ALTER TABLE:  \n  Modifies the structure of an existing table. You can add, modify, or delete columns, constraints, and other table properties.\n        ALTER TABLE Customers\n    ADD Email VARCHAR(255);\n    \n\n- DROP TABLE:  \n  Deletes an existing table from the database. This operation is irreversible and will permanently delete all data in the table.\n        DROP TABLE Customers;\n    \n\n- CREATE DATABASE:  \n  Creates a new database. You need to specify the database name.\n        CREATE DATABASE MyDatabase;\n    \n\n- ALTER DATABASE:  \n  Modifies the properties of an existing database. You can change the database name, character set, and other settings.\n        ALTER DATABASE MyDatabase\n    CHARACTER SET utf8mb4;\n    \n\n- DROP DATABASE:  \n  Deletes an existing database. This operation is irreversible and will permanently delete all data in the database. For example:\n        DROP DATABASE MyDatabase;	192:00:00	2025-03-28 09:57:29.682973+00	t
295	10	Estimate the number of rows in a table	#postgres	1. Using system metadata: Most database management systems (DBMS) store metadata about tables, including the number of rows. This information can be accessed through system tables or views. For example, in PostgreSQL, the number of rows in a table can be obtained from the pg_stat_user_tables system table.\n    \n2. Using statistics: DBMSs maintain statistics about tables and indexes. These statistics may include information about the number of rows. You can query this information through system functions or SQL queries. For example, in PostgreSQL, you can use the pg_stat_get_live_tuples function to get an approximate estimate of the number of live (non-deleted) rows in a table.\n    \n3. Using indexes: If you have an index that covers the table, you can estimate the number of rows included in that index. This is a rough estimate, but it can be useful for initial assessment. For example, in PostgreSQL, you can get an approximate number of rows in an index using the pg_stat_get_live_tuples(index_oid) function.\n    \n4. Calculating from table size and average row size: You can get the table size from metadata and divide it by the average row size. You can extract this data using the EXPLAIN command.\n  \n\nSELECT \n  relname AS object_name\n  ,pg_stat_get_live_tuples(c.oid) AS live_tuples\n  ,pg_stat_get_dead_tuples(c.oid) AS dead_tuples\nFROM pg_class c;\n\nSELECT \n  relname AS table_name\n  ,n_live_tup AS live_tuples\n  ,n_dead_tup AS dead_tuples\nFROM pg_stat_user_tables;	96:00:00	2025-03-22 07:43:05.728104+00	t
171	10	Clean architecture	#code	Clean architecture is an approach to software design that seeks to isolate business logic from external dependencies. It's a way of structuring your application to make it simpler to develop/extend/support and more understandable.\n\nA _plug-in_ is a component that can be connected or disconnected from the main application without affecting its operation. In a clean architecture, all components except business logic are treated as plug-ins.\n\nThe following are the most common layers of clean architecture project design:\n1. Controller ‚Äî a layer responsible for processing incoming requests and interacting with external components such as web servers or the user interface. This layer may contain structures, methods, and functions related to request processing.\n\n2. Service (use cases) ‚Äî the layer containing the basic business logic and functions of the application. Here you can find structures, methods and functions that implement the main application usage scenarios. Interaction with external components should be minimal.\n\n3. Repository ‚Äî a layer responsible for interaction with the database or other data sources. This is where structures, methods, and functions related to retrieving and storing data are located.\n\n4. Entities ‚Äî representation of business entities. Usually, it consists of structs with methods. Each struct describes the fields and behavior of a business entity.¬† It‚Äôs important, that¬†entity is not a database or ORM model.\n\nThe entity knows nothing about external layers, including the type of storage, that we use in the application.	384:00:00	2025-04-09 04:18:34.248922+00	t
180	10	Interface segregation	#code	> The bigger the interface, the weaker the abstraction.\n\nClients should not be forced to depend upon the interfaces that they do not use.\n\nMultiple client-specific interfaces are better than a single general-purpose interface. In other words, classes should only implement the interfaces they actually use and should not be loaded with methods they don't need. This reduces class cohesion and increases class flexibility.	192:00:00	2025-03-20 05:06:55.644357+00	t
334	10	Kafka components	#kafka	- Kafka ‚Äî¬†A messaging broker that enables communication between different services. It's also used for shipping logs (e.g. Graylog or Elastic), data storage, and more.\n- Broker ‚Äî A Kafka server / node in the Kafka cluster responsible for receiving, storing, and delivering messages between producers and consumers.\n- Consumer ‚Äî A message receiver / recipient.\n- Producer ‚Äî A message sender.\n- Zookeeper ‚Äî A configuration and coordination service for Kafka / distributed coordination service. Manages configuration and state, detects brokers, elects the cluster controller, monitors node health, and ensures the cluster's reliability and functionality.\n- Cluster Controller / Controller ‚Äî A service responsible for partition leader election and broker state monitoring / managing the Kafka cluster.\n- Offset ‚Äî A term that indicates a consumer's position in a message stream. It tracks where in the stream each consumer is reading so that messages are processed sequentially, moving forward one message with each read.\n- Partition ‚Äî The unit of parallelism in Kafka. A topic's partition count can only be increased. These partitions act like shards used for data distribution.\n- Replica ‚Äî A copy of a partition that can reside on another node to provide fault tolerance through data replication.\n- Topic ‚Äî A channel for writing and reading messages.\n- Consumer Group ‚Äî A set of consumers that collectively consume messages from one or more topics, allowing Kafka to support real-time event-driven applications. Such groups can run as a cluster spanning multiple servers or data centers.	96:00:00	2025-03-27 04:36:13.92276+00	t
178	10	Open/closed principle	#code	> A software module should be open for extension but closed for modification.\n\nIn Go, an object can be extended by _composition_. However, this object is _not aware_ of any type may embed it and its set of methods _cannot be altered_ by objects it is embedded to.	192:00:00	2025-03-20 04:27:20.415957+00	t
227	10	Command	#code	A #behavioral design pattern that converts requests or simple operations into objects.\n\nThe conversion allows deferred or remote execution of commands, storing command history, etc.\n\nhttps://disk.yandex.ru/i/Zka1mFRu1ry_Gw	384:00:00	2025-04-04 05:40:52.493879+00	t
277	10	Replication methods	#code	### Primary-Replica Replication \n_(Historically known as Master-Slave Replication)_\nThe primary server handles all write operations, while the replicas copy data from the primary and can serve read queries. Crucially, all write operations are exclusively performed on the primary server.\n\n### Multi-Master Replication\n_(Historically known as Master-Master Replication)_\nTwo or more primary servers each capable of accepting both write and read queries. These primaries communicate with each other to synchronize data changes.\n\nMulti-master replication allows for the distribution of both read and write loads. However, synchronization between the primaries can be complex and often requires additional configuration to manage data conflicts and ensure data consistency.\n\n### Logical Replication\nLogical replication allows you to selectively replicate specific tables and columns, rather than copying the entire database. It is generally less performant than physical replication, especially for replicating large datasets, as it operates at the logical level (row-level changes) rather than at the physical block level.\n\n### Bi-Directional Replication\n_(a specialized form of Multi-Master)_\nIt is specifically designed for creating highly available and geographically distributed clusters, often in active-active configurations.\n\n### Advanced Replication Models\nComplex replication models, such as cascading replication, where replication is propagated through multiple levels (a replica replicates from the primary, and another replica replicates from that first replica, and so on). \n\n\nhttps://disk.yandex.ru/i/lMtOGQ2EhoR62w	96:00:00	2025-03-16 14:32:25.34272+00	t
149	10	Processor cache	#code	The cache in the processor is a small but very fast memory used to store the most frequently used data and instructions. \n\nThe processor cache consists of several levels, including L1, L2, and L3 caches.\n\nThe cache serves to speed up data access, reduce memory access delays, and improve processor performance.	384:00:00	2025-04-01 17:04:03.123609+00	t
156	10	GC tuning	#code	- GOGC  \n    This environment variable controls the GC target percentage. It defaults to 100, meaning the GC aims to keep the amount of live heap plus the reachable heap at the end of the cycle to be no more than 100% of the live heap size at the beginning of the cycle. Lowering GOGC makes the GC more aggressive (more frequent collections, lower memory usage, but potentially higher CPU usage). Increasing GOGC makes the GC less aggressive (less frequent collections, higher memory usage, but potentially lower CPU usage).\n\n- runtime.GC()  \n    You can manually trigger a garbage collection, but it's generally discouraged unless you have a very specific reason. The Go GC is usually quite good at managing memory automatically.	96:00:00	2025-03-17 16:51:48.32811+00	t
211	10	Builder	#code	A #creational design pattern that allows for step-by-step construction of complex objects. It separates the construction process from the final representation, enabling the same construction process to create different representations.\n\nThis pattern is particularly useful when an object requires numerous parameters or when the construction process involves multiple steps.	192:00:00	2025-03-17 14:49:28.685382+00	t
384	10	CI/CD key concepts	#cicd	### Continuous Integration\n- Code Integration: Developers regularly integrate their code into a shared repository. This practice ensures early feedback on potential integration conflicts and errors.\n    \n- Automated Build and Testing: After each integration, the code is automatically built and subjected to a suite of automated tests. This allows for the identification and resolution of errors in the early stages of development.\n    \n\n### Continuous Delivery\n- Automated Deployment (to staging): Upon successful integration and passing all tests, the code is automatically deployed to a testing (or staging) environment.\n    \n- Testing in Real-World Conditions: After deployment to the staging environment, the code undergoes testing under conditions that closely resemble the production environment. This step is crucial for identifying environment and configuration-related issues.\n    \n- Manual Testing and Approval: Following successful automated tests and staging environment validation, developers and QA engineers can perform additional manual testing and formally approve the release for production deployment.\n\n### Continuous Deployment\n- Automated Deployment to Production: In Continuous Deployment, the validated code is automatically deployed to the production environment without manual intervention from developers or QA. This step is typically triggered after all preceding testing stages are successfully completed.\n    \n- Monitoring and Feedback: Once the product is deployed to production, it is continuously monitored for issues and errors. Metrics and logs are used for rapid problem detection and response.	96:00:00	2025-03-22 04:33:42.308087+00	t
212	10	Prototype	#code	A #creational pattern that enables object creation by copying an existing instance, known as the prototype.\n\nThis approach is particularly useful when the cost of creating a new object is high, or when creating a new instance directly is not feasible. By cloning the prototype, new objects can be created efficiently, preserving the state of the original object.	192:00:00	2025-03-17 04:41:36.259775+00	t
197	10	Agile methodologies	#code	Agile is an umbrella term for approaches that emphasize flexibility, customer collaboration, and rapid, incremental delivery of working software. Agile methods break the project into short iterations or sprints and adapt to changes as they occur.\n\n### Scrum \nOne of the most popular agile frameworks, Scrum organizes work into fixed-length sprints (typically 2‚Äì4 weeks), with roles like Product Owner and Scrum Master ensuring continuous planning, daily stand-ups, sprint reviews, and retrospectives.  \n\n### Kanban\nKanban focuses on visualizing work (usually via a Kanban board) and limiting work-in-progress to improve flow and efficiency. It‚Äôs ideal for teams needing continuous delivery and process improvement without fixed-length sprints.  \n\n### Extreme Programming (XP)\nXP emphasizes technical practices like pair programming, test-driven development, and continuous integration to enhance software quality and responsiveness to changing requirements.	96:00:00	2025-03-15 11:55:50.427348+00	t
229	10	Proxy	#code	A #structural design pattern that provides an object that acts as a substitute for a real service object used by a client. A proxy receives client requests, does some work (access control, caching, etc.) and then passes the request to the service object.\n\nThe proxy object has the same interface as a service, which makes it interchangeable with a real object when passed to a client.	192:00:00	2025-03-17 04:48:50.857436+00	t
199	10	ATDD & TDD	#code	Acceptance Test-Driven Development and Test-Driven Development are software development approaches that emphasize testing. ATDD proposes the creation of automated test scenarios that define the requirements for the system. TDD suggests creating tests before writing code, which allows the developer to focus on realizing the requirements and provide more reliable code.\n\nThese approaches help improve software quality, reliability, and compliance.	192:00:00	2025-03-18 04:04:08.050211+00	t
418	10	Nil pointer vs nil interface	#code	In Go the key point is that interfaces carry two pieces of information‚Äîa type and a value‚Äîand pointer types are concrete types. This leads to some common pitfalls when comparing nil pointers with nil interfaces:\n\n### Nil Pointers vs. Nil Interfaces\nA nil pointer (e.g. a variable of type \\*T that has not been initialized) is considered nil. However, when you assign a nil pointer to an empty interface (i.e. an interface{}), the interface stores both the pointer‚Äôs type (such as \\*T) and its nil value. As a result, the interface itself is not nil. For example:\n  var p *int = nil\n var i interface{} = p\n fmt.Println(i == nil) // prints "false"\n \n    \n Even though p is nil, i is not nil because it holds a non-nil type (*int) along with the nil value.\n    \n### Interface Equality\n Two interface values are considered equal only if both their dynamic types and underlying values are equal. This means:\n    \n - Two interfaces that both hold a nil pointer of the same type will be equal.\n - An interface holding a nil pointer (with an underlying type) is not equal to an interface that is completely nil (where both type and value are nil).\n### Direct Pointer Comparisons\n Directly comparing two pointers of the same type (e.g. var a, b *int) is allowed, and if both are nil, the comparison returns true. However, comparing a pointer with an interface is not permitted unless you explicitly extract the pointer value from the interface.\n    \n\n### In summary\n- A nil pointer by itself is nil.\n- An interface that holds a nil pointer is not nil because its type information is present.\n- Comparisons between interface values check both type and value, so an interface with a nil pointer (type != nil) does not equal a nil interface (where type is nil).\n\nUnderstanding this distinction is crucial to avoid subtle bugs in your Go programs.	96:00:00	2025-03-19 15:41:04.621054+00	t
83	10	System call features	#code	1. Interface:   System calls provide a well-defined interface between user programs and the operating system. Programs make requests by calling specific functions, and the operating system responds by executing the requested service and returning a result.\n\n2. Protection:   System calls are used to access privileged operations that are not available to normal user programs. The operating system uses this privilege to protect the system from malicious or unauthorized access.\n\n3. Kernel Mode:   When a system call is made, the program is temporarily switched from user mode to kernel mode. In kernel mode, the program has access to all system resources, including hardware, memory, and other processes.\n\n4. Context Switching:   A system call requires a context switch, which involves saving the state of the current process and switching to the kernel mode to execute the requested service. This can introduce overhead, which can impact system performance.\n\n5. Error Handling:   System calls can return error codes to indicate problems with the requested service. Programs must check for these errors and handle them appropriately.\n\n6. Synchronization:   System calls can be used to synchronize access to shared resources, such as files or network connections. The operating system provides synchronization mechanisms, such as locks or semaphores, to ensure that multiple programs can access these resources safely.	48:00:00	2025-02-16 12:48:31.664697+00	t
43	10	Virtual machines	#code	A virtual machine #VM is a software-based emulation of a computer system. It provides a virtual environment that behaves like a physical computer, with its own operating system, applications, and resources.\n\nVirtual machines are typically created and managed using virtualization software, also known as a #hypervisor. The hypervisor is responsible for creating and managing the virtual machines, as well as allocating and sharing the physical hardware resources among them.\n\nThe key characteristics of a virtual machine are:\n\n1. Abstraction of Hardware ‚Äî virtual machine abstracts the underlying physical hardware, such as the CPU, memory, storage, and network interfaces. This allows the virtual machine to run independently of the actual hardware configuration. \n\n2. Isolation ‚Äî Virtual machines are isolated from each other and from the host operating system, providing a secure and contained environment for running applications. This isolation helps prevent conflicts between different virtual machines and the host system. \n\n3. Resource Allocation ‚Äî Virtual machines can be allocated a specific amount of CPU, memory, storage, and other resources, which can be dynamically adjusted as needed. This allows for efficient utilization of the physical hardware resources. \n\n4. Portability ‚Äî Virtual machines can be easily moved, copied, or backed up, as they are essentially software-based computer systems. This makes them highly portable and easy to migrate between different physical hosts.	384:00:00	2025-03-24 11:55:25.639664+00	t
389	10	End-to-End vs Acceptance testing	#cicd	### End-to-End Testing\nE2E testing validates the entire application flow from start to finish, mimicking real user interactions through the application's user interface. It typically requires the application to be deployed in a production-like environment. E2E testing is valuable for verifying that the application functions correctly as a whole system.\n    \n### Acceptance Testing\nAcceptance testing focuses on verifying that the software meets the pre-defined acceptance criteria outlined in documentation, such as a Product Design specification. These criteria define the expected behavior and rules of the application from the user's perspective.\n\nEssentially, acceptance testing verifies the acceptance criteria _during_ end-to-end testing. Acceptance tests are structured as a series of end-to-end test scenarios designed to simulate the conditions and behaviors described in the acceptance criteria.	96:00:00	2025-03-29 07:19:25.29835+00	t
275	10	Sharding	#code	Sharding is a horizontal scaling method where data is partitioned into multiple independent datasets called "shards." Each shard is then stored on a separate server or node. Critically, each shard is responsible for managing only a subset of the total data and handles queries related to its specific data partition. This division of data and query processing across multiple servers significantly increases overall throughput and distributes the load, preventing any single server from becoming a bottleneck.\n\nAdvantages:\n\n- Handles Large Datasets and Query Loads: Sharding enables systems to manage and process volumes of data and numbers of queries that would be impossible for a single server to handle.\n- Enhances Horizontal Scalability: Adding more capacity to a sharded system is as simple as adding more shards (and servers) to the system. This allows for near-linear scalability as data and user traffic grow.\n- Improves Performance and Fault Tolerance: By distributing the workload, sharding reduces the load on individual servers, leading to faster query response times. Moreover, if one shard (server) fails, only a fraction of the data and queries are affected, while the rest of the system continues to operate, thus improving fault tolerance.\n\nExample:\n\n- Imagine a social media platform where user data is sharded based on geographical regions. For instance, users in North America might be assigned to one shard, European users to another, and Asian users to a third. Each shard would contain the profiles, posts, and related data for users in its designated region and would be hosted on a separate set of servers.	192:00:00	2025-03-20 17:41:49.947178+00	t
166	10	Checksum	#code	A checksum is a short, fixed-size value‚Äîoften represented as a string of numbers and letters‚Äîthat is computed from a larger block of digital data using a specific algorithm. Its primary purpose is to verify data integrity by detecting accidental errors or unauthorized changes. In practice, you calculate a checksum before sending or storing data, then later recompute it to see if it still matches. Even a tiny alteration in the original data will produce a completely different checksum, signaling that the data might be corrupted or tampered with.\n\nChecksums are widely used in:\n- File Downloads:   To ensure a downloaded file hasn‚Äôt been corrupted during transfer.\n- Data Transmission:   Embedded in protocols like TCP/IP to detect errors in packet headers.\n- Digital Preservation:   To monitor that files stored over time remain unchanged.\n- Cybersecurity:   As part of verification processes to detect potential tampering.\n\nDifferent checksum algorithms exist‚Äîfrom simple additive methods and cyclic redundancy checks (CRCs) to cryptographic hash functions like MD5, SHA-1, and SHA-256‚Äîeach balancing speed, complexity, and security in different ways.	192:00:00	2025-03-19 07:56:17.172339+00	t
29	10	Language concept	#code	In computer programming, a language concept refers to a fundamental idea or construct that is part of a programming language's design and functionality. These concepts define how programmers can express computations, structure their code, and interact with data. Here are some key language concepts:\n\n1. Syntax: The set of rules that defines the structure of valid statements in a programming language. Syntax dictates how code must be written for the compiler or interpreter to understand it.\n\n2. Semantics: The meaning behind the syntactical elements of a language. While syntax focuses on form, semantics deals with what the code does when executed.\n\n3. Data Types: The classification of data that tells the compiler or interpreter how the programmer intends to use the data. Common data types include integers, floats, characters, strings, and booleans.\n\n4. Control Structures: Constructs that control the flow of execution in a program, such as loops (for, while), conditionals (if, switch), and branching statements (break, continue).\n\n5. Functions/Methods: Blocks of code that perform a specific task and can be reused throughout a program. Functions can take parameters and return values.\n\n6. Object-Oriented Programming (OOP): A programming paradigm based on the concept of "objects," which can contain data and methods. Key concepts in OOP include classes, inheritance, encapsulation, and polymorphism.\n\n7. Concurrency: The ability of a program to execute multiple tasks simultaneously. This includes concepts like threads, processes, and synchronization mechanisms (e.g., mutexes and semaphores).\n\n8. Error Handling: Mechanisms for responding to and managing errors that occur during program execution, such as exceptions and error codes.\n\n9. Scope and Lifetime: Concepts that define the visibility and duration of variables and functions within a program. Scope determines where a variable can be accessed, while lifetime refers to how long a variable exists in memory.\n\n10. Abstraction: The concept of hiding complex implementation details and exposing only the necessary parts of a system, allowing programmers to work at a higher level of complexity.\n\nThese concepts are foundational to understanding how programming languages work and how to effectively use them to write software. Different programming languages may emphasize different concepts or implement them in unique ways, leading to various programming paradigms (e.g., procedural, functional, object-oriented).	168:00:00	2025-03-15 04:04:30.828722+00	t
74	10	Thread key characteristics	#code	1. Shared Resources:\n  i. Threads within the same process share the same:\n    - Address space (code, data, and heap segments).\n    - File descriptors.\n    - Environment variables.\n  ii. Each thread has its own:\n    - Stack for local variables and function calls.\n    - Thread-specific data (e.g., thread ID, registers, program counter).\n\n2. Lightweight:   Creating and switching between threads is faster than processes because threads share the same address space and resources.\n\n3. Concurrency:   Threads enable concurrent execution of tasks, improving performance on multicore systems.\n\n4. Synchronization:   Since threads share memory, synchronization mechanisms like mutexes, semaphores, and condition variables are required to avoid race conditions and ensure safe access to shared resources	192:00:00	2025-03-17 04:28:30.586615+00	t
234	10	Bridge	#code	A #structural design pattern that lets you split a large class or a set of closely related classes into two separate hierarchies‚Äîabstraction and implementation‚Äîwhich can be developed independently of each other.\n\nOne of these hierarchies (often called the #abstraction) will get a reference to an object of the second hierarchy ( #implementation). The abstraction will be able to delegate some (sometimes, most) of its calls to the implementations object. Since all implementations will have a common interface, they‚Äôd be interchangeable inside the abstraction.	192:00:00	2025-03-22 08:29:55.678395+00	t
330	10	MongoDB replication	#code	MongoDB replication is an essential feature that provides high availability, fault tolerance, and data redundancy by using a replica set‚Äîa group of MongoDB instances maintaining identical data copies. \n\nIn a replica set, one node is designated as the primary, receiving all write operations, while one or more secondary nodes replicate these changes by reading from the primary‚Äôs oplog, a capped collection that logs every write. This asynchronous process keeps the secondaries in sync, although a slight replication lag may occur, meaning secondary data is eventually consistent.\n\nIn case the primary node fails, an automatic election process promotes one of the secondaries to become the new primary, ensuring minimal downtime and continuous service. Although writes are directed solely to the primary, you can configure your application to read from secondary nodes to distribute the query load, which can help alleviate pressure on the primary and improve overall performance.\n\nFor production environments, it‚Äôs recommended to use at least three data-bearing nodes to form a robust majority during elections. In some cases, an arbiter‚Äî a lightweight member that participates in elections without storing data‚Äîmay be added to avoid split votes while conserving resources.	96:00:00	2025-03-18 06:41:32.921919+00	t
380	10	Basic k8s commands	#code	kubectl is the command-line tool for interacting with Kubernetes clusters. Here are some essential commands:\n\n- `kubectl create` is used to create resources in Kubernetes.  \n  Example: kubectl create deployment my-app --image=my-image (creates a deployment named "my-app" using the container image "my-image").\n  \n- `kubectl apply` applies configuration files to create or update resources. This is often used with YAML configuration files defining Kubernetes objects.  \n    Example: kubectl apply -f my-deployment.yaml (applies the configuration defined in "my-deployment.yaml").\n   \n- `kubectl get`: Retrieves information about resources.  \n    Example: kubectl get pods (lists all pods in the current namespace).\n   \n- `kubectl describe`: Provides detailed information about a specific resource.  \n    Example: kubectl describe pod my-pod (shows detailed information about the pod named "my-pod").\n   \n- `kubectl logs`: Fetches logs from a container within a pod.  \n    Example: kubectl logs my-pod (shows logs from the first container in the pod "my-pod"). To specify a container in a pod with multiple containers: kubectl logs my-pod -c my-container.\n   \n- `kubectl exec`: Executes a command inside a container.  \n    Example: kubectl exec -it my-pod -- /bin/bash (opens an interactive bash shell within the pod "my-pod"). The -it flags enable interactive TTY and stdin.\n   \n- `kubectl delete`: Deletes resources.  \n    Example: kubectl delete pod my-pod (deletes the pod named "my-pod").\n   \n- `kubectl scale`: Scales the number of replicas for a deployment, replica set, or replication controller.  \n    Example: kubectl scale deployment my-app --replicas=3 (scales the "my-app" deployment to 3 replicas).\n   \n- `kubectl rollout`: Manages application rollout and updates.  \n    Example: kubectl rollout status deployment/my-app (tracks the status of the rollout for the "my-app" deployment).\n   \n- `kubectl port-forward`: Forwards a local port to a port on a pod, allowing local access to services running inside the cluster.  \n    Example: kubectl port-forward my-pod 8080:80 (forwards local port 8080 to port 80 on the "my-pod", allowing you to access the service on http://localhost:8080).\n   \n- `kubectl create secret`: Creates secrets for storing sensitive data, like passwords or API keys, securely in Kubernetes.  \n    Example: kubectl create secret generic my-secret --from-literal=username=admin --from-literal=password=password123 (creates a generic secret named "my-secret" with username and password).\n   \n- `kubectl apply -f`: As mentioned before, applies configurations from a file, often a YAML file.  \n    Example: kubectl apply -f my-service.yaml (applies the service definition from "my-service.yaml").	96:00:00	2025-03-19 03:38:03.807315+00	t
352	10	Consistent hashing	#code	Consistent hashing can be described as follows:\n\n- It represents resource requestors (which we will refer to as ‚Äúrequests‚Äù) and server nodes in a virtual ring structure known as a hash ring.\n- The number of locations is no longer fixed, but the ring is considered to have an infinite number of points, and server nodes can be placed at arbitrary locations on this ring. (Of course, this random number can again be chosen using a hash function, but the second step of dividing it by the number of available locations is skipped since it is no longer a finite number).\n- Requests, i.e., users, computers, or serverless functions, which are analogous to keys in the classical hashing approach, are also placed on the same ring using the same hash function.\n\n If we assume that the ring is ordered such that traversing the ring clockwise corresponds to the order of addresses, each request can be served by the server node that first appears in this clockwise traversal. The first node with an address greater than the request's address gets to serve it. If the request's address is higher than the highest node address, it is served by the node with the lowest address, as the ring traversal is circular.\n\nhttps://files.bool.dev/site/blog/f2b69526-8bdc-4fe0-9c49-955fc9727fd9/7d0cd5c4-d3f7-4389-9692-9c8d9df07de0.png	96:00:00	2025-03-15 11:57:06.736637+00	t
124	10	HTTP/3	#code	HTTP/3 is the latest major version of HTTP. Web browsers and servers can use it for significant upgrades to user experience, including performance, reliability, and security.\n\nAn important difference in HTTP/3 is that it runs on QUIC, a new transport protocol. QUIC is designed to be fast and to support switching rapidly between networks. It relies on the User Datagram Protocol (UDP) rather than the Transmission Control Protocol (TCP), which mitigates an issue called head-of-line blocking in TCP, where network packet loss or reordering can slow down high-transaction connections. Furthermore, QUIC separates out the layer 4 transport connection from the layer 3 IP flow, allowing for migration between different networks without disruption.\n\nQUIC can better support mobile-heavy Internet usage in which people carry smartphones and constantly switch from one network to another as they move about their day. This type of Internet usage was not common when the first Internet protocols were developed: devices were less portable and did not switch networks very often.\n\nQUIC helps fix some of HTTP/2's biggest shortcomings:\n- Decreasing the effects of packet loss ‚Äî when one packet of information does not make it to its destination, it will no longer block all streams of information, a problem known as "head-of-line blocking"\n\n- Faster connection establishment: QUIC combines the cryptographic and transport handshakes\n\n- Zero round-trip time (0-RTT): For servers they have already connected to, clients can skip the handshake requirement (the process of acknowledging and verifying each other to determine how they will communicate)\n\n- More comprehensive encryption: QUIC is encrypted by default, making HTTP/3 more secure than HTTP/2 (more on this below)\n\n- Protecting against HTTP/2 "Rapid Reset" distributed denial-of-service (DDoS) attacks, which can slow down or crash a web server, by using a credit-based system for streams (a "stream" is a single HTTP request and response exchange) to allow HTTP/3 servers fine-grained control over stream concurrency\n\n- Developing a workaround for the sluggish performance when a smartphone switches from WiFi to cellular data, such as when leaving the house or office	384:00:00	2025-03-31 17:35:58.56623+00	t
136	10	Linux containers	#code	A Linux container is a lightweight, isolated environment that packages an application and its dependencies. Unlike virtual machines (VMs) which include a full operating system, containers share the host kernel, making them more portable and resource-efficient.\n\n#### Key components of a Linux container\n\n1. Container Image: A container image is a lightweight, standalone, and executable software package that contains everything needed to run an application, including the application code, runtime, libraries, and dependencies. Container images are typically built from a Dockerfile or similar configuration file using tools like Docker or Podman.\n\n2. Container Runtime: The container runtime is responsible for creating, running, and managing containers on a host system. Examples of container runtimes include Docker Engine, containers, and cri-o.\n\n3. Namespace and Cgroups: Linux namespaces and control groups (groups) are kernel features that provide process isolation and resource management for containers. Namespaces create isolated environments for processes, filesystems, networks, and other system resources, while groups control and limit the resource usage of containers, such as CPU, memory, and disk I/O.\n\n4. Container Orchestration: Container orchestration platforms, such as Kubernetes, provide tools and APIs for automating the deployment, scaling, and management of containerized applications. Orchestration platforms manage clusters of container hosts and provide features like service discovery, load balancing, and automated rollout and rollback of application updates.	96:00:00	2025-03-17 15:05:45.448042+00	t
238	10	Procedural programing	#code	Procedural programming is a subset of imperative programming that structures code into procedures or functions. These are blocks of code designed to perform specific tasks and can be called multiple times within a program. This paradigm emphasizes a clear sequence of actions and promotes code reusability.\n\nIn essence, while all procedural programming is imperative‚Äîsince it relies on explicit instructions‚Äînot all imperative programming is procedural. The key distinction lies in the use of #procedures: procedural programming emphasizes dividing a program into modular procedures, whereas imperative programming encompasses a broader range of programming styles that may or may not utilize such structured units.	192:00:00	2025-03-17 06:53:37.983603+00	t
203	10	Golang channel	#code	A channel is a communication mechanism that allows goroutines to exchange data.\n\n### Properties\n- goroutine-safe: one channel can work with multiple goroutines (hchan mutex);\n- FIFO: elements are added and removed by First-In-First-Out principle (hchan buf);\n- channels provide data transfer between goroutines (sendDirect, operations with buf);\n- an empty channel blocks goroutines (sendq/recvq, sudog, calls to scheduler: gopark(), goready());\n- channel sends a copy of the data passed through it;\n\n### Channel Header\ntype hchan struct {  \n    qcount   uint           // total data in the queue  \n    dataqsiz uint           // size of the circular queue  \n    buf      unsafe.Pointer // points to an array of dataqsiz elements    \n    elemsize uint16  \n    closed   uint32  \n    elemtype *_type // element type  \n    sendx    uint   // send index  \n    recvx    uint   // receive index  \n    recvq    waitq  // list of recv waiters  \n    sendq    waitq  // list of send waiters  \n      \n    lock mutex  \n}	192:00:00	2025-03-17 12:22:04.361646+00	t
155	10	Garbage collection process	#code	1. Initialization (STW):\n    - The GC starts with a brief stop-the-world (STW) phase.\n    - It initializes the marking process. All objects are initially considered white.\n    - The root objects (e.g., objects directly reachable from global variables, the stack of running goroutines) are marked gray. These are the starting points for the reachability analysis.\n\n2. Concurrent Marking:\n    - The GC runs concurrently with the program.\n    - The GC picks a gray object.\n    - It scans the gray object, finding all the objects it points to.\n        - If a pointed-to object is white, it's marked gray.\n    - The original gray object is then marked black.\n    - This process continues until there are no more gray objects.\n\n3. Write Barriers:\n    - Because the mutator (your program) is running concurrently, it might change pointers during the marking phase. This could lead to a situation where the GC misses a reachable object.\n    - Write barriers* are used to prevent this. It is a small function that kicks in whenever a pointer in the heap is modified, which means that the object is now reachable and needs to be put in the grey set.\n\n4. Termination (STW):\n    - The marking phase ends with another short STW phase.\n    - Some final cleanup and accounting are done.\n\n5. Sweeping:\n    - The sweeping phase reclaims the memory occupied by white objects. This is typically done lazily. Instead of immediately freeing all the memory, the sweeper might free memory as new allocations are needed.\n\n*There are different types of write barriers, but the general idea is to ensure that if a white object becomes reachable from a black object, the white object is either marked gray immediately or is otherwise protected from being collected. Write barriers are essential for the correctness of concurrent garbage collection. They ensure that the GC doesn't mistakenly collect objects that become reachable during the marking phase. Go uses a hybrid write barrier.\n\nhttps://disk.yandex.ru/i/BbVMpE26TpTMwA	192:00:00	2025-03-17 15:45:50.200437+00	t
195	10	Traditional software development methodologies	#code	### Waterfall\nA linear, sequential model where each phase (requirements, design, implementation, testing, deployment, and maintenance) must be completed before the next begins. It‚Äôs best suited for projects with well-defined requirements and minimal expected changes.\n\n### V-Model\nAn extension of Waterfall, the V-Model emphasizes testing by mapping each development phase to a corresponding testing phase. This model is often used in safety-critical or highly regulated environments where verification and validation are essential.	384:00:00	2025-03-31 04:27:16.205431+00	t
344	10	Microservices	#code	Microservices are an architectural approach to software development where an application is structured as a collection of small, independent services that communicate with each other through APIs.\n\nMicroservices address several challenges in software development and deployment:\n1.Independent Development and Deployment\n2. Technology Diversity\n3. Flexible Infrastructure\n4. Faster Time to Market (TTM\n\nTo function effectively, microservices should adhere to certain key principles:\n1.Independence:\n  Each microservice must be autonomous and function independently of others. Changes in one service should not necessitate changes in others.\n\n2. Statelessness:\n  Microservices themselves should ideally be stateless. This means they should not store session-specific or persistent data within the service instance. Instead, state should be managed in external data stores like databases, caches, or distributed session stores. This stateless nature is crucial for scalability and resilience.\n\n3. Scaling on Demand:  \n  Microservices should be scalable, meaning they can be easily scaled up or down based on demand. New service instances can be provisioned as needed, for example, to handle increased load or to offload processing from another service.\n\nFor effective microservice design, several best practices are commonly employed:\n- Domain-Driven Design (DDD):\n  Applying DDD principles is highly recommended for defining service boundaries and ensuring that services are aligned with business capabilities. DDD helps in creating cohesive and contextually bounded services.\n    \n\n- Design Patterns: \n  Utilizing established design patterns is crucial for building robust and maintainable microservices. For example, the __Facade pattern__ is frequently used to create an API Gateway microservice. This facade acts as a single entry point to the system, handling initial requests and routing them to the appropriate backend microservices.	96:00:00	2025-03-17 03:45:38.646314+00	t
96	10	Socket	#code	In Unix-like operating systems and network operations, a socket is represented as a file descriptor and serves as an endpoint for communication between processes, potentially across different machines.\n\nOperating system reads data from a socket in the following steps:\n1. User Process Request: When a process wants to read data from a socket, it makes a system call like recv(). This system call essentially asks the kernel to get the data from the socket.\n    \n2. Kernel Check: The kernel checks the file descriptor table to find the entry corresponding to the socket. It verifies that the socket is in a readable state (e.g., connected and has data available).\n    \n3. Data Retrieval:\n    - If data is already in a buffer: If the kernel has already received data from the network, and it's stored in a buffer associated with the socket, the kernel copies the data from the buffer to the user process's memory.\n    - If data is not yet available: If no data is currently available in the buffer, the kernel may:\n        - Block the process: The process is put to sleep until data arrives. When data arrives, the kernel wakes up the process and copies the data.\n        - Return immediately: The kernel can also be configured to return immediately with an indication that no data is currently available (e.g., an error code). This allows the process to do other things while waiting for data.\n4. Data Transfer: The kernel copies the data from its internal buffer to the memory location provided by the user process.\n    \n5. Return to Process: The recv() system call returns, indicating the number of bytes read.\n    \n\nImportant Points:\n- Buffering: The kernel uses buffers to store data received from the network before it's read by the process. This helps to handle variations in data arrival rates and allows the process to read data in chunks.\n- System Calls: All socket operations, including reading and writing data, are performed through system calls. This ensures that the operating system has control over network communication and can enforce security policies.\n- Asynchronous Operations: Some socket operations can be performed asynchronously. This means that the process can continue doing other things while waiting for data to arrive or for a connection to be established. This can improve performance in some applications.	192:00:00	2025-03-24 04:41:48.531054+00	t
231	10	Facade	#code	A #structural design pattern that provides a simplified (but limited) interface to a complex system of classes, library, or framework.\n\nIn a complex system, it‚Äôs easy to get lost and easy to break stuff if you‚Äôre doing something wrong. That‚Äôs why there‚Äôs a concept of the #facade pattern: a thing that lets the client work with dozens of components using a simple interface.\n\nIn addition to decreasing the overall complexity of the application, the Facade also helps to move unwanted dependencies to one place.	192:00:00	2025-03-20 06:30:24.07426+00	t
426	10	RPC	#code	#RPC is a client-server communication protocol that allows the client to call procedures (functions, methods) on a remote server as if they were local. This provides an abstraction of network communication and allows programs to work in a distributed environment, hiding the complexities of data transfer and remote operations.\n\n### Main RPC components\n1. Client: program or component that can initialize remote procedure call;\n2. Server: program or component providing methods that can be called from remote;\n3. Proxy: a client uses _proxy object_ for calling procedures on a server, as if they were local;\n4. Serializing: is the process of converting data and procedure parameters to a format applicable for transferring via network (JSON, binary, etc.);\n5. Transport: data transfer protocol between client and server (TCP, HTTP, etc.);\n6. #IDL (Interface Definition Language): determines procedures structure and signatures;\n\n### RPC design pattern\nThe RPC design pattern is an architectural pattern that allows client applications to invoke methods on remote servers. It includes the following components:\n1. Client ‚Äî a client application that initiates remote method calls.\n2. Server ‚Äî a remote server that provides methods to be invoked.\n3. Protocol ‚Äî the protocol by which the client and server communicate. Examples of protocols include gRPC, Thrift, or JSON-RPC.\n4. Serialization ‚Äî The process of converting data into a format that can be transmitted over a network. For example, data can be serialized into JSON format or binary format.\n\n### RPC advantages\nRPC has several advantages over REST:\n1. Performance. RPC is usually much faster because it is more compact and efficient in transferring data across the network.\n2. Strict typing. RPC allows you to define strict data typing, which simplifies development and provides more reliable communication between client and server.\n3. Greater flexibility. RPC supports a variety of protocols and data formats, allowing you to choose the most appropriate options for your particular application.\n4. Ease of use. RPC provides transparent communication between client and server, hiding the complexities of low-level networking.	24:00:00	2025-03-15 13:21:04.109127+00	t
417	10	Empty struct pointer doesn't equal to an empty interface, although both are nil	#code	type Person struct {\n  name [3]byte\n}\n\nfunc (p *Person) Name() []byte {\n  if p == nil {\n    return nil\n  }\n\n  return p.name[:]\n}\n\nfunc main() {\n\n  var person1 Person\n  var person2 *Person\n  var person3 interface{}\n\n  fmt.Printf("1. %v", person1.Name()) // [0 0 0]\n  println()\n  fmt.Printf("2. %v", person2.Name()) // []\n  println()\n  fmt.Println(person2, person3)           // <nil> <nil>\n  fmt.Printf("3. %v", person2 == person3) // false (nil pointer doesn't equal empty interface)\n  println()\n\n  person3 = person2\n  fmt.Printf("3. %v", person2 == person3) // true\n  println()\n\n  person1.Name()[0] = 'A'\n  fmt.Printf("4. %v", person1 == Person{}) // false\n}	24:00:00	2025-03-14 16:24:36.712833+00	t
121	10	HTTP	#code	HyperText Transfer Protocol is the foundation of data communication on the Internet. It's an _application-layer_ protocol that defines how messages are formatted and transmitted between a client (like a web browser) and a server.\n\nA key characteristic of HTTP is that it's _stateless_. This means that each request from a client to a server is treated as an independent transaction, unrelated to any previous requests. The server doesn't retain any information about past client interactions. While this simplifies the server's design, it also _necessitates techniques like cookies and sessions_ to maintain user state across multiple requests when needed (e.g., for login authentication or shopping carts).\n\nHTTP supports the transfer of various data types, including:\n- text (like HTML documents), \n- images, \n- audio, and video. \n\nWhile HTTP is versatile, it has limitations. Notably, the original HTTP/1.0* protocol typically handles one request-response cycle per TCP connection, which can lead to inefficiencies.\n\n*HTTP 1.1 improves upon HTTP 1.0 primarily by introducing persistent connections (allowing multiple requests over a single TCP connection), adding support for pipelining (sending multiple requests without waiting for responses), requiring the Host header (enabling virtual hosting), and offering better caching mechanisms, leading to significantly improved performance and efficiency compared to its predecessor.\n\nThe default port for HTTP communication is port 80, although other ports can be used.	48:00:00	2025-03-17 15:57:39.689305+00	t
268	10	Database indexes	#code	An index is a data structure that allows you to quickly locate data within a database. Indexes are primarily used to speed up data retrieval.\n\nThere are several types of indexes:\n\n- Clustered Indexes:  \n    A clustered index determines the physical order of data in a table. Since the data rows themselves are stored in the index order, a table can have only one clustered index.\n    \n- Non-clustered Indexes:  \n    A non-clustered index does not dictate the physical order of the data. Instead, it creates a separate structure from the data rows. A table can have multiple non-clustered indexes.\n    \n- Covering Indexes:  \n    A covering index includes all the columns needed to satisfy a query. Because of this, the database engine can retrieve the required data directly from the index without accessing the underlying table.\n\nhttps://disk.yandex.ru/i/jWNSi7btT6y7XQ	96:00:00	2025-03-15 16:14:14.542133+00	t
44	10	Containers	#code	Containers are provided by the host operating system such as Windows or Linux and mostly run just the user mode portion of an operating system. Each container shares the host operating system kernel and typically the binaries and libraries in a read-only fashion. This way, a Linux host can support many Linux containers. Since a container does not contain a full operating system, it can be extremely lightweight.\n\nUnlike virtual machines, there is no strict resource partitioning. The container may be restricted in what it may access on SSD or disk and how much CPU time it gets, but all containers still share the resources in the underlying host operating system. Phrased differently, containers are process-level isolated. This means that a container that messes with the stability of the underlying kernel will also affect other containers.	192:00:00	2025-03-20 03:33:37.021957+00	t
194	10	Waterfall	#code	The following list describes the basic steps that were introduced by the waterfall model:\n\n- Requirement collection: During this stage, the customer's requirements are captured and analyzed and a requirements document is produced.  \n\n- Design: Based on the requirement's document contents, analysts will plan the system's architecture. This step is usually split into two sub-steps: the logical system design, which models the system as a set of high-level components, and the physical system design, where the appropriate technologies and hardware components are selected.\n\n- Implementation: The implementation stage is where the design documents from the previous step get transformed by software engineers into actual code.  \n\n- Verification: The verification stage follows the implementation stage and ensures that the piece of software that got implemented actually satisfies the set of customer requirements that were collected during the requirements gathering step.\n\n- Maintenance: The final stage in the waterfall model is when the developed software is deployed and operated by the customer:	384:00:00	2025-03-28 17:48:57.258534+00	t
248	10	Database	#code	> A database is a digitally stored, organized collection of data that can be easily accessed, managed, and updated.\n \nDatabase provides a systematic way to store various types of information‚Äîsuch as text, numbers, images, and more‚Äîand is typically managed by specialized software called a Database Management System. The #DBMS not only handles the storage of data but also offers tools to query, update, and secure that data.\n\nFor example, in a relational database, information is stored in tables made up of rows and columns, where each row represents an individual record and each column represents an attribute of that record. Other models, such as NoSQL, may store data as documents, key-value pairs, or even graphs, depending on the application needs.\n\nIn essence, databases are essential for organizing large volumes of data and enabling efficient processing, analysis, and decision-making in everything from small applications to large enterprise systems.	192:00:00	2025-03-24 05:10:02.43579+00	t
159	10	Interface internals	#code	Interface values are stored as two structures #eface and #iface at runtime. Go keeps track of the mapping between types and interfaces in an internal list called #itabTable. This list is created both at compile time and runtime, and will be used to check whether a type implements an interface.\n\nAt runtime, interface values will be stored in the #eface structure if the interface is empty, or in #iface if it has methods. The structure contains a _type field, which is information about the type of the value the interface contains, and a data field, which is a pointer to the actual data. In case of #iface, #itab contains information about both the interface and its target type, and fun[0] indicates whether the type implements the interface or not.\n\n#eface is the root type that represents the empty interface within the runtime:\ntype eface struct {  \n    _type *_type  \n    data  unsafe.Pointer  \n}\n\n#iface is the root type that represents an interface with methods within the runtime:\ntype iface struct {\n    tab  *itab  \n    data unsafe.Pointer  \n}\n\nAn #itab is the heart & brain of an interface.\ntype itab struct { \n    inter *interfacetype  \n    _type *_type  \n    hash  uint32\n    _     [4]byte  \n    fun   [1]uintptr  \n}\n\nThus, #itab is used to store the mapping between an interface and a structure, and contains information about whether the structure implements the interface, via its fun[0] != 0 property.\n\nhttps://go.ptflp.ru/course2/img/8_13_38_04.png	96:00:00	2025-03-16 04:49:45.921199+00	t
141	10	Docker components	#code	1. Docker Engine is the core of Docker, a client-server application with a daemon, API, and CLI. It builds, runs, and manages Docker containers by packaging applications with their dependencies, ensuring consistent execution.\n\n2. Daemon is the main process responsible for managing containers, images, and other resources. It runs in the background and executes commands passed by the client. In other words, it is a Docker server. \n\n3. Client is a command line interface for interaction with Docker system. It provides connection with Docker service that is running locally or in remote. For example, the commands docker run or docker build are passed to the daemon through the client.\n\n4. Host is the machine on which Docker is running.\n\n5. Image is a template for creating new containers. It can include program code, various libraries, and project dependencies. Docker uses base and additional layers for building an image, which allows for efficient resource's management. Containers are created from images. \n\n6. A container is an isolated environment created on the basis of an image. Each container¬†can be thought of as a separate machine running the application as each container has its own¬†_file system_, its own¬†_processes_, its own¬†_network address_, or¬†_IP address_. However, it is still using the kernel an resources of the main system.\n\n7. Repository stores various image versions. All layer are _read-only_.\n\n8. Registry provides information about repositories. It can be either local or remote. The most popular remote registry is _Docker Hub_.	192:00:00	2025-03-23 04:37:15.601592+00	t
188	10	Inheritance	#code	#inheritance is a mechanism that allows you to create new classes (in the case of golang, these are various objects, mostly of type struct) based on existing ones. A class that inherits properties and methods from another class is called a subclass, and the class from which the properties and methods are inherited is called a superclass or parent class.  \n\nIn Golang, structures inherit methods of other structures by embedding (or composing). Interfaces inherit other interfaces, also by embedding. This allows you to reuse code and create a hierarchy of objects.	192:00:00	2025-03-16 15:34:07.34366+00	t
256	10	Relational database	#code	A relational database is a type of database that is based on the relational data model. In this model, data is organized into tables made up of rows and columns. Each row represents a record, while each column represents an attribute.\n\nKey concepts:\n- Tables: The fundamental structure for storing data in a relational database.\n- Rows (Records): Each row represents a single entity or instance of the data.\n- Columns (Attributes): Each column stores a specific piece of information about the entities in the table.\n- Relationships: Tables can be linked together based on relationships between their data.\n\nTables in a relational database can be linked to one another. For example, a users table may be related to a posts table, where each user can have multiple posts. This relationship is typically implemented by including a foreign key in the posts table that references the users table.\n\nWhile foreign keys are very useful for maintaining data integrity‚Äîensuring that a post references an existing user‚Äîthey are sometimes omitted to boost performance. Without a foreign key, a table might simply store a user ID, but using a foreign key adds an extra layer of verification.\n\nRelational databases are celebrated for their robustness and ability to handle complex queries. They commonly use Structured Query Language for data definition, manipulation, and querying. Moreover, techniques like normalization help reduce redundancy and maintain consistency across the database.	96:00:00	2025-03-14 15:40:43.181338+00	t
265	10	Longset Consecutive Sequence\n\nGiven an unsorted array of integers nums, return the length of the longest consecutive elements sequence.\n\nYou must write an algorithm that runs in O(n) time.	#code	func longestConsecutive(nums []int) (maxLen int) {\n    mp := make(map[int]bool, len(nums)) \n    for _, v := range nums {\n        mp[v] = true\n    }\n\n    for x := range mp {\n        if !mp[x-1] {\n            y := x+1\n            for ;mp[y]; y++ {}\n            maxLen = max(maxLen, y-x)\n        }\n    }\n\n    return maxLen\n}	96:00:00	2025-03-15 09:39:44.229696+00	t
92	10	epoll	#code	epoll is a Linux-specific system call that provides a more efficient way to monitor multiple file descriptors. It creates an interest list of file descriptors and efficiently monitors them for events.\n\n### Main features\n1. Highly efficient for numerous file descriptors due to its constant time complexity (O(1)).\n2. Supports edge-triggered and level-triggered modes* for event notification.\n3. Manages file descriptors and events in separate structures.\n\n### Key differences from poll\n1. More efficient for numerous file descriptors\n2. Linux-specific, not portable to other Unix systems.\n3. Offers more advanced features like edge-triggered mode.\n\n### Use case examples\n- High-performance network servers with a large number of concurrent connections.\n- Applications requiring maximum efficiency and scalability on Linux.\n\n*In edge-triggered mode, a call to epoll_wait will return only when a new event is enqueued with the epoll object, while in level-triggered mode, epoll_wait will return as long as the condition holds.\n\nhttps://disk.yandex.ru/i/IQLZc50KNTMwyw	384:00:00	2025-03-27 15:30:36.554141+00	t
205	10	Redis	#code	#redis (Remote Dictionary Server) is a high-performance database management system that provides caching and in-memory storage capabilities (basically, it is an in-memory database). It supports a variety of data types, including strings, lists, hash tables, sets, and sorted sets.\n\n### Redis' role in high load systems\nRedis is a popular tool for implementing caches in highly loaded systems for several reasons:\n\n- High performance. Redis stores data in RAM, which allows fast access to data. It is also optimized to handle numerous operations per second, making it ideal for highly loaded systems.\n\n- Flexibility. Redis supports a variety of data structures and provides powerful operations to work with them. This allows developers to leverage Redis for a variety of tasks, including caching, counters, user sessions, and more.\n\n- Reliability. Redis ensures high availability of data through replication and failsafe mechanisms. It also provides data persistence capabilities, allowing you to save data to disc for recovery after a system restart.\n\n- Extensibility. Redis supports horizontal scaling by creating a cluster of multiple nodes. This allows to handle a large volume of requests and ensures high data availability.\n\nRedis is a powerful tool for implementing caches in highly loaded systems, providing high performance, flexibility, reliability, and extensibility.\n\n### Most frequently used Redis commands\n\n1. SET key value:   Assigns the specified value to the given key.\n2. GET key:   Retrieves the value associated with the specified key\n3. DEL key [key ‚Ä¶]:   Deletes the specified key(s) from the database.\n4. EXPIRE key seconds:   Sets a timeout on the specified key, after which it will be automatically deleted.\n5. LPUSH key value [value ‚Ä¶]:   Inserts the specified value(s) at the head of the list stored at key.	96:00:00	2025-03-15 03:51:16.673576+00	t
375	10	Stack trace	#code	A stack trace is a list of function calls that shows the execution path a program took leading up to the point where an error or exception occurred. It's a crucial debugging tool that typically includes the following information:\n\n- Function Names: The stack trace lists the names of all functions that were called in sequence during the program's execution.\n- Line Numbers: For each function in the trace, it specifies the line number in the code where the next function call originated. This pinpoints the exact location in your code where the call was made.\n- Exceptions: If the stack trace is generated due to an error or exception, it will often include details about the type of error and the associated error message. This helps in understanding what went wrong.\n\nStack traces are powerful aids for debugging and identifying the root cause of problems in code, such as incorrect operations, stack overflows, memory access errors, and other issues. By examining the sequence of function calls in a stack trace, developers can effectively trace back the execution flow and pinpoint the exact location in the code where the error originated. This significantly speeds up the debugging process.	96:00:00	2025-03-16 11:38:17.214327+00	t
296	10	Consensus	#code	In the world of distributed systems and databases, consensus is like getting everyone to agree on something, even when there might be problems like computer crashes or slow internet connections. It's super important for making sure these systems are reliable and that everyone has the same information.\n\nImagine a group of friends trying to decide where to go for dinner. Consensus is like making sure everyone agrees on the restaurant, even if some people are late or their phones die.\n\n### Why Consensus Matters\n- Atomicity: It's all or nothing. Either everyone agrees on the same decision, or nobody does. There's no in-between.\n- Consistency: Everyone needs to be on the same page. Once a decision is made, everyone has the same understanding of what happened.\n- Durability: The decision sticks, even if computers crash and restart.\n- Fault Tolerance: The system can still reach a consensus even if some participants are unreliable or try to cause problems.\n\nAchieving consensus is tough, especially when you have lots of computers and things can go wrong. Some clever algorithms (like Paxos, Raft, and Zab) have been invented to help with this.\n\n### Where is consensus used?\n- Databases: Making sure all copies of data are the same.\n- File Systems: Ensuring everyone sees the latest version of a file.\n- Server Clusters: Coordinating tasks across multiple servers.	96:00:00	2025-03-19 12:19:53.259255+00	t
368	10	Grafana	#code	As a metrics visualization tool, Grafana excels at taking data from various sources and creating insightful visualizations.\n\nKey features of Grafana include:\n\n- Support for Diverse Data Sources: Grafana can integrate with a wide array of data sources. These include databases (such as Prometheus, InfluxDB, MySQL), monitoring systems, logs (via systems like Elasticsearch), APIs, and many others. \n\n - Flexible Panels and Graphs: Grafana offers rich customization for panels and graphs. Users can choose from various graph types, configure data queries, add annotations and time markers, and apply templates to create numerous panels with a consistent design.  \n\n- Interactive Dashboards: Grafana dashboards are interactive. Users can interact with graphs, change time ranges, apply data filters, and even create their own ad-hoc queries. \n\n- Alerts and Notifications: Grafana allows setting up alerts based on rules and conditions defined within dashboards. Users can receive notifications about problems via email, Slack, PagerDuty, and other channels.  Grafana's alerting complements Prometheus' alerting capabilities by providing visualization and dashboard-level alerting.	96:00:00	2025-03-14 16:45:10.360977+00	t
173	10	Gracefull shutdown	#code	It is a controlled shutdown process in which the application terminates by performing all the necessary operations to terminate correctly, such as closing database connections, saving data, and so on.\n\nA Golang application can process operating system signals to initiate graceful shutdown. The operating system can send various signals such as SIGINT (Ctrl+C), SIGTERM and others to notify the application to shut down.\n\nDuring graceful shutdown, the application must perform the following steps:\n1. Stop accepting new requests: and enter read-only mode.\n2. Finish processing current requests: by performing any necessary operations, such as saving data or closing database connections.\n3. Close connections and resources. Active client connections and resources: release resources in use and shutdown.	192:00:00	2025-03-21 14:51:34.511277+00	t
97	10	operating system kernel	#code	In operating systems, the kernel is the core program that manages system resources and facilitates communication between hardware and software. It acts as an intermediary between applications and the computer hardware, ensuring that different programs can run simultaneously without interfering with each other.\n\n### Key functions\n1. Process Management:   The kernel handles the creation, scheduling, and termination of processes. It allocates CPU time to various processes and manages their execution.\n\n2. Memory Management:   The kernel manages the system's memory, including allocating and deallocating memory for processes, handling virtual memory, and ensuring that processes do not access each other's memory space.\n\n3. Device Management:   The kernel communicates with hardware devices through device drivers, managing input and output operations and ensuring that applications can interact with hardware components like disks, printers, and network interfaces.\n\n4. File System Management:   OS kernel provides a unified way to interact with different file systems (ext4, XFS, Btrfs, etc.) through a layer called the Virtual File System (VFS).\n\n5. System Calls:   The kernel provides a set of APIs (application programming interfaces) that applications can use to request services from the operating system, such as file operations, process control, and communication.	192:00:00	2025-03-25 08:48:37.761982+00	t
329	10	Using MongoDB	#code	MongoDB is a document-oriented database management system (DBMS) that uses JSON for storing and exchanging data. Unlike relational databases, MongoDB doesn‚Äôt rely on tables. Instead, it stores data in documents‚Äîstructured JSON objects‚Äîthat can include any number of fields, with each field capable of holding any value (even another document or an array).\n\nThe main data types in MongoDB include:\n\n- String ‚Äì A Unicode string\n- Integer ‚Äì An integer number\n- Double ‚Äì A floating-point number\n- Boolean ‚Äì A boolean value\n- Date ‚Äì Date and time\n- Object ID ‚Äì A unique identifier for a document\n- Array ‚Äì An array of data\n- Binary Data ‚Äì Binary data such as images or videos\n\nThe data hierarchy in MongoDB is as follows:\n\n- Database ‚Äì A collection of collections.\n- Collection ‚Äì A set of documents, analogous to a table in relational databases. Each document in a collection has a unique identifier (ObjectId) that is used for searching and updating.\n- Document ‚Äì The basic data element in MongoDB, represented by a structured JSON object. A document can contain any number of fields, and each field may hold its own value, including another document or an array.	96:00:00	2025-03-18 04:13:39.676566+00	t
261	10	CAP theorem	#code	CAP is an abbreviation that stands for #consistency, #availability, #partitionTolerance. These are fundamental properties of distributed systems.\n\n- Consistency:  \n  In the context of a distributed system, consistency ensures that every node in the system views the same data at the same time. \n    \n- Availability:  \n  Availability means that every request to the system receives a response, and every request will be completed successfully.\n    \n- Partition Tolerance:  \n  Partition tolerance is the ability of a distributed system to continue to function and operate even when network partitions occur.\n\nThe CAP theorem directly relates to distributed systems, and particularly to distributed databases. This theorem, formulated by Eric Brewer in 2000, fundamentally states that in a distributed system, it is only possible to guarantee two out of these three properties, i.e. you cannot simultaneously achieve all three.\n\nPractical use:\n- CA: Traditional relational databases (RDBMS) often aim for CA in their distributed implementations.\n- CP: Examples of CP systems include databases like MongoDB and HBase.\n- AP: Consistency is often achieved eventually ("eventual consistency"). Examples of AP systems include databases like Cassandra and Couchbase.	96:00:00	2025-03-14 18:08:57.017122+00	t
276	10	Replication	#code	Replication is the process of creating and maintaining multiple identical copies of data across different servers or nodes. The primary goal of replication is to guarantee high data availability and fault tolerance. In a replicated system, if one server fails, other servers holding copies of the data can immediately take over, ensuring continuous service and preventing data loss.\n\nAdvantages:\n\n- Increases Data Fault Tolerance: With multiple copies of data, the system can withstand the failure of one or more servers without data loss. If a server fails, data is still available from the replicas.\n- Improves Data Availability: Replication ensures that data remains accessible even if some parts of the infrastructure fail. Users can continue to access data from the available replicas.\n- Enables Read Load Balancing: In read-heavy applications, read requests can be distributed across all replicas. This can significantly improve read performance and reduce the load on the primary database server.\n    \n\nExample:\n\n- Consider a database system where critical customer order information is replicated. The primary database server handles write operations, and its data is synchronously or asynchronously replicated to several replica servers. These replicas can then handle read queries from applications and serve as hot standbys ready to take over if the primary server fails.	192:00:00	2025-03-14 12:23:04.165456+00	t
148	10	Register types	#code	1. Accumulator: The accumulator register stores intermediate results of calculations and operations. It's the primary register for arithmetic operations.\n\n2. Index Registers: Index registers store memory addresses or offsets used for array indexing or accessing elements within data structures.\n\n3. General-Purpose Registers: These registers can be used for various purposes within a program. They can store function arguments, local variables, instruction addresses, and other data.\n\n4. Flag Registers: Flag registers store processor status flags, such as overflow, zero, negative result, and others. They play a crucial role in controlling program execution and making decisions based on conditions.\n\n5. Program Counter (PC): The program counter register holds the address of the next instruction to be executed by the processor. During instruction execution, the program counter's value is automatically incremented to point to the subsequent instruction.\n\n6. Segment Registers: Segment registers are used for managing segmented memory. They contain information about the base addresses of code, data, and stack segments, which are used to access the corresponding memory segments.\n\n7. Special Registers: Processors may have special registers with unique functions. For example, the _Stack Pointer_ points to the top of the stack, the _Flags Register_ contains processor status flags, and the _Status Register_ contains information about the processor's current state.	192:00:00	2025-03-17 04:01:40.143136+00	t
291	10	Normalization and denormalization	#code	Normalization and denormalization are two contrasting approaches to database design, each serving specific purposes in optimizing data structure.\n\n### Normalization\nNormalization is the process of organizing data in a database to reduce redundancy and ensure data integrity. This is achieved by dividing tables into smaller, related tables. Normalization helps prevent data anomalies, such as insertion, update, or deletion anomalies. The most common forms of normalization are the first, second, and third normal forms (1NF, 2NF, and 3NF).\n\n1. 1NF (First Normal Form)  \n   In 1NF, each table cell should contain only one value (atomic value), and each column should contain data of the same type. This eliminates repeating groups of data.\n\n2. 2NF (Second Normal Form)  \n   In 2NF, data must meet the requirements of 1NF, and every non-key column should be fully dependent on the primary key. This eliminates redundancy caused by partial dependencies, where a column depends on only part of a composite key.\n\n3. 3NF (Third Normal Form)  \n   In 3NF, data must meet the requirements of 2NF, and there should be no transitive dependencies. A transitive dependency occurs when a column depends on another non-key column. 3NF eliminates redundancy caused by this type of dependency.\n\n### Denormalization\nDenormalization, on the other hand, is the process of combining data from multiple tables into a single table. This is done to improve query performance, especially when fast data access is crucial. Denormalization introduces redundancy but can be acceptable in certain situations.\n\n### Choosing Between \nThe choice between normalization and denormalization depends on the specific needs of the application. Normalization is typically used in online transaction processing (OLTP) systems, where data integrity and minimizing anomalies are critical. \n\nDenormalization can be beneficial in online analytical processing (OLAP) systems, where fast data analysis and retrieval are prioritized. Sometimes, a hybrid approach is used, combining elements of both normalization and denormalization to optimize performance while maintaining data integrity.	192:00:00	2025-03-20 09:43:53.231846+00	t
201	10	Heap memory	#code	Heap memory is a dynamic pool of memory available to a program during runtime. Unlike stack memory, which is used for local variables and function calls, heap memory allows for more flexible allocation and deallocation of memory. Here's a breakdown of key points about heap memory:\n\n### Functionality\n__Dynamic allocation:__¬†Programs can request memory from the heap at runtime whenever needed. This is useful for data structures whose size might not be known beforehand or that need to grow or shrink dynamically.  \n\n__Deallocation:__¬†Programs are responsible for explicitly freeing memory allocated from the heap when it's no longer needed. This helps prevent memory leaks, where unused memory remains allocated.\n\n### Management\n__OS control:__¬†The operating system manages the heap memory for each running program. It keeps track of free and allocated regions within the heap.  \n\n__Allocation and deallocation:__¬†Programs typically use library functions like¬†`malloc`¬†(allocate) and¬†`free`¬†(deallocate) to interact with the heap. These functions communicate with the operating system's memory manager. \n\n### Comparison to Stack Memory\n__Allocation order:__¬†Heap memory allocation happens in a non-contiguous and potentially random order. This is because previously freed memory blocks can be scattered throughout the heap.\n\n__Access speed:__¬†Heap memory access might be slightly slower than stack memory due to the dynamic allocation and potential fragmentation.  \n\n__Lifetime:__¬†Data stored in heap memory persists as long as the program holds a reference to it (using a pointer) and frees it appropriately. Stack memory is automatically freed when the function that allocated it finishes execution.\n\n### Use Cases\n__Large data structures:__¬†Arrays, linked lists, trees, and other dynamic data structures that can grow or shrink in size often leverage heap memory.  \n\n__Objects with complex lifetimes:__¬†Objects whose lifetime spans multiple function calls or that need to be shared between different parts of the program are typically allocated on the heap.	192:00:00	2025-03-16 15:14:07.6792+00	t
73	10	Reverse Words\n\nGiven an input string s, reverse the order of the words.\n\nA word is defined as a sequence of non-space characters. The words in s will be separated by at least one space.\n\nReturn a string of the words in reverse order concatenated by a single space.\n\nNote that s may contain leading or trailing spaces or multiple spaces between two words. The returned string should only have a single space separating the words. Do not include any extra spaces.	#leetcode	func reverseWords(s string) string {\n    s = strings.TrimSpace(s)\n    words := strings.Fields(s)\n    for i, j := 0, len(words)-1; i < j; i, j = i+1, j-1 {\n        words[i], words[j] = words[j], words[i]\n    }\n    return strings.Join(words, " ")\n}	384:00:00	2025-04-27 04:21:53.209193+00	f
257	10	NoSQL	#code	#NoSQL, which stands for "Not Only SQL", represents an alternative approach to database design that moves away from the rigid structure of the relational data model. NoSQL databases are specifically designed to handle and manage __large volumes of diverse data__, especially data that isn't neatly structured or doesn't fit well into predefined table schemas. This makes them suitable for modern applications dealing with flexible, rapidly changing data and high scalability requirements.\n\n### NoSQL Database Types\nNoSQL databases are generally categorized into four main types, each optimized for different data structures and use cases:\n\n1. Key-Value Databases:  \n   These are the simplest type of NoSQL database, storing data as key-value pairs, similar to a dictionary or hash map. They are known for their speed and scalability, making them ideal for caching, session management, and real-time data serving.\n\tExamples: Redis, Memcached\n    \n2. Document-Oriented Databases:  \n   These databases store data in flexible, semi-structured documents, typically using JSON, XML, or similar formats. Each document can have a different structure, allowing for greater flexibility and easier adaptation to evolving data requirements. They are well-suited for content management, e-commerce platforms, and applications with complex, evolving data schemas.\n\tExamples: MongoDB, CouchDB\n    \n3. Column-Family Databases:  \n  These databases store data in columns grouped into column families, rather than rows like in relational databases. This columnar storage model is highly efficient for read-heavy workloads and massive datasets, particularly when only a subset of columns needs to be accessed. They are often used for analytics, time-series data, and applications requiring high write throughput and scalability.\n   Examples: Cassandra, HBase\n        \n4. Graph Databases:  \n  These databases are designed to store and query data represented as relationships between entities. They excel at managing complex, interconnected data and are optimized for graph traversal and relationship analysis. They are particularly useful for social networks, recommendation engines, knowledge graphs, and fraud detection.\n  Examples: Neo4j, OrientDB\n        	192:00:00	2025-03-24 03:44:43.728294+00	t
347	10	Microservices pros, cons, and challenges	#code	Microservices architecture breaks a complex application into smaller, independent services that are easier to develop, understand, and maintain. \n\nThis modular design offers several advantages over a monolithic approach:\n1. Each service can be scaled individually based on its specific load, leading to better resource utilization and flexibility. \n2. Teams can choose the best technology for each service, promoting innovation and allowing for faster, more focused development cycles since changes in one service don‚Äôt necessitate redeploying the entire application. \nThis separation also enables smaller teams to work autonomously on distinct services, further increasing overall agility.\n\nHowever, these benefits come with notable trade-offs: \n1. Managing a distributed system inherently increases complexity.\n2. It requires advanced infrastructure, robust tools for monitoring and deployment, and reliable service discovery methods. \n3. Network complexity is another significant challenge‚Äîcommunication between services introduces latency and potential points of failure, along with added security concerns. \n4. Ensuring data consistency across distributed services is difficult and often requires intricate solutions such as distributed transactions or eventual consistency models.\n\nMoreover, testing becomes more demanding as it now involves not just unit tests for individual services, but also comprehensive integration and functional tests to verify inter-service interactions. \n\nFinally, troubleshooting and monitoring the overall system necessitate sophisticated tracing and logging tools to effectively pinpoint and resolve issues.	96:00:00	2025-03-16 11:24:29.27187+00	t
278	10	Data sharding principles	#code	### Horizontal Sharding\nData is split horizontally, based on rows or records. Each such ‚Äúhorizontal‚Äù part is called a shard and contains a specific set of rows from the original table.\n\nOften used for geographically distributing data. For example, customer data from different regions can be stored in separate shards.\n\n### Vertical Sharding\nSplitting data vertically, based on columns. In this case, the original table is divided into several shards, where each shard contains a specific set of columns from the initial table.\n\nUseful for optimizing data access when some columns of a table are queried much more frequently than others. \n\n### Range Sharding\nDistributes data across shards based on a defined range of values. It could be date ranges, numerical value ranges, or alphabetical ranges (e.g., last names starting with A to K in one shard, and L to Z in another).\n\nWell-suited for data that has a natural order or range, and where queries are often based on this order or range.\n\n### Hash Sharding\nDistributes data among shards based on the result of a hash function.\n\nHash sharding provides a more uniform distribution of data across shards, which helps to avoid situations where some shards are overloaded while others are idle.\n\n### Type Sharding\nor example, you could create separate shards for different types of entities, such as ‚Äúusers‚Äù and ‚Äúproducts.‚Äù In this case, all data related to users would be stored in one set of shards, and data about products in another.\n\nAllows for better organization of data according to the logic of the subject domain and can simplify system management and scaling.\n\n### Composite Sharding\nUse a combination of different sharding methods. For example, different sharding strategies may be applied to different parts of the system or for processing different types of requests. \n\nAllows for the best possible consideration of all system features and the achievement of maximum performance.\n\nhttps://disk.yandex.ru/i/lhMntrJJcU2kXQ	96:00:00	2025-03-14 13:35:42.951801+00	t
272	10	Reduce database load using indexes	#code	Indexes are a powerful tool for optimizing database queries and reducing database load. Here are several ways to reduce load using indexes:\n\n1. Accelerating Searches:  \n   Indexes enable a Database Management System (DBMS) to quickly locate rows that satisfy specific conditions within a query. This is especially beneficial for queries using WHERE, JOIN, and ORDER BY clauses. Creating indexes on columns frequently used in these operations significantly speeds up query execution.\n    \n2. Reducing Load During Filtering:  \n    Utilizing indexes for data filtering can substantially decrease the number of rows the DBMS needs to process. This reduction in processing translates directly to lower CPU and memory load.\n    \n3. Optimizing Joins:  \n    When performing JOIN operations, indexes on the columns used for joining tables accelerate the process of combining data from multiple tables.\n    \n4. Sorting and Grouping:\n    Indexes can dramatically speed up the sorting (ORDER BY) and grouping (GROUP BY) of query results.\n    \n5. Improving Primary Key Lookups:  \n    Primary Keys are automatically indexed. Using a Primary Key to search for records based on unique identifiers is a very efficient method.\n\n### Beware Of\n1. Reducing Load on Data Insertion and Updates:  \n    While indexes enhance data retrieval speed, it's important to note they can slightly slow down data insertion and update operations. The optimization strategy here is to carefully select the most critical columns for indexing and avoid creating redundant or excessive indexes. A balance must be struck: indexes improve read performance but can slightly decrease write performance.\n    \n2. Using Appropriate Data Types:  \n    Selecting the correct data types for columns can also contribute to reduced load. For instance, using numerical data types for numerical values (instead of string types) speeds up searching and filtering operations. Using the most efficient data type for the data being stored is a fundamental aspect of database optimization and efficient indexing relies on this foundation.\n    \n3. Index Management:  \n    Index optimization is an ongoing process of choosing the most suitable indexes for specific query patterns. Regularly reviewing and adjusting indexes is essential. Periodically, it may become necessary to remove or recreate indexes to maintain and improve performance as data and query patterns evolve over time. This might involve analyzing query execution plans to identify inefficient index usage or missing indexes.\n\nhttps://disk.yandex.ru/i/TJov3JEh9BJKhg	96:00:00	2025-03-16 16:14:50.152805+00	t
262	10	Database transactions	#code	A #transaction is a sequence of operations that are performed as a single logical unit of work. Transactions are fundamental to maintaining data integrity in relational databases and other systems where data consistency is critical. They ensure that data remains in a valid and consistent state even when operations are interrupted by system failures or concurrent access.\n\n### Transaction Patterns\n1. Savepoint:  allows you to establish "savepoints" within a transaction. Savepoints are like markers in your transaction's progress. If a part of the transaction fails after a savepoint, you can rollback the transaction only to the nearest savepoint, rather than undoing the entire transaction.\n\n2. Gather-Insert:  Instead of locking the entire table, which can lead to performance bottlenecks, each client initially inserts their data into a separate, temporary table. Then, within a transaction, these temporary tables are efficiently merged or copied into the main target table.\n\n3. Pessimistic Locking:  An application acquires locks on resources (e.g., rows in a table) immediately when a transaction begins and holds these locks until the transaction is completed (either committed or rolled back).\n\n4. Optimistic Locking:  transactions do not hold locks for the duration of the entire transaction. Instead, data is read without acquiring locks. Before updating data, the system checks if the data has been changed since it was initially read at the start of the transaction.\n\n5. Distributed Transaction:  comes into play in distributed systems, where a single transaction needs to operate on resources across multiple, independent systems ‚Äì for example, databases located on different servers or even different types of storage.\n\n6. Compensating Transaction:  is used to handle errors in systems where a traditional rollback is not always feasible. Instead of a direct rollback, a compensating transaction is designed to undo or correct the effects of a previously executed (but now deemed "failed" or needing reversal) transaction.\n\nhttps://disk.yandex.ru/i/1uaVNEx04YgbTA	192:00:00	2025-03-20 12:53:13.931175+00	t
372	10	Scaling Prometheus	#code	If Prometheus is struggling to handle the monitoring load, here are several scaling strategies:\n\n- Horizontal Scaling of Prometheus:\n    - Adding Additional Prometheus Instances: You can deploy multiple Prometheus instances and configure them to collect metrics from different parts of your infrastructure. Each Prometheus instance would then be responsible for scraping metrics from a subset of targets.\n    - Federation: Prometheus Federation allows you to aggregate metrics from multiple Prometheus instances into a central Prometheus server. This creates a hierarchical monitoring setup. Federation enables scaling by offloading scraping and storage to individual Prometheus instances while providing a consolidated view at a central level.\n        \n- Using Remote Write and Remote Read:\n    - Remote Write: Configure Prometheus to send (write) metrics to a remote data storage system. Options include Thanos, Cortex, or other compatible solutions. This offloads the storage burden from the Prometheus server itself, allowing it to handle more scrape targets.\n    - Remote Read: Use Remote Read to query (read) metrics from a remote data storage system. This can reduce the query load on the Prometheus server, especially for long-term data analysis or when Grafana dashboards are heavily used.\n        \n- Using Long-Term Data Storage:  \n  Configure Prometheus to use long-term storage solutions. By offloading older data to specialized long-term storage, you can keep the main Prometheus instance focused on recent, high-resolution data. This improves query performance and reduces resource usage on the primary Prometheus server.\n\n- Query Optimization:\n    When writing PromQL queries, strive for efficiency. Select only the necessary metrics and optimize query structure to avoid complex operations that can negatively impact performance.\n    \n- Using Caching:\n    Components like Grafana can use caching to reduce the load on the Prometheus server. Configure caching where applicable in your environment, especially for frequently accessed dashboards.\n    \n- Resource Monitoring and Optimization:\n    Continuously monitor the resources (CPU, memory, disk space) of the server running Prometheus. Adjust resource allocation as needed to meet demand. \n    \n- Using Scaling Tools:  \n  Consider using container orchestration platforms like Kubernetes or Docker to manage and scale your Prometheus infrastructure. These tools simplify the deployment, scaling, and management of Prometheus instances. Kubernetes, for example, facilitates automated scaling and high availability for Prometheus.	96:00:00	2025-03-16 05:15:14.919548+00	t
363	10	Caching algorithms	#code	Caching algorithms determine which data to evict (remove) from the cache when it's full to make space for new data. Common algorithms include:\n\n- Least Recently Used (LRU):  This algorithm discards the data that hasn't been accessed for the longest period. It assumes that data used recently is more likely to be used again soon.\n    \n- Least Frequently Used (LFU): LFU evicts the data that has been accessed the fewest number of times. It prioritizes data that is accessed more often, even if it hasn't been accessed recently.\n    \n- First-In, First-Out (FIFO): FIFO is a simple algorithm that removes the data that was added to the cache earliest, regardless of how frequently it has been accessed.\n    \n- Random Replacement (RR): As the name suggests, RR randomly selects data to remove from the cache when space is needed. This is typically used as a baseline for comparison with more sophisticated algorithms.	96:00:00	2025-03-14 11:33:11.488254+00	t
84	10	TCP/IP	#code	Transmission Control Protocol/Internet Protocol is a suite of protocols used for communication and data transmission in computer networks, including the Internet. TCP/IP is the fundamental protocol stack that underpins network communication in modern networks. \n\nIt works based on the concept of the client-server model, where the client and server exchange data over a TCP connection and consists of two main protocols:\n\n1. TCP   provides reliable, ordered, and error-checked delivery of data across the network. It breaks data into packets, manages the establishment and termination of connections, controls data flow, and ensures acknowledgment of delivery. TCP guarantees that data will be delivered in the correct order and without loss or corruption. \n\n2. IP   handles the routing and addressing of data packets within the network. It assigns unique IP addresses to devices on the network and determines how data packets should be forwarded from the sender to the receiver through various nodes and routers in the network. \n\nIP has two versions in use today: IPv4, which uses 32-bit addresses, and IPv6, which uses 128-bit addresses to accommodate the growing number of devices connected to the Internet.\n\nTCP/IP is essential for the functioning of the Internet and is widely used in both local and wide area networks. Its design allows for interoperability between different types of networks and devices, making it a versatile and robust framework for data communication.\n	192:00:00	2025-03-20 12:11:56.305366+00	t
349	10	Microservices architecture patterns	#code	Microservices architecture relies on several key patterns to address challenges and enhance system resilience, scalability, and maintainability:\n\n1.Service Discovery allows dynamic detection of service instances in volatile environments‚Äîusing tools like Consul or Kubernetes DNS‚Äîto eliminate hardcoded network addresses. \n\n2. An API Gateway centralizes client access, managing routing, authentication, authorization, rate limiting, and protocol translation, thereby abstracting the microservices' internal complexity. \n\n3. The Circuit Breaker pattern improves fault tolerance by temporarily halting calls to a failing service to prevent cascading issues, while... \n\n4. ...the Bulkhead pattern isolates failures by partitioning resources such as thread pools, ensuring that a fault in one component doesn‚Äôt compromise others. \n\n5. The Saga Pattern manages distributed transactions by breaking them into smaller local transactions and orchestrating compensating actions if any fail, thus ensuring eventual consistency. \n\n6. Event Sourcing preserves a sequence of state-changing events rather than only the current state, aiding in debugging and audits. \n\n7. CQRS (Command-Query Responsibility Segregation) separates write operations from read operations, allowing independent optimization of each pathway for better performance and security.	96:00:00	2025-03-15 17:41:44.518316+00	t
260	10	ACID implementation	#code	DBMS employ various mechanisms and techniques to implement ACID properties. These typically include:\n\n1. Logging (Journaling):  \n   This involves recording all transaction operations and data changes in a transaction log (or journal) _before_ they are applied to the actual database. \n    \n2. Locking:  \n   Locking mechanisms are essential for ensuring isolation. When a transaction needs to access or modify data, locks are used to prevent other concurrent transactions from interfering. Different types of locks exist (e.g., shared locks for reading, exclusive locks for writing) with varying levels of restrictiveness. \n    \n3. Rollback Mechanisms:  \n   In case a transaction cannot be completed successfully (e.g., due to errors, constraint violations, or system failures), rollback mechanisms are used to undo any changes made by the transaction and restore the database to its state before the transaction began. \n    \n4. Logging and Replication:  \n   To enhance durability and provide fault tolerance, DBMS often use replication in conjunction with logging. Database replication involves creating and maintaining multiple copies of the database on different servers. \n    \n\nDifferent DBMSs may implement ACID properties with variations in their specific techniques and approaches.\n\n- Relational Databases (RDBMS) like PostgreSQL and MySQL are traditionally strong in ACID compliance. They heavily rely on transactions and logging as core features to guarantee these properties. They often provide configurable isolation levels, allowing developers to choose the balance between concurrency and strict isolation based on application needs.\n    \n- Non-relational databases (NoSQL DBMS) like Cassandra and MongoDB, while originally prioritizing scalability and availability over strict ACID guarantees in distributed environments, have evolved to offer varying degrees of ACID-like properties. For instance, they might provide atomicity and durability at the level of a single document or within a partition, but might not guarantee full ACID across multiple documents or partitions in a distributed cluster by default. \n\nSome NoSQL databases offer tunable consistency levels, allowing users to choose between strong consistency (more ACID-like) and eventual consistency (prioritizing availability and performance). The term BASE (Basically Available, Soft state, Eventually consistent) is often used to describe the consistency model of some NoSQL databases, contrasting it with ACID.\n\nhttps://disk.yandex.ru/i/Ko5ckEKs3MnAQg	96:00:00	2025-03-15 14:51:23.716299+00	t
359	10	Algorithms for load balancing	#code	Server-side load balancing can use several key algorithms, each with its own advantages based on the nature of the workload and server characteristics. Here are some of the main ones:\n\n- __Round Robin:__ Distributes requests sequentially across servers in a cyclic order. This method is simple and works well when all servers have roughly equal capacity.\n    \n- __Weighted Round Robin:__ Similar to Round Robin, but each server is assigned a weight based on its capacity. Servers with higher weights receive more requests, allowing the system to account for different server performance levels.\n    \n- __Least Connections:__ Routes new requests to the server with the fewest active connections. This helps balance the load when servers have varying connection durations.\n    \n- __Weighted Least Connections:__ Enhances the Least Connections method by incorporating server weights, ensuring that more powerful servers handle a proportionally higher share of the load.\n    \n- __IP Hash (or Client Hashing):__ Uses a hash of the client‚Äôs IP address (or another client attribute) to determine which server receives the request. This can help with session persistence by ensuring that a client‚Äôs requests are consistently routed to the same server.\n    \n- __Random Selection:__ Chooses a server at random for each request. While simple, it can be effective in environments where the load is naturally balanced.\n    \n- __Consistent Hashing:__ A variation of IP Hash that minimizes re-distribution of clients when servers are added or removed. It‚Äôs particularly useful in distributed caching systems and dynamic environments.\n    \n\nEach of these algorithms is chosen based on the specific requirements, such as the need for session persistence, the variability in server performance, or the type of workload being balanced. For instance, environments with highly variable request durations might benefit from the Least Connections method, while a system with diverse server capacities might perform better with weighted strategies.	96:00:00	2025-03-16 08:59:18.632775+00	t
423	10	Preorder vs Inorder vs Postorder	#code	In Preorder Traversal, the root node is visited first, followed by the left and right subtrees (Root‚ÜíLeft‚ÜíRight). \n\nInorder Traversal starts with the left subtree, visits the root, and then the right subtree, often used in binary search trees (Left‚ÜíRoot‚ÜíRight). \n\nPostorder Traversal begins with the left subtree, moves to the right subtree, and visits the root last, useful for deleting nodes (Left‚ÜíRight‚ÜíRoot).\n\nhttps://www.geeksforgeeks.org/preorder-vs-inorder-vs-postorder/	24:00:00	2025-03-15 10:46:23.680427+00	t
123	10	HTTP request	#code	### HTTP Methods\n\nHTTP methods define the action that the client wants to perform on a resource. Some common methods include:\n\n- GET: Retrieves data from a specified resource.\n- POST: Submits data to be processed to a specified resource.\n- PUT: Replaces all current representations of the target resource with the request entity.\n- DELETE: Deletes the specified resource.\n- HEAD: Similar to GET, but only retrieves the header information.\n- OPTIONS: Describes the communication options for the target resource.\n\n### HTTP Headers\n\nHTTP headers contain additional information about the request or response. They are key-value pairs. Some common headers include:\n\n- Host: Specifies the domain name of the server.\n- User-Agent: Identifies the client application.\n- Content-Type: Indicates the media type of the entity body.\n- Content-Length: Indicates the size of the entity body.\n- Cookie: Stores client-side data.\n- Set-Cookie: Sets a cookie on the client.\n- Authorization: Provides authentication credentials.\n\n### HTTP Request Body\n\nThe request body contains the data sent by the client to the server. It's typically used with POST, PUT, and PATCH methods. The format of the data depends on the Content-Type header. Common formats include:\n\n- JSON: JavaScript Object Notation\n- XML: Extensible Markup Language\n- Form data: Key-value pairs typically used for form submissions	96:00:00	2025-03-15 17:23:51.730066+00	t
170	10	Authorization	#code	#identification ‚Äî  assigning a number or sequence of characters that will allow distinguishing an object from others;\n#authentication ‚Äî proving that the identified person is indeed who they claim to be;\n#authorization ‚Äî is a process of assigning certain roots based on identify.\n\n### Authorization by token\n1. User fills in their credentials to login into the account;\n2. Server generates a #token ‚Äî¬†unique sequence of characters¬†‚Äî¬†and sends it to the user;\n3. The token is saved in the user's web browser;\n4. When opened again, the browser sends the token to the server;\n5. The server verifies token and relevant information;\n6. The user becomes authorized or receives an errors message;\n7. When logged out, the browser deletes the token.\n\n### Token structure\nA token of JWT (JSON Web Token) standard consists of three parts:\n- #header ‚Äî¬†service information;\n- #payload ‚Äî token's data;\n- #signature ‚Äî¬†prevents token data replacement.	96:00:00	2025-03-17 18:22:17.644887+00	t
258	10	In-memory storage	#code	In-memory storage (or memory storage) is a technology that utilizes a computer's Random Access Memory (RAM) to store data instead of traditional Hard Disk Drives (HDDs) or Solid State Drives (SSDs). The primary advantage of in-memory storage lies in the rapid data access, as RAM is significantly faster than disk-based storage. This characteristic makes in-memory storage solutions ideal for applications demanding high performance and low latency in data processing.\n\n### Why Use In-Memory Storage?\n- Increased Performance: RAM is significantly faster than disk-based storage, allowing data to be accessed for reading and writing very quickly. This speed advantage translates directly into improved application performance and responsiveness.\n    \n- Reduced Latency: In-memory storage solutions minimize latency when accessing data. This is crucial for applications demanding extremely low response times, such as financial trading platforms, real-time analytics dashboards, and competitive online gaming environments.\n    \n- Improved Scalability: In-memory storage solutions can be easily scaled horizontally to handle high loads. Horizontal scaling means adding more nodes (servers) to distribute the workload, which is a common and effective way to manage increasing data and traffic demands in modern applications.\n    \n- Data Caching: They are frequently employed for data caching to accelerate data access. Caching in memory avoids the need to retrieve data from slower persistent storage repeatedly, drastically improving access times for frequently used information.\n    \n- Real-time Operations: In-memory storage is ideally suited for real-time data processing, such as real-time analytics, fraud detection, and system monitoring. The ability to process and analyze data instantly as it arrives is essential for many modern applications that need to react to events in real-time.	96:00:00	2025-03-15 03:26:49.216209+00	t
72	10	Common file system types	#code	- ext4 ‚Äî file system is the default choice for many Linux distributions. It supports journaling, can efficiently handle large files and partitions, backward-compatible with ext3 and ext2.\n\n- XFS ‚Äî is a high-performance journaling file system ideal for handling large files and parallel I/O operations. It is known for effective scalability in environments with massive amounts of data and online defragmentation capabilities.\n\n- Btrfs ‚Äî (B-tree file system) is designed for fault tolerance and advanced features as snapshots (allow point-in-time copies of the file system) and dynamic inode allocation.\n\n- ZFS (Zettabyte File System) ‚Äî is an advanced file system originally developed for Solaris-based operating systems but also available on Linux. It offers powerful features such as data integrity checks, snapshots, error recovery, and efficient memory usage.\n\n- FAT32 and exFAT ‚Äî are used for compatibility with non-Linux systems. They lack advanced features but are widely supported across different platforms.	192:00:00	2025-03-21 08:24:50.373554+00	t
370	10	Zabbix	#code	Zabbix is a powerful and popular open-source monitoring system designed to track the status and performance of various IT infrastructure components. This includes servers, network equipment, databases, applications, and much more. Zabbix is an all-in-one monitoring solution.\n\nKey features and capabilities of Zabbix include:\n\n- Monitoring Diverse Resources: Zabbix can monitor virtually all components of your infrastructure. This includes servers (physical and virtual), network devices (routers, switches), databases, applications, and even custom metrics.  \n\n- Data Collection: Zabbix actively collects data about the health and status of systems and devices.  This includes collecting metrics via SNMP, JMX, IPMI, HTTP, as well as the ability to create custom scripts and checks. \n\n- Automatic Resource Discovery: Zabbix can automatically discover new resources and begin monitoring them based on predefined templates.  Auto-discovery simplifies the onboarding of new infrastructure components for monitoring.\n\n- Flexible Alerting: Zabbix provides numerous options for alerting administrators about problems.  This includes sending emails, SMS messages, chat messages, and triggering API calls. .\n\n- Graphs and Visualization: Zabbix allows creating visually appealing graphs and dashboards for visualizing metrics and system performance.  Built-in visualization capabilities provide immediate insights into monitoring data.\n\n- Templates and Configuration: You can define monitoring templates for different types of resources and quickly apply them to new devices. \n\n- Event and Trigger System: Zabbix can automatically react to events and state changes, triggering specific actions or alerts.  Event-driven actions enable automated responses to monitoring data, like restarting services or running scripts.	96:00:00	2025-03-15 13:48:03.565061+00	t
169	10	JWT	#code	JWT is an open standard (RFC 7519) that defines tokenized user info representation. It consists of three parts:\n1. Header;\n2. Payload;\n3. Signature.\n\n### Header\nstores information about token type and signature algorithm. Typically, it's a JSON with two fields:\n\n–ü—Ä–∏–º–µ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–∞ JWT:\n{\n  "alg": "HS256",\n  "typ": "JWT"\n}\n\n### Payload (claims)\nbears user info and other data that needs to be transferred. It can have multiple user-defined JSON fields:\n\n{\n  "sub": "1234567890",\n  "name": "John Doe",\n  "admin": true\n}\n\n### Signature\nA secret key is used to sign and verify the JWT. When creating a token, the header and payload are converted into a JSON string, then signed using the chosen algorithm and secret key. The resulting signature is added to the token.\n\nWhen validating the token, the recipient extracts the header and payload from the token, re-signs them using the same algorithm and secret key, and compares the resulting signature with the original signature. If the signatures match, the token is considered valid.\n\n### Authorization bearer\nJWT can be transferred via HTTP Authorization header using Bearer scheme. Authorization header has ‚ÄúBearer‚Äù prefix followed by a token:\n¬†\nAuthorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c	192:00:00	2025-03-19 14:32:16.190921+00	t
267	10	Transaction isolation levels	#code	Transaction isolation levels determine what data is visible to concurrent transactions while a transaction is in progress. They are essential for balancing data consistency with system performance. Below are the main isolation levels along with additional context:\n\n1. Read Uncommitted (Reading Uncommitted Data)  \n    At this level, a transaction can see changes made by other transactions even if those changes have not yet been committed. While this allows for high concurrency, it introduces the risk of "dirty reads"‚Äîwhere uncommitted (and possibly later rolled back) data is accessed.\n    \n2. Read Committed (Reading Committed Data)  \n    In this isolation level, a transaction sees only the data that other transactions have committed. This approach prevents dirty reads but can still lead to non-repeatable reads, where data might change if re-read during the transaction.\n    \n3. Repeatable Read  \n    Here, a transaction sees only the data that was read at the start of the transaction. Any changes made by other transactions to that data after the initial read are not visible, ensuring consistency for repeated reads. However, this level might still be susceptible to phantom reads, where new rows added by other transactions may appear in subsequent queries.\n    \n4. Serializable  \n    This is the strictest isolation level. Transactions are executed in a way that they appear to run sequentially rather than concurrently. A transaction sees only the data it initially read; if any concurrent transaction modifies that data, a conflict is detected and one of the transactions will be aborted to maintain full isolation. This level prevents all common concurrency issues but can significantly impact performance due to increased locking and reduced concurrency.\n    \n\nEach isolation level offers a trade-off between performance and consistency. Choosing the right level depends on the specific requirements of your application and the acceptable balance between data integrity and system throughput.	96:00:00	2025-03-16 09:03:46.365072+00	t
406	10	Thread pool, its size, what it does	#code	In Go the ‚Äúthread pool‚Äù refers to the set of OS threads managed by the Go runtime scheduler that execute goroutines. Here are the key points:\n\n### What It Is\nIt‚Äôs a pool of operating system threads that the runtime reuses to run many lightweight goroutines. The Go scheduler maps numerous goroutines onto these threads using an m:n scheduling model.\n\n### Size\nThe effective size of this pool is largely determined by the GOMAXPROCS setting, which defaults to the number of available CPU cores. This parameter controls how many threads can run Go code concurrently. However, if a goroutine blocks (for example, on I/O), the runtime may allocate additional threads temporarily to keep the program running efficiently.\n\n### Benefits\nThe thread pool allows Go to efficiently manage concurrency by avoiding the overhead of constantly creating and destroying OS threads. By reusing threads, the runtime ensures better performance and scalability when handling thousands or even millions of goroutines.\n\nThus, the thread pool is an essential part of Go‚Äôs runtime infrastructure that helps to provide high-performance concurrency while keeping resource usage optimized.	24:00:00	2025-03-14 14:24:03.580049+00	t
53	10	init()	#code	init is the first process that is started during the system boot. When the init process terminates, the system is rebooted or shut down. The PID (Process ID) of the init process is always equal to 1.\nIts primary functionalities include:\n\n1. System Initialization: Sets up the environment for the operating system, including mounting filesystems, initializing hardware, and loading necessary drivers.\n\n2. Service Management: Starts and manages essential system services and daemons (e.g., networking, logging, and scheduling).\n\n3. Runlevel Management: Handles system runlevels (or targets in systemd), determining which services are started or stopped based on the desired system state (e.g., single-user mode, multi-user mode, or graphical mode).\n\n4. Orphan Process Handling: Acts as the parent process for orphaned processes, ensuring they are properly reaped to prevent resource leaks.\n\n5. Shutdown and Reboot: Manages system shutdown, reboot, or transition to single-user mode by stopping services and cleaning up resources.\n\nIn modern systems, init is often replaced by more advanced implementations like systemd, Upstart, or SysV init, but its core purpose remains the same: to initialize and manage the system lifecycle.	384:00:00	2025-03-23 03:19:47.708676+00	t
242	10	Golang idioms	#code	Golang is designed with simplicity and efficiency in mind. Over time, the Go community has established a set of idioms and best practices that guide developers toward writing clear and maintainable code. Here are some of the key idioms in Go:\n\n### Multiple Return Values\nGo functions can return multiple values, a feature often used for returning a result along with an error.\n  \n### Named Return Values\nFunctions can have named return values, which can enhance code readability and allow for simplified return statements.\n\n### Defer\nThe defer statement schedules a function call to be executed after the surrounding function completes, commonly used for resource cleanup.\nDeferred calls are executed in last-in, first-out order.\n\n### Variadic Functions\nGo allows functions to accept a variable number of arguments, known as variadic functions.\n\n### Error Handling\nIn Go, errors are treated as values. The idiomatic way to handle errors is to check them immediately after an operation. This pattern ensures that errors are handled gracefully and promptly.\n\n### Interfaces\nGo emphasizes the use of small, focused interfaces. A well-known proverb in the Go community is, ‚ÄúThe bigger the interface, the weaker the abstraction.‚Äù This encourages defining minimal interfaces that capture the essential methods.\nBy keeping interfaces small, implementations remain straightforward and flexible.\n\n### Composition Over Inheritance\nGo does not support traditional class-based inheritance. Instead, it promotes composition, where complex types are built by combining simpler ones.\n\n### Goroutines and Channels\nGo provides built-in support for concurrent programming through goroutines and channels.\nThe go keyword launches a new goroutine. Channels can be used to synchronize and communicate between goroutines.\n\n### The Blank Identifier\nThe blank identifier _ is used to ignore values in assignments.\n\n### Initialization\nGo provides the init function for package-level initialization. The init function is called automatically when the package is initialized, before the program‚Äôs main execution begins. It‚Äôs typically used to set up state or initialize resources that the package requires.\n\n### Closures\nGo supports anonymous functions, which can form closures. A closure is a function value that references variables from outside its body. The function may access and assign to the referenced variables.\n\n### Methods\nIn Go, you can define methods on types. This is a powerful feature that allows you to define behavior associated with a particular type, making your code more object-oriented.\n\n### Function Types\nFunctions in Go are first-class citizens, which means they can be assigned to variables, passed as arguments, and returned from other functions.	96:00:00	2025-03-16 03:05:51.435349+00	t
131	10	TLS/SSL handshake	#code	The exchange of data between a server and a client during a TLS/SSL handshake occurs in the following stages:\n\n1. Client Hello: The client initiates the connection by sending a "Client Hello" message to the server. This message includes information such as the TLS/SSL version supported by the client, a random number (client random), and a list of supported cipher suites (encryption algorithms).\n\n2. Server Hello: The server responds with a "Server Hello" message. This message contains the TLS/SSL version chosen by the server, a random number (server random), the selected cipher suite, and the server's SSL/TLS certificate.\n\n3. Certificate: The server sends its SSL/TLS certificate to the client. The certificate contains the server's public key and information about the certificate authority (CA) that issued the certificate.\n\n4. Certificate Verification: The client verifies the validity of the server's SSL/TLS certificate. This involves checking the certificate's expiration date, ensuring that it was issued by a trusted CA, and verifying that the domain name in the certificate matches the domain name of the server. If the verification fails, the connection is terminated.\n\n5. Pre-master Secret: If the certificate is valid, the client generates a random "pre-master secret." The client encrypts this pre-master secret using the server's public key (obtained from the certificate) and sends it to the server.\n\n6. Key Exchange: The server decrypts the pre-master secret using its private key. Both the client and the server now possess the pre-master secret. Using the client random, the server random, and the pre-master secret, both parties independently calculate the session keys. These session keys are symmetric keys used for encrypting and decrypting the actual data transmitted during the session.\n\n7. Change Cipher Spec & Finished: The client sends a "Change Cipher Spec" message to inform the server that it will now start encrypting data using the newly negotiated session keys. It then sends an encrypted "Finished" message to verify that the key exchange was successful.\n\n8. Change Cipher Spec & Finished: The server does the same, sending a "Change Cipher Spec" message and an encrypted "Finished" message to the client.\n\n9. Secure Communication: After these steps, the TLS/SSL handshake is complete. All subsequent data exchanged between the client and the server is encrypted using the negotiated session keys, ensuring secure communication.\n\nhttps://disk.yandex.ru/i/HJgrbBDvRlCwQA	96:00:00	2025-03-14 19:01:41.437713+00	t
350	10	Geohashing	#code	Geohash is a geocoding system invented by Gustavo Niemeyer that allows us to determine the area on a map where a user is located. While latitude and longitude represent a single point on the map, a geohash describes a fixed area, making it more flexible and convenient for location-based search services. Geohashing can also be used to provide anonymity, as it doesn‚Äôt require revealing the user‚Äôs exact location. Depending on the length of the geohash, we can determine that the user is somewhere within a specific area.\nhttps://files.bool.dev/site/blog/f2b69526-8bdc-4fe0-9c49-955fc9727fd9/edeaeb32-c3dd-4566-82ae-28444839a57a.png	96:00:00	2025-03-15 05:50:15.1354+00	t
421	10	Asynchronous system calls	#code	### Asynchronous System Calls\nIn Go, certain system calls (like I/O operations) are handled asynchronously, meaning they don't block the entire program while waiting for an operation to complete.\n\n### Separate Thread\nThese asynchronous calls are executed in a dedicated thread separate from the main execution thread.\n\n### gomaxprocs=1\nEven if you set `gomaxprocs=1`‚Äîwhich limits the Go scheduler to using only one OS thread for running goroutines‚Äîthese asynchronous system calls will still run concurrently because they are managed by that separate thread.\n\nIn short, even with a single processing thread allocated for goroutines, asynchronous system calls are offloaded to their own thread and can execute concurrently, ensuring that long-running I/O or blocking operations do not halt the progress of your program.	96:00:00	2025-03-16 04:01:42.445944+00	t
413	10	UTF-8	#code	> Understanding UTF-8 encoding involves grasping how it represents Unicode characters using a variable-width approach.\n\n### Code Point\nUnicode assigns a unique numerical value, called a ‚Äúcode point,‚Äù to every character, symbol, and glyph. UTF-8 is a way to encode these Unicode code points into a sequence of bytes. It uses a variable number of bytes (1 to 4) to represent a Unicode code point. This means that frequently used characters (like those in the ASCII range) take up less space, while less common characters require more bytes.\n\n### Backward Compatibility with ASCII\nThe first 128 Unicode code points (U+0000 to U+007F) are identical to the ASCII character set.  These are encoded using a single byte in UTF-8, ensuring that ASCII text is also valid UTF-8.\n\n### Encoding Structure\n The leading bits of each byte in a UTF-8 sequence indicate whether it's a single-byte character or part of a multibyte sequence. This allows decoders to easily identify the start and end of each character.\n\n### Size of Symbols\nThe number of bytes required to encode a Unicode character in UTF-8 depends on its code point value. Here's a general guideline:\n - 1 byte: ASCII characters (U+0000 to U+007F)\n - 2 bytes: Characters in the range U+0080 to U+07FF\n - 3 bytes: Characters in the range U+0800 to U+FFFF\n - 4 bytes: Characters in the range U+10000 to U+10FFFF\n\n### Cyrillic Symbols\nCyrillic characters fall within the Unicode range that typically requires 2 bytes in UTF-8. This is because Cyrillic characters are within the U+0400 to U+04FF Unicode range. So they are within the range that requires 2 bytes.\n\nIn summary, UTF-8's efficiency lies in its ability to adapt the number of bytes used based on the character's frequency and complexity.	24:00:00	2025-03-14 14:21:00.786336+00	t
321	10	ClickHouse	#code	ClickHouse is an open‚Äësource, column‚Äëoriented DBMS designed for real‚Äëtime analytics and OLAP workloads. Instead of storing data in rows like traditional databases, it organizes data by columns, which allows for fast aggregations and efficient compression. \n\nOriginally developed at Yandex, it excels at processing massive datasets‚Äîmaking it ideal for applications such as:\n- real‚Äëtime reporting, \n- log/event analysis,\n- IoT data processing, \n- and ad‚Äëhoc queries. \n\nIts distributed architecture enables horizontal scaling across multiple nodes, ensuring high performance even as data volumes grow. \n\nKey advantages include: \n- rapid query execution, \n- cost savings from reduced hardware needs, \n- and high throughput for analytical queries. \n\nHowever, ClickHouse is not designed for OLTP scenarios, offering limited support for transactional updates and complex single‚Äërow operations. Optimal performance may require careful configuration and tuning, and its SQL support, while robust for analytics, may not cover all advanced SQL features. \n\nOverall, ClickHouse is best suited for environments where fast, large‚Äëscale data analysis is paramount.	96:00:00	2025-03-19 11:53:50.135318+00	t
397	10	XSS attacks	#code	XSS (Cross-Site Scripting) attacks are vulnerabilities in web applications that occur when an attacker injects malicious JavaScript code into a webpage. This code then executes in the browsers of unsuspecting users, potentially leading to session hijacking, phishing, unauthorized content changes, and other harmful consequences. Here are some effective methods to prevent XSS attacks:\n\n1. Data Escaping:   \n   Before embedding any data into a webpage, ensure that all special characters (like <, >, &, ", and ') are properly escaped or replaced with their corresponding HTML entities (for example, &lt;, &gt;, &amp;, &quot;, and &#39;).\n\n2. Implement a Content Security Policy (CSP):   \n   A CSP is a security header that defines which sources of content are allowed to run on a page. This policy can block unauthorized scripts from executing.\n\n3. Validate and Filter Input Data:   \n   Enforce strict validation on all incoming data to make sure it conforms to expected formats and types. Reject any data that doesn't meet these criteria.\n\n4. Secure Session Handling:   \n   If your application stores session data on the client side (such as in cookies), ensure that these sessions are well-protected against tampering.\n\n5. Use HTTP Only and Secure Cookies:   \n   When using cookies for sessions, set the HttpOnly and Secure flags. HttpOnly prevents JavaScript access to cookies, and Secure ensures cookies are only transmitted over HTTPS.\n\n6. Sanitize Data:   \n   In situations where escaping and validation aren‚Äôt enough, employ sanitization libraries that remove or strip out potentially dangerous content from user input.\n\n7. Follow the ‚ÄúSecure by Default‚Äù Principle:   \n   Design your application with the assumption that all user-supplied data is potentially harmful, and handle it accordingly from the start.	96:00:00	2025-03-18 19:09:38.932534+00	t
60	10	Network interfaces on Linux	#code	On Linux, a network interface is a logical or physical device that facilitates data exchange between a computer and a network. A network interface is the bridge between your Linux system and the external network. It's essentially how computer communicates with the outside world.\n\nA network interface on Linux can be an Ethernet network card, a Wi-Fi wireless adapter, a modem, or another network device. Each network interface has a unique name. On Linux, the most common naming convention for network interfaces is based on the following schemes:\n\n- Ethernet interfaces are typically named eth0 or enp0s3;\n- Wi-Fi wireless interfaces may be named wlan0 or wlp2s0\n- The loopback interface, which is a virtual interface for internal communication at the local level, is named lo.\n\nNetwork interfaces on Linux can be configured to work with various network protocols, including IPv4 and IPv6. They can also be configured with different network parameters, such as:\n\n- IP address\n- Subnet mask\n- Default gateway\n- DNS servers\n\nTo manage and configure network interfaces in Linux, various command-line tools are used, including: ifconfig, ip addr show, iw wlan0 info.	96:00:00	2025-03-15 03:48:41.427746+00	t
358	10	Reverse proxy	#code	In web applications, a reverse proxy is commonly used for load balancing.\n\nA reverse proxy is a server that sits in front of backend servers. It receives incoming requests from clients and then forwards those requests to one of the backend servers or instances.\n\nPopular choices for reverse proxies that are used as load balancers include:\n\n- Nginx: Widely adopted and often considered the de facto standard for reverse proxy and web server functionalities.\n- HAProxy: Known for its speed and reliability, especially in high-traffic environments.\n- Envoy: A modern, high-performance proxy, often used in cloud-native and microservices architectures.\n- Traefik: A cloud-native edge router that is designed to be easy to configure and integrates well with containerized environments like Docker and Kubernetes.	96:00:00	2025-03-17 03:43:38.228117+00	t
331	10	Elastic search	#code	Elasticsearch is a distributed search and analytics engine with support for various languages, hight performance, and schema-free JSON documents. It is commonly used for log analytics, full-text search, business analytics, security and operational intelligences.\n\n### How it Works\nYou can send data in the form of JSON documents to Elasticsearch using the API or ingestion tools such as¬†Logstash.¬†Elasticsearch automatically stores the original document and adds a searchable reference to the document in the cluster‚Äôs index. You can then search and retrieve the document using the Elasticsearch API. You can also use¬†Kibana, a visualization tool, with Elasticsearch to visualize your data and build interactive dashboards.\n\n### Benefits\n1. Fast time-to-value:  \n   Elasticsearch offers simple REST-based APIs, a simple HTTP interface, and uses schema-free JSON documents, making it easy to get started and quickly build applications for various use cases.\n2. High performance:  \n   The distributed nature of Elasticsearch enables it to process large volumes of data in parallel.\n3. Complimentary tooling and plugins:  \n   Integration with Logstash, Kibana and other open-source plugins such as language analyzers and suggesters enables for reach functionality in your application.\n4. Near real-time operations:  \n   Elasticsearch operations such as reading or writing data usually take less than a second to complete, which makes it handy in application monitoring and anomaly detection.\n\n> Since January 21, 2021 Elasticsearch and Kibana are not open source projects. For a secure, high-quality, fully open-source search and analytics suite, you can use the¬†OpenSearch*¬†project, a community-driven, ALv2 licensed fork of open-source Elasticsearch and Kibana.\nhttps://opensearch.org/	96:00:00	2025-03-16 09:25:46.032574+00	t
322	10	Redis applications	#code	Redis is used in many different scenarios. Here are some common examples:\n\n1. Caching:   \n   Redis is often employed to cache data, significantly speeding up access by keeping information in memory.\n\n2. Sessions:   \n   It can manage user sessions by storing session data in memory, ensuring rapid retrieval.\n\n3. Data Storage:   \n   Redis isn‚Äôt just for caching‚Äîit‚Äôs also capable of storing various data structures, including lists, sets, hashes, and more.\n\n4. Task Queues:   \n   Redis can efficiently handle task queues for processes such as image or video processing, payment handling, and other background jobs.\n\n5. Event Subscription:   \n   It supports real-time event publishing and subscribing, making it ideal for handling live updates and notifications.\n\n6. Analytics:   \n   Redis is useful for data analysis, such as processing event logs, time-series data, transactional records, and similar datasets.\n\n7. Geospatial Data:   \n   It can store and process geospatial information like location coordinates, regions, cities, and other location-based data.\n\n8. Graph Data:   \n   Redis is capable of managing graph structures, which makes it suitable for applications like social networks or modeling relationships between various entities.\n\n9. Counters:   \n   It can also keep track of counts, such as tracking visits, likes, comments, and other metrics.	96:00:00	2025-03-16 10:14:36.021013+00	t
353	10	Leaky bucket	#code	The Leaky Bucket algorithm provides the simplest, most intuitive approach to rate limiting using a queue, which can be thought of as a ‚Äúbucket‚Äù containing requests. When a request is received, it is added to the end of the queue. At regular intervals, the first item in the queue is processed. This is also known as a FIFO queue. If the queue is full, additional requests are dropped (or ‚Äúleak‚Äù).\n\nThe advantage of this algorithm is that it smooths out bursts and processes requests at approximately the same rate. It is easy to implement on a single server or load balancer and is memory efficient, as the queue size for each user is limited.\n\nHowever, during a sudden traffic spike, the queue may fill up with old requests, depriving the system of the ability to process newer requests. It also does not guarantee that requests will be processed within a fixed time. Additionally, if you load balance for fault tolerance or increased throughput, you must implement a coordination policy to ensure global rate limiting between them.\n\nhttps://files.bool.dev/site/blog/f2b69526-8bdc-4fe0-9c49-955fc9727fd9/eeeb37b8-e08e-4423-a3d0-8ffeddf8b061.png	96:00:00	2025-03-15 05:44:44.580994+00	t
348	10	Microservice scaling	#code	Scaling is critical in microservice architecture to ensure performance, fault tolerance, and adaptability under varying workloads. \n\n### Vertical Scaling (Scale Up)\nIncreasing the resources (CPU, memory, storage) of a single server hosting a microservice instance. This method works for moderate load increases but is limited by the physical constraints of the server.\n\n### Horizontal Scaling (Scale Out)\nAdding more instances of a microservice to distribute incoming requests. This is the most common and recommended approach as it enhances both scalability and fault tolerance by preventing any single instance from becoming a bottleneck.\n\n### Auto-Scaling\nDynamically adjusts the number of microservice instances based on real-time load metrics such as CPU usage and request queue length. This automated process optimizes resource utilization by scaling out when demand increases and scaling in as demand subsides.\n\n### Load Balancing\nDistributes incoming traffic evenly across multiple instances, ensuring that no single instance is overwhelmed. It also contributes to system resilience by rerouting traffic from failed instances.\n\n### Scalable Microservice Design\nKey design principles include:\n- Statelessness: Simplifies horizontal scaling.\n- Data Partitioning and Sharding: Distributes data across databases to improve performance.\n- Asynchronous Processing: Employs message queues or event-driven architectures for non-blocking operations.\n- Fault Tolerance: Uses strategies like circuit breakers, retries, and graceful degradation to handle failures.\n- Caching:  Implements in-memory or distributed caches to reduce the load on databases and improve response times.\n- Functional Decomposition: Breaks down complex microservices into smaller, specialized services to remove bottlenecks and improve manageability.\n- Monitoring and Analytics:  Uses tools (e.g., Prometheus, Grafana) to continuously assess performance, identify bottlenecks, and guide scaling decisions.\n\nCollectively, these strategies form a comprehensive approach to designing microservices that are flexible, resilient, and capable of handling fluctuating demands.	96:00:00	2025-03-16 18:20:29.223489+00	t
360	10	Database load balancing	#code	In databases, replication is frequently used as a form of load balancing, particularly for read operations.\n\nReplication in databases involves copying data from one database server (the master or primary) to one or more other database servers (replicas or secondaries).\n\nDatabase replication can be categorized as:\n\n- Synchronous Replication: In synchronous replication, data is written to the master server and then immediately copied to all replica servers _before_ the transaction is considered complete. This ensures strong data consistency across all servers, but can introduce some latency.\n    \n- Asynchronous Replication: In asynchronous replication, data is written to the master server first, and then copied to the replica servers shortly after, but not necessarily immediately. This approach generally offers better performance and lower latency for write operations, but there might be a slight delay in data being reflected on the replicas.\n    \n\nA typical setup for database load balancing involves a master database server and multiple replica database servers. All write operations (e.g., INSERT, UPDATE, DELETE) are directed to the master server. Read operations (SELECT queries), which usually constitute the majority of database requests in many applications, are distributed among the replica servers.	96:00:00	2025-03-14 13:06:49.083928+00	t
328	10	MongoDB	#code	MongoDB is a document-oriented NoSQL database that stores data in flexible, JSON-like documents rather than rigid relational tables. This flexibility allows developers to rapidly iterate on application design, as the schema can evolve without significant downtime or database restructuring.\n\n### Key Characteristics\n- Document Model: Data is stored in BSON (Binary JSON) documents, which makes it easier to represent hierarchical relationships.\n- Scalability: It supports horizontal scaling via sharding, allowing applications to manage large volumes of data and high throughput.\n- High Availability: Built-in replication and automated failover ensure that your applications remain resilient and available.\n\n### Key Applications\n1. Web and Mobile Applications: MongoDB‚Äôs flexible schema is ideal for rapidly developing applications that require frequent changes to data structures, such as e-commerce sites, social networks, and mobile apps.\n\n2. Content Management Systems: Its document-based structure allows for efficient storage and retrieval of diverse and dynamic content.\n\n3. Real-Time Analytics: The ability to quickly ingest and process large volumes of data makes it suitable for real-time analytics and reporting.\n\n4. Internet of Things (IoT): MongoDB efficiently handles time-series data generated by IoT devices, supporting real-time monitoring and analysis.\n\n5. Big Data and Data Hub: It can serve as a central data repository that aggregates information from various sources, making it easier to analyze trends and patterns across large datasets.\n\nMongoDB is widely adopted by organizations that require agile development and the ability to scale quickly in response to growing data volumes and user demand. Its versatility makes it a go-to choice for modern, data-driven applications.	96:00:00	2025-03-15 04:08:39.85828+00	t
428	10	Mutexes vs Semaphores vs Conditional Variables	#code	Mutexes are simple locks that ensure exclusive access to a critical section. When a thread locks a mutex, it gains sole access to a shared resource, blocking other threads from entering until it unlocks the mutex. This is particularly useful when protecting sensitive shared data‚Äîsuch as updating a bank account balance‚Äîto prevent concurrent modifications.\n\nSemaphores expand on the concept of exclusive locking by incorporating a counter that tracks available resources. They allow a fixed number of threads to access a resource concurrently. With operations like ‚Äúwait‚Äù (which decrements the counter and blocks the thread if no resources are available) and ‚Äúsignal‚Äù (which increments the counter), semaphores are ideal for managing limited resources, such as a pool of database connections where only a certain number of threads may interact with the database simultaneously.\n\nConditional Variables enable threads to wait for a specific condition before proceeding. They are always used with a mutex to avoid race conditions. A thread waiting on a condition variable temporarily releases the mutex and sleeps until another thread signals that the required condition (like new data arriving) is met. This mechanism is especially effective in scenarios such as the producer-consumer model, where consumers wait for producers to supply data.\n\nIn summary, while mutexes enforce strict one-thread-at-a-time access, semaphores manage access to a finite number of identical resources, and conditional variables facilitate waiting for particular conditions to be met.	96:00:00	2025-03-17 16:36:47.105669+00	t
146	10	Machine word	#code	In computer architecture, a machine word refers to the fundamental unit of data that a processor (or CPU‚ÄîCentral Processing Unit) can handle in a single operation. It's a fixed-size group of bits that the processor processes as a single unit. The machine word size determines the size of the processor's registers and the width of the data bus used for transferring data between the processor and memory or other devices.\n\n\nThe size of the machine word typically determines how much data the computer can process at once, i.e. the maximum amount of memory the processor can address. For example, a 32-bit computer can address a maximum of 4 GB (gigabytes) of RAM (Random Access Memory), while a 64-bit computer can theoretically address up to 16 EB (exabytes) of RAM.\n\nThis influences the architecture's performance and capabilities. Here are the common categories based on machine word size:\n\n1. 8-bit Computers: Early microcontrollers, some vintage personal computers (e.g., Commodore 64).\n\n2. 16-bit Computers; Intel 8086, early gaming consoles (e.g., Sega Genesis), and some early personal computers.\n\n3. 32-bit Computers: Intel Pentium series, many personal computers from the 1990s to early 2000s, and some embedded systems.\n\n4. 64-bit Computers: Modern personal computers, servers, and workstations (e.g., Intel Core i7, AMD Ryzen).\n\n5. 128-bit Computers: While there are no general-purpose 128-bit CPUs, some specialized processors (e.g., certain GPUs) and cryptographic applications utilize 128-bit data types.\n\n### Key Points\n- The machine word size affects the amount of memory a computer can address and the types of data it can process efficiently.\n- As technology has advanced, the shift from 8-bit to 64-bit architectures has allowed for greater computational power and memory capacity.\n- While 128-bit architectures exist, they are not commonly used in general-purpose computing.	192:00:00	2025-03-26 20:21:06.428273+00	t
427	10	REST	#code	REST (Representational State Transfer) is an architectural style introduced by Roy Fielding that outlines a set of constraints to build scalable, efficient web applications. It doesn‚Äôt prescribe a specific protocol but provides guidelines such as \n1. statelessness, \n2. client‚Äìserver separation, \n3. cacheability, \n4. a uniform interface, \n5. layered system design, \n6. and optionally, code on demand. \n\nIn a stateless system, every client request must contain all the information needed to process it, freeing the server from maintaining session state and enhancing scalability.\n\nWhen an API adheres strictly to these REST constraints, it‚Äôs termed ‚ÄúRESTful.‚Äù While many APIs use HTTP methods like GET, POST, PUT, and DELETE (which map to CRUD operations) and exchange data typically in JSON or XML, not all of them fully comply with every REST principle. For instance, many ‚ÄúREST APIs‚Äù may not implement hypermedia controls (HATEOAS) or enforce strict cacheability, making them more ‚ÄúREST-like‚Äù rather than fully RESTful. \n\n> The Richardson Maturity Model is one way to gauge this adherence, with higher levels (especially Level 3, which includes HATEOAS) being indicative of a more fully RESTful system.\n\n\nRESTful APIs are popular across web, mobile, cloud, IoT, streaming, and payment services because they leverage existing HTTP infrastructure, promote loose coupling between client and server, and offer scalability and flexibility. Their standardized nature makes them easier to understand, integrate, and maintain, ultimately serving as the backbone for modern distributed web services.	96:00:00	2025-03-18 12:11:29.080736+00	t
390	10	TDD	#code	Test-Driven Development is a software development practice where developers write tests _before_ writing the actual code.\n\nBy writing tests first, developers approach coding with a testing mindset, focusing on inputs and outputs. This perspective helps them better understand the code from a QA standpoint, often resulting in code that is inherently more testable and better designed.\n\nThe TDD Cycle (Red-Green-Refactor):\n- Red: Write a test that is expected to fail initially (because the corresponding code hasn't been written yet).\n- Green: Write the minimum amount of code necessary to make the test pass.\n- Refactor: Improve the code's structure, readability, and efficiency without changing its external behavior. This includes making the code more abstract, readable, and optimized while ensuring all tests still pass.	96:00:00	2025-03-15 04:13:38.981292+00	t
154	10	Tricolor algorithm	#code	Imagine coloring all objects in memory one of three colors:\n\n- White:   Potentially garbage. These objects haven't been reached by the GC during the current cycle. At the end of the GC cycle, white objects are considered unreachable and are collected.\n- Gray:   Reachable, but the GC hasn't yet scanned its children (the objects it points to). Gray objects are on the ‚Äúto-do‚Äù list.\n- Black:  Reachable, and the GC has scanned its children. These objects are definitely alive.\n\nThe tri-color algorithm is crucial for concurrency. It allows the GC to make progress even while the program is running. The gray color acts as a ‚Äúworklist,‚Äù keeping track of objects that need to be processed.\n\nhttps://disk.yandex.ru/i/uisCCxT2NidGWw	192:00:00	2025-03-21 04:04:19.250017+00	t
345	10	Microservices data consistency problem	#code	Ensuring data consistency among independent microservices is a significant challenge that can be addressed using several strategies. \n\nOne common method is leveraging distributed transactions (ACID transactions), where a set of operations across services is treated as a single atomic unit. If any part of the transaction fails, the entire operation is rolled back, maintaining data integrity. However, this approach can lead to performance overhead and scalability issues due to the complexity of managing distributed transactions.\n\nAnother approach involves asynchronous updates that promote eventual consistency. In this model, message queues (such as Kafka or RabbitMQ) are used to publish data change events, and microservices subscribe to these events to update their local data. While this may lead to temporary inconsistencies, the system eventually converges to a consistent state‚Äîmaking it ideal for highly scalable systems where immediate consistency is not essential.\n\nFor more intricate scenarios, transaction orchestration using the Saga pattern can be implemented. Here, an orchestrator service manages the sequence of transactions across microservices, and if one step fails, compensating transactions are executed to undo preceding actions, thereby preserving overall consistency.\n\nAdditional consistency models include \n- strong consistency, which ensures that all subsequent accesses reflect the latest data update, and \n- durability or quorum consistency, where operations are confirmed only after data is reliably stored across multiple nodes. Specialized tools like distributed transactional coordinators (e.g., Two-Phase Commit) can enforce atomicity, though they tend to be complex and can create bottlenecks. \n\nAlternatively, centralized data storage can serve as a single source of truth, simplifying consistency management, but it may reduce scalability and lead to tighter coupling between services.	96:00:00	2025-03-16 13:21:37.272349+00	t
94	10	Is palindrome\n\nA phrase is a palindrome if, after converting all uppercase letters into lowercase letters and removing all non-alphanumeric characters, it reads the same forward and backward. Alphanumeric characters include letters and numbers.\n\nGiven a string s, return true if it is a palindrome, or false otherwise.	#leetcode	func isPalindrome(s string) bool {\n    s = tidy(s) \n\n    for i := 0; i < len(s)/2; i++ {\n        if s[i] != s[len(s)-1-i] {\n            return false\n        }\n    }\n\n    return true\n}\n\nfunc tidy(s string) string {\n    var str strings.Builder\n    for _, b := range s {\n        if b >= 65 && b <= 90 {\n            b += 32\n        }\n        if !(b >= 48 && b <= 57 || b >= 97 && b <= 122) {\n            continue\n        }\n        str.WriteRune(b)\n    }\n    return str.String()\n}	384:00:00	2025-05-01 06:30:43.152213+00	f
71	10	Linux file system components	#code	1. Superblock ‚Äî is a critical part of the operating system. It contains file system _metadata_ such as: total size, block size, free block count, free inode count, file system state (clean or needs repair). Due to its importance, the¬†_superblock_¬†is stored in multiple redundant copies across the file system.\n\n2. Block Group ‚Äî is a subdivision of the file system that contains a fixed number of blocks. Each block group has its own copy of the¬†`superblock`, a¬†`block bitmap`, an¬†`inode bitmap`, and an¬†`inode table`, facilitating efficient file system management.\n  \n3. Data Block and Inode Bitmaps are data structures that keep track of  data blocks inodes availability.\n\n4. Inode Table. Inodes store _metadata_¬†about files and directories. Each inode contains:\n  - File type: Regular file, directory, symlink, etc.\n  - Permissions: Read, write, and execute permissions for the user, group, and others.\n  - Ownership: User ID and group ID.\n  - Timestamps: Creation, modification, and access times.\n  - Pointers to data blocks: Locations of the actual data blocks.\n  \n5. Data Block is the basic unit of storage in a file system and. Data blocks store the actual content of files. The size and number of data blocks determine how much data a file system can hold.\n\n6. Fragment is a subdivision of data block, it is the smallest unit of storage that the file system can allocate to a file. \n\n7. Directories in Linux are special files that map file names to¬†`inode`¬†numbers. This structure allows the file system to locate files based on their names. The hierarchical nature of directories helps organize files logically.\nhttps://miro.medium.com/v2/resize:fit:764/1*dTU2g9ynFbcdI1dzwGjLAw.png\n\n8. Journal (optional). Journaling file systems, like¬†`ext4`¬†and¬†`XFS`, use a journal to log changes before they are committed. This feature helps recover from crashes by replaying or reverting changes, ensuring data integrity.	384:00:00	2025-03-27 19:54:58.32148+00	t
264	10	Saga	#code	Saga is an alternative approach to managing distributed transactions, designed to overcome some limitations of 2PC. Saga focuses on providing flexibility and fault tolerance in distributed systems, particularly in long-lived and scalable microservices architectures.\n\nUnlike 2PC, Saga does not aim for strict atomicity within a single global transaction. Instead, Saga breaks down a long distributed transaction into a sequence of local transactions, each performed within the scope of one service (or resource).\n\n### Saga Operation Principle\n- Decomposition into Local Transactions: A large distributed business transaction is broken down into a series of smaller, autonomous #steps or #operations. Each step is a local transaction that is executed atomically in the context of a single service.\n\n- Compensating Transactions: Instead of a global rollback like in 2PC, Saga uses compensating transactions to handle errors. If one step of a Saga fails, a sequence of compensating transactions is triggered to undo the effects of previously successfully executed local transactions. Compensating transactions are essentially ‚Äúreverse‚Äù operations that cancel out or correct the changes made by previous steps.\n\n- Organization of Step Sequence: Sagas can be implemented with different approaches to managing the sequence of steps and compensations. The most common approaches are:\n    - #choreography: Saga participants interact directly by exchanging events. Each service ‚Äúknows‚Äù when it needs to perform its step and when to initiate compensation based on received events from other services. There is no central coordinator.\n    - #orchestration: A central orchestrator (coordinator service) manages the entire Saga sequence. It sends commands to each participant to perform local transactions and, in case of errors, commands to execute compensations.	96:00:00	2025-03-15 03:58:01.310008+00	t
153	10	Garbage collector	#code	Garbage collection is the process of freeing up memory space that is no longer in use. Garbage collector sees which objects are out of scope and cannot be referenced anymore, and frees the memory space they consume. This process happens concurrently while a Go program is running. \n\n> The GC runs concurrently with mutator threads, is type accurate (aka precise), allows multiple GC threads to un in parallel. It is a concurrent mark and sweep that uses a write barrier. It is non-generational and non-compacting. Allocation is done using size segregated per P allocation areas to minimize fragmentation while eliminating locks in the common case.\n\n### Key Characteristics\n- Non-Generational:   Go's GC doesn't divide objects into generations (young/old). This simplifies the implementation but can sometimes lead to more frequent collections.\n\n- Concurrent (Mostly):   The GC runs concurrently with the main program (mutator). This significantly reduces pause times. It's _mostly_ concurrent because it does require short "stop-the-world" (STW) phases.\n\n- Tri-Color Mark and Sweep:   Go uses a tri-color mark and sweep algorithm. This is the heart of how it identifies and reclaims unused memory.\n\n- Write Barriers:   To maintain correctness during concurrent marking, Go uses write barriers. These ensure that the GC doesn't miss objects that become reachable during the marking phase.	192:00:00	2025-03-22 12:12:49.845171+00	t
298	10	Logical partitioning	#code	Logical partitioning is a data management strategy used in databases where large tables are divided into smaller, more manageable logical sections called partitions. These partitions are created based on specific criteria, such as values in one or more columns. This approach enables efficient management and storage of large volumes of data, making it easier to perform read and write operations.\n\n### Key Aspects of Logical Partitioning\n- Performance Improvement: Logical partitioning can significantly enhance query performance by allowing the system to work with only the relevant data subset required for a specific query.\n    \n- Data Volume Management: This strategy facilitates the management of large datasets by breaking them down into smaller, more manageable units.\n    \n- Simplified Data Archiving and Deletion: Partitioning simplifies data archiving and deletion as entire partitions, rather than individual rows, can be removed or archived when no longer needed.\n    \n- Improved Maintenance: Maintenance and optimization tasks can be performed on individual partitions, simplifying database management.\n    \n- Data Distribution: Partitioning allows for easy distribution of data across different physical storage devices or servers, which is beneficial in distributed systems.\n    \n- Access Control: Partitioning can be used to manage data access, for example, by granting users access to different partitions.\n    \n- Scalability: This approach improves scalability as new partitions can be added as needed without altering the structure of the entire table.\n    \n\nExamples of criteria for logical partitioning include time ranges, categories, geographical regions, and more, depending on the nature of the data and the application's requirements.	192:00:00	2025-03-26 03:06:33.939887+00	t
377	10	Canary deployment	#code	Canary deployments are similar to blue-green deployments but offer more granular control and employ a progressive, phased rollout approach. This category includes various strategies, such as ‚Äúdark launches‚Äù and A/B testing.\n\nThis strategy is particularly useful when you need to test new functionality, especially in the backend of an application, in a real production environment with live user traffic but with minimal risk. The core idea is to have two almost identical server setups. One server setup continues to serve the vast majority of users with the current stable version. The other server setup, containing the new features or changes, serves only a small subset of users, the ‚Äúcanary‚Äù group. The performance and error rates between these two groups are then carefully compared and monitored.\n\nIf the ‚Äúcanary‚Äù version performs as expected without errors, the new version is gradually rolled out to the entire infrastructure, replacing the old version step-by-step.\n\nCanary deployments limit the impact of a potentially problematic new release by exposing it to a small percentage of users initially. It allows for real-world testing and monitoring before a full rollout, reducing risk. This approach is ideal for validating performance and stability under production load for backend changes.	96:00:00	2025-03-15 06:40:17.541269+00	t
323	10	NATS	#code	NATS is a lightweight message‚Äêoriented middleware that enables real‚Äêtime data exchange between applications and services. It empowers developers to build scalable, distributed client-server architectures that can operate seamlessly across various environments, languages, cloud platforms, and on-premises systems.\n\nAt its core, NATS splits into two main components: \n1. client applications \n2. and service infrastructure. \n\nClient applications integrate one of the NATS client libraries into their code, enabling them to publish, subscribe, request, and reply to messages. This interaction model allows distinct applications or instances within a single application to communicate effectively through a simple message‚Äêexchange paradigm based on subject strings rather than network locations.\n\nThe service infrastructure is provided by one or more NATS server processes that can be interconnected. Despite its minimal footprint (with the nats-server process being less than 20 MB), this infrastructure scales from a single process running on an end device to a global super-cluster‚Äîexemplified by solutions like Synadia's NGS‚Äîspanning multiple cloud providers and regions.\n\nTo connect a client application with a NATS service, developers need only configure a NATS URL (which specifies the IP address, port, and connection type such as TCP, TLS, or WebSocket) and provide authentication details if required. NATS supports a range of authentication methods including username/password, token, decentralized JWT, TLS certificates, and Nkey with challenge.\n\nNATS‚Äô design emphasizes simplicity and efficiency: messages are published by senders and processed by one or more subscribers, allowing applications to share common message-handling logic, isolate dependencies, and scale to handle increasing message volumes.\n\nAdditionally, NATS offers different quality of service levels. The core functionality ensures an ‚Äúat most once‚Äù delivery‚Äîreminiscent of TCP‚Äôs fire-and-forget approach‚Äîwhile NATS JetStream, when enabled, provides enhanced features like persistent streaming, flow control, and support for ‚Äúat least once‚Äù or ‚Äúexactly once‚Äù delivery guarantees.	96:00:00	2025-03-17 04:04:32.682899+00	t
312	10	Types of locks	#code	Databases employ various locking mechanisms to control data access and prevent conflicts when multiple users or processes attempt to access the same information simultaneously. Below are some of the main types of locks along with their typical applications:\n\n1. Shared Lock (S-lock):   \n   This lock allows multiple processes to read data at the same time, but prohibits write operations.\n\n2. Exclusive Lock (X-lock):   \n   This lock prevents other processes from accessing the data until the lock is released (can neither read or write).\n\n3. Update Lock (U-lock):   \n   A specialized lock that permits one process to update data while still allowing others to read it.\n\n4. Intent Lock:   \n   These locks signal a process‚Äôs intention to acquire a more granular lock on a higher-level data structure. For instance, an INTENT SHARE (IS) lock indicates that the process plans to obtain a Shared Lock (S-lock) on a lower level. This mechanism helps reduce the chances of recursive locking issues.\n\n5. Schema Lock:   \n   Schema locks restrict access to the database structure itself. They are typically used during operations like altering a table‚Äôs structure (using ALTER TABLE) or creating new indexes.\n\n6. Row-Level Lock:   \n   This type of lock targets individual rows rather than locking an entire table. It is employed in systems that support row-level locking, minimizing conflicts by allowing simultaneous operations on different parts of the table.\n\n7. Page-Level Lock:   \n   Here, the lock is applied to a page‚Äîa fixed-size block of data within a table. Some systems use page-level locks as a compromise between the granularity of row-level locks and the broader scope of table-level locks.\n\n8. Table-Level Lock:   \n   This lock covers the entire table and is generally used sparingly because it significantly impacts concurrency and overall performance.\n\nhttps://disk.yandex.ru/i/vRU-eh1Dzdy8ng	96:00:00	2025-03-16 04:38:43.697571+00	t
392	10	SemVer	#code	Semantic Versioning is a system and set of conventions designed to simplify version management for libraries and applications. Its core principles are:\n\n- Major Version:  \n    Incremented when you introduce changes that are incompatible with previous versions.\n    \n- Minor Version:  \n    Incremented when you add new functionality in a backward-compatible way.\n    \n- Patch Version:  \n    Incremented when you implement backward-compatible bug fixes.\n    \n- Pre-release Versions:  \n    These are indicated by suffixes such as "alpha," "beta," or "rc" (release candidate), often followed by a number (e.g., "1.0.0-alpha.1"). They are subject to change until the final stable release.\n    \n- Metadata:  \n    Additional build or version information can be appended, for instance:  \n    1.0.0+20130313144700 might include a timestamp indicating when the version was created.\n    \n\n_Example:_ For the version "2.1.0":\n\n- The major version is 2.\n- The minor version is 1.\n- The patch version is 0.	96:00:00	2025-03-19 07:24:00.214725+00	t
373	10	Log collection tools	#code	To collect logs from Kubernetes containers, you can utilize a variety of tools. Here are several popular examples:\n\n1. Fluentd: A widely-used tool designed for collecting, aggregating, and forwarding logs from containers to various storage destinations. These destinations can include systems like Elasticsearch, Kafka, AWS S3, and many others.\n\n2. Logstash: Another tool that facilitates log collection, processing, and forwarding. Logstash excels at transforming and enriching log data before it is sent to storage. This pre-processing can be invaluable for making logs more searchable and insightful.\n\n3. Promtail: Specifically developed as a log collection client for Prometheus, a leading monitoring system in Kubernetes environments. Promtail efficiently gathers logs from containers and sends them to Prometheus for further processing and analysis. This integration makes it a natural choice for teams already using Prometheus for monitoring.\n\n4. ELK Stack: This represents a comprehensive suite of tools for end-to-end log management, covering collection, analysis, and visualization. It includes:\n  - Elasticsearch: For robust log storage and powerful search capabilities.\n  - Logstash: For flexible log collection and processing (as described above).\n  - Kibana: For creating rich visualizations and dashboards to explore and analyze log data.\n\n5. Splunk: A powerful log management platform offering extensive features for collecting, aggregating, analyzing, and visualizing logs at scale. Splunk provides sophisticated search and analysis functionalities, making it easy to pinpoint the root cause of issues and thoroughly investigate system behavior.	96:00:00	2025-03-15 17:25:26.819579+00	t
48	10	Journaled file system	#code	A journaled file system is a type of file system that keeps track of changes not yet committed to the main part of the file system by recording them in a data structure known as a journal or log. This journal acts as a safeguard to ensure data consistency and integrity, especially in the event of a system crash, power failure, or other unexpected interruptions.\n\n\n### How It Works:\n1. Journaling Process: Before any changes (such as file creation, deletion, or modification) are written to the main file system, they are first recorded in the journal.\n2. Commit Changes: Once the changes are safely logged in the journal, they are then applied to the main file system.\n3. Crash Recovery: If the system crashes or loses power before the changes are fully written to the main file system, the journal can be used to "replay" the logged changes and restore the file system to a consistent state.\n    \n### Benefits of Journaling:\n- Data Integrity: Reduces the risk of file system corruption by ensuring that incomplete or interrupted operations can be recovered.\n- Faster Recovery: In the event of a crash, the file system can be restored quickly by replaying the journal, avoiding lengthy file system checks (e.g., fsck in Unix-like systems).\n- Improved Reliability: Provides a more robust and reliable file system, especially for systems that require high availability.\n\n### Examples of Journaled File Systems:\n- ext3/ext4 (Linux)\n- NTFS (Windows)\n- HFS+ (macOS, though it has been largely replaced by APFS)\n- XFS (Linux)\n- Btrfs (Linux)\n- ReiserFS (Linux)\n\n### Types of Journaling:\n\n1. Metadata Journaling: Only metadata (e.g., file names, directory structures, permissions) is logged in the journal. This is faster but may not protect file contents in case of a crash.\n2. Full Data Journaling: Both metadata and file data are logged in the journal. This provides the highest level of data integrity but can be slower due to the additional write operations.\n3. Ordered Journaling: Metadata is journaled, but file data is written to the main file system first. This strikes a balance between performance and data integrity.\n\nJournaled file systems are widely used in modern operating systems because they provide a reliable way to manage data and recover from unexpected failures.	384:00:00	2025-04-05 08:34:01.810095+00	t
147	10	Register (—Ä–µ–≥–∏—Å—Ç—Ä)	#code	In computer architecture, a register is a small, high-speed storage location within the processor used to hold and manipulate data during program execution. Registers are a crucial part of the processor's hardware architecture and serve various functions in data processing.\n\n### Register key characteristics\n1. Data Storage: Registers temporarily store data being processed by the CPU. This includes numbers, memory addresses, calculation results, and other intermediate values.\n\n2. Fast Access: Registers are located directly within the processor, providing extremely fast access speeds. They operate at the hardware level and can be read from or written to with very low latency, making them significantly faster than accessing memory.\n\n3. Size: Registers have a fixed size, determined by the processor architecture. Register size is typically measured in bits, ranging from a few bits to several dozen bits.\n\n4. Quantity: The number of registers in a processor depends on the specific architecture. Different processor architectures have varying numbers of registers with different purposes and specific functions.\n\nRegisters play a vital role in instruction execution and data processing within the processor. They enable the processor to store, manipulate, and exchange data with memory and other devices in the computer system. Register operations occur at the hardware level and are performed directly by the processor, making them a critical element in ensuring system performance and functionality.	96:00:00	2025-03-16 14:22:39.122875+00	t
122	10	HTTP terminology	#code	#HTTP (Hypertext Transfer Protocol) is an application layer data transfer protocol used to exchange information between a _client_ and a _server_ on the Internet. It defines the format of messages exchanged between clients and servers and the rules for processing them.\n\nWeb #server is a piece of software that process HTTP requests from clients and send HTTP responses to them. They serve as the main component of web applications and enable communication between clients and servers.\n\nHTTP #request is a message sent by a client to a server to request a specific operation or resource. It contains the request method, the URL of the resource, headers, and possibly the body of the message.\n\nHTTP #response is a message sent by the server to the client in response to a request. It contains the response status, headers, and (possibly) the message body.\n\nHTTP #packets are units of data transmitted via the HTTP protocol. They consist of a header and a message body. The header contains metadata about the message, such as content type, encoding, length, and other attributes. The message body contains the actual data transmitted between the client and the server.\n\n#multiplexing, or #muxing, is¬†a way of sending multiple signals or streams of information over a communications link at the same time in the form of a single, complex signal.	192:00:00	2025-03-16 09:04:07.828148+00	t
356	10	Trie algorithm	#code	A Trie is a data structure that implements the associative array interface, allowing the storage of key-value pairs. In most cases, the keys are strings, but any data type that can be represented as a sequence of bytes can be used as keys.\n\nTries are particularly useful for implementing dictionaries, spell checkers, and T9-like systems, where it is necessary to quickly retrieve sets of keys with a given prefix.\n\n### How Does It Work?\nA Trie differs from ordinary n-ary trees in that keys are not stored in the nodes. Instead, single-character labels are stored in the nodes, and the key corresponding to a node is the path from the root of the tree to that node, or more precisely, the string composed of the labels of the nodes encountered on this path. In this case, the root of the tree obviously corresponds to an empty key.\n\nhttps://files.bool.dev/site/blog/f2b69526-8bdc-4fe0-9c49-955fc9727fd9/f421f71d-b0ef-4cd2-a756-7385d3d822ef.png	96:00:00	2025-03-15 14:47:43.537451+00	t
364	10	Common caching challanges	#code	While caching offers many benefits, it also introduces some challenges:\n\n- Maintaining Data Consistency: This is a major concern. When data is cached, you have multiple copies: the original in the database and the cached version. Ensuring that the cache accurately reflects the latest data from the source is crucial. If not handled properly, users might see stale or outdated information.\n\n- Cache Invalidation: This is closely related to consistency. Cache invalidation is the process of making sure that the cache is updated when the original data changes. Deciding _when_ and _how_ to invalidate the cache is a complex problem. If invalidation is too slow, stale data persists. If it's too aggressive, you might frequently invalidate the cache even when data hasn't changed, reducing caching effectiveness.\n\n- Cache Pollution (or ‚ÄúCache Thrashing‚Äù): If the cache becomes filled with irrelevant or infrequently accessed data, its effectiveness decreases. This is known as cache pollution. This can happen if the caching algorithm isn't well-suited to the access patterns of the application, or if a large volume of non-cacheable or one-time data is processed. Effectively managing cache size and using appropriate eviction policies are important to prevent this.	96:00:00	2025-03-15 10:42:12.512327+00	t
89	10	File descriptor	#code	A file descriptor is a low-level integer handle used by an operating system to identify an open file or other input/output resource, such as a socket or a pipe. In Unix-like operating systems, file descriptors are used to manage files and I/O streams.\n\nWhen a program opens a file, the operating system assigns a unique file descriptor to that file, which the program can then use to read from or write to the file. The standard file descriptors are:\n\n    0: Standard Input (stdin)\n    1: Standard Output (stdout)\n    2: Standard Error (stderr)\n\nFile descriptors are essential for performing I/O operations in a program, as they provide a way to reference files and other resources without needing to manage the actual file names or paths directly. They are typically used in system calls like open(), read(), write(), and close().	192:00:00	2025-03-22 08:17:06.418726+00	t
378	10	Dark (hidden) or A/B deployment	#code	Dark deployment, also known as a hidden deployment, is another variation of the canary strategy. The key difference between dark deployments and typical canary deployments is that dark deployments primarily focus on frontend changes and user-facing features, whereas canary deployments are often used for backend changes.\n\nAnother common term for dark deployments is A/B testing. Instead of making a new feature available to all users immediately, it is exposed to only a limited segment of the user base. Crucially, these users are typically unaware that they are part of a test group experiencing a new feature ‚Äì hence the term ‚Äúhidden deployment‚Äù.\n\n![](/Images/abtesting.png)\n\nFeature toggles (or feature flags) and other analytics tools are used to track how these users interact with the new feature. Metrics tracked might include user engagement, conversion rates, or whether users find the new user interface confusing. This data helps determine if the new feature is successful and user-friendly before a wider rollout.\n\nDark/A/B deployments are used to validate user interface changes, new features, or marketing campaigns by testing them on a small, often unaware, subset of users. Data-driven decisions on feature rollout are made based on user behavior metrics gathered during the test. This strategy is crucial for optimizing user experience and feature adoption in frontend development.\n\nhttps://blog.christianposta.com/images/abtesting.png	96:00:00	2025-03-15 09:03:03.581701+00	t
299	10	Cache performance	#code	You can assess the effectiveness of a cache using a variety of metrics. Here are some of the most fundamental and useful ones:\n\n- Memory Usage: This is the most basic metric. It tells you how much memory the cache is consuming.\n\n- Reads/Writes per Second (RPS): This measures the number of read and write operations the cache handles per unit of time. Typically, reads should significantly outnumber writes. If you see the opposite, it might indicate a problem with your cache setup.\n\n- Number of Items in Cache: Knowing the number of items, in addition to memory usage, can help you identify if you have any large entries in the cache.\n\n- Hit Rate: This is the percentage of times data is successfully retrieved from the cache. The closer it is to 100%, the better. This metric directly reflects how useful and efficient your cache is.\n\n- Expired Rate: This is the percentage of items removed from the cache due to exceeding their Time-To-Live (TTL). A high expired rate can point to performance issues, especially if many items expire at the same time.\n\n- Eviction Rate: This is the percentage of items evicted from the cache when it reaches its memory limit.\n\n### What to Cache\nData can be broadly categorized by how frequently it changes:\n- Frequently Changing Data: This data changes within seconds or minutes. Caching it is usually not worth it, though there can be exceptions.\n\n- Infrequently Changing Data: This data changes within minutes, hours, or days. This is where you'll most often ask yourself, "Should I cache this?"\n\n- Rarely or Never Changing Data: This data changes within weeks, months, or years. It's generally safe to cache this kind of data. \n\n> Never assume that any data will _never_ change. Sooner or later, it will. Always set a reasonable TTL for all data, even if you think it's static. Always!\n\n### Additional Considerations\n- Cache Invalidation: When data changes, you need a strategy to update the cache (invalidate it). This can be done through techniques like:\n    - Write-through: Update the cache and the main database simultaneously.\n    - Write-back: Update the cache first, then the database later.\n    - Cache expiration: Let the data expire naturally and get refreshed when needed.\n\n- Cache Size: Choosing the right cache size is crucial. Too small, and you'll have a lot of cache misses. Too large, and you're wasting memory.\n- Eviction Policies: When the cache is full, you need a strategy to decide which items to remove. Common policies include:\n    - LRU (Least Recently Used): Remove the item that was used least recently.\n    - FIFO (First-In, First-Out): Remove the item that was added first.\n    - Random: Remove a random item.	96:00:00	2025-03-14 15:49:55.453899+00	t
396	10	Security issues: SQL injections	#code	SQL injections are attacks where hackers insert malicious SQL code into database queries to gain unauthorized access or modify data. These attacks happen when an application doesn‚Äôt properly handle user input, allowing harmful SQL code to run alongside legitimate commands.\n\nHere are some best practices to prevent SQL injections:\n\n1. Use Parameterized Queries (Prepared Statements):  \n    Instead of embedding user input directly into SQL queries, use parameters that get replaced with actual values during execution. This method ensures that input is treated strictly as data, not as executable code.\n    \n2. Sanitize Input Data:  \n    If parameterized queries aren‚Äôt feasible, make sure all input data is properly escaped. This means converting special characters (like quotes) into a safe format so they can‚Äôt be mistaken for part of the SQL command.\n    \n3. Minimize Database Privileges:  \n    Grant your application only the minimum permissions it needs. For example, an application that only reads data shouldn‚Äôt have the authority to modify it.\n    \n4. Keep Your Database Updated:  \n    Regularly update your DBMS and apply security patches to fix known vulnerabilities.\n    \n5. Validate and Cleanse Input:  \n    Perform thorough validation on all incoming data and reject any requests containing invalid characters or structures.\n    \n6. Use Stored Procedures and Functions:  \n    By encapsulating SQL code in stored procedures or functions, you can reduce the risk of injections. This makes it easier to manage and secure the code that interacts directly with your database.\n    \n7. Monitor and Log Database Activity:  \n    Keep detailed logs of queries and access attempts. Monitoring can help you detect and respond to suspicious activities early.\n    \n8. Prefer Whitelisting Over Blacklisting:  \n    Define what input is allowed by using whitelists (acceptable characters and data formats) rather than relying solely on blacklists that specify forbidden elements.\n    \n\nThis approach not only secures your database from malicious injections but also improves overall application security by ensuring that all user input is treated safely.	96:00:00	2025-03-14 17:01:21.070504+00	t
382	10	K8s health checks	#code	Health checking in Kubernetes is a crucial process for monitoring and maintaining the health and availability of containers and applications deployed in a Kubernetes cluster. Kubernetes health checks provide automated monitoring of container status and automatically restart or relocate containers if they become unhealthy, ensuring continuous application operation.\n\nKubernetes offers two primary types of health checks: Readiness Probes and Liveness Probes. Both types can be implemented using various methods, including checking network connectivity (TCP probes), sending HTTP requests to API endpoints (HTTP probes), or executing commands inside the container (Exec probes).\n\n### Readiness Probes\nReadiness probes determine if a container is ready to start accepting traffic and serving requests from other containers or services.\n - A readiness probe returns an HTTP status code (e.g., 200 OK) if the container is ready to receive traffic and a different code (e.g., 503 Service Unavailable) if it is not ready.\n - If a readiness probe fails, Kubernetes does not consider the container ready to handle requests and will not route traffic to it. The container remains running, but is excluded from service load balancing.\n   \n### Liveness Probes\nLiveness probes are used to determine if a container is still running and healthy _during_ its execution.\n - Unlike readiness probes, liveness probes aim to recover a container if it enters a broken or unhealthy state, such as when an application hangs or stops responding to requests.\n - If a liveness probe fails, Kubernetes will restart the container. This can help recover from transient issues within the application or container environment.\n\n### Common Parameters\nBoth readiness and liveness probes can be configured with various parameters, such as:\n- periodSeconds: How often to perform the probe.\n- timeoutSeconds: How long to wait for a response from the probe.\n- successThreshold: Number of consecutive successful probes after failure to consider healthy.\n- failureThreshold: Number of consecutive failed probes before considering unhealthy.\n- initialDelaySeconds: Delay before the first probe is executed after container start.\n\n### Startup Probes\nFurthermore, Kubernetes supports startup probes, which are used to handle the case where applications take a long time to start up. Startup probes can prevent liveness and readiness probes from failing prematurely during the initial application startup phase.	96:00:00	2025-03-17 11:58:37.603525+00	t
376	10	Blue-green deployment	#code	Blue-green deployment is a strategy that involves running two identical environments: one environment is live, serving production traffic (this is typically referred to as "green"), and a second, identical environment (referred to as "blue") is prepared for the new release. Sometimes this approach is also known as red/black deployment.\n\nWhile the live "green" environment continues to operate, the new version of the application is deployed to the "blue" environment. Once the deployment to "blue" is complete, it undergoes testing, often by a QA team using automated tests and direct port access, or via a dedicated testing service. During this testing phase, the "green" environment remains live and stable for end-users, ensuring no disruption.\n\nAfter successful testing and release approval of the "blue" environment, the live traffic is switched from the "green" environment to the "blue" environment. The "green" environment, now running the old version, can then be idled for rollback purposes, or immediately decommissioned and updated for the next release cycle.\n\nBlue-green deployment provides a near-instant rollback capability by keeping the old environment readily available. It minimizes downtime during updates as the switchover can be very fast. However, it requires double the infrastructure resources as you need to maintain two production-like environments.\n\nhttps://dz2cdn1.dzone.com/storage/temp/10286821-blue-green-1.jpg	96:00:00	2025-03-14 14:19:44.523408+00	t
332	10	Logstash	#code	Logstash is a lightweight, open-source, server-side data processing pipeline that allows you to collect data from various sources, transform it on the fly, and send it to your desired destination. It is most often used as a data pipeline for #Elasticsearch. \n\nBecause of its tight integration with Elasticsearch, powerful log processing capabilities, and over 200 prebuilt open-source plugins that can help you easily index your data, Logstash is a popular choice for loading data into Elasticsearch.\n\n### Benefits\n1. Easily load unstructured data:  \n   Logstash allows you to easily ingest unstructured data from various data sources including system logs, website logs, and application server logs.¬†\n2. Prebuilt filters:   \n   Logstash offers prebuilt filters, so you can readily transform common data types, index them in Elasticsearch, and start querying without having to build custom data transformation pipelines.\n3. Flexible plugin architecture:   \n   With over 200 plugins already available on GitHub, it is likely that someone has already built the plugin that you need to customize your data pipeline. But if one is not available that suits your requirements, you can easily create one yourself.	96:00:00	2025-03-16 11:15:41.514716+00	t
374	10	Jaeger	#code	Jaeger is an open-source tracing tool for monitoring and troubleshooting distributed microservices. Inspired by projects like Dapper and OpenCensus, it was originally built on the OpenTracing standard and now follows OpenTelemetry practices. \n\nJaeger collects detailed trace data as requests move through various services, capturing timing details, function calls, and inter-service communications. Its user-friendly web interface displays this information through visualizations such as tree views and Gantt charts, making it easier to identify execution paths, performance bottlenecks, and service dependencies.\n\nBeyond visualization, Jaeger aggregates and analyzes trace data to pinpoint latency issues and optimize overall application performance. To use Jaeger, applications must be instrumented with client libraries (available for languages like Go, Java, Python, and Node.js) that send data to its collector, while the UI is accessed via http://localhost:16686. \n\nIn production, it can be configured with external storage backends like Elasticsearch or Cassandra to ensure scalable, persistent trace storage.\n\nThe quickest way to get started with Jaeger is using Docker. The command below launches an ‚Äúall-in-one‚Äù Jaeger instance, which includes all Jaeger components (agent, collector, query, and UI) in a single container, suitable for development and testing:\n    \n```bash\ndocker run -d --name jaeger \\\n-e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \\\n-p 5775:5775/udp \\\n-p 6831:6831/udp \\\n-p 6832:6832/udp \\\n-p 5778:5778 \\\n-p 16686:16686 \\\n-p 14268:14268 \\\n-p 9411:9411 \\\njaegertracing/all-in-one:latest\n```	96:00:00	2025-03-15 05:49:20.568927+00	t
371	10	Key metrics to monitor	#code	When monitoring microservices, it's important to track various types of metrics to ensure their health, performance, and business impact. Here's a breakdown of key metric categories:\n\n- Performance Metrics:\n    - CPU Load: High CPU usage might indicate inefficient code or overload.\n    - Memory Usage: Increasing memory usage over time without corresponding increases in load can point to memory leaks.\n    - Disk Space: Low disk space can lead to service disruptions.\n    - Network Activity: High network traffic might indicate security issues or unexpected communication patterns.\n        \n- Availability and Status Metrics:\n    - HTTP Status Codes: Error status codes (4xx, 5xx) indicate problems that need investigation.\n    - Request Duration (Latency): High latency can negatively impact user experience.\n        \n- Logging Metrics:\n    - Event Logging: Logs provide detailed information about application behavior and errors. Centralized logging systems are crucial for microservices architectures.\n- Resource Utilization Metrics:\n    - Number of Requests Handled (Throughput): Throughput metrics are key for capacity planning.\n    - Cache Usage: Efficient caching improves performance and reduces load on backend systems.\n        \n- Alerting System Metrics:\n    - Errors and Failures: Error rates should be closely monitored.\n    - Threshold Values: Threshold-based alerting ensures timely notifications of potential problems.\n        \n- Business Metrics:\n    - Business KPIs (Key Performance Indicators): Aligning technical metrics with business metrics helps demonstrate IT value.\n- Services and Dependencies Metrics:\n    - External Dependency Monitoring: If microservices interact with external services (databases, third-party APIs, etc.), it's important to track the availability and performance of these dependencies. Dependency failures can cascade and impact microservice performance.\n   \n- Security and Confidentiality Metrics:\n    - Security Monitoring and Data Leak Detection: Monitoring security aspects and detecting unauthorized access to microservices is crucial for protecting sensitive data and maintaining security.	96:00:00	2025-03-20 11:00:38.342781+00	t
361	10	Caching	#code	> Caching is the process of storing data in a high-speed storage location (the ‚Äúcache‚Äù) to accelerate future access to that same data. Think of it like creating a shortcut for frequently used information.\n\nCaching addresses a range of performance and efficiency issues across different layers of a system. It primarily helps by:\n\n- Speeding up data access:  \n  This is the most direct benefit. Retrieving data from a cache is significantly faster than fetching it from the original, slower source (like a database, file system, or remote service).\n  \n- Reducing database load:  \n  By serving frequently requested data from the cache, you drastically reduce the number of queries hitting your main database. This allows your database to operate more efficiently and handle more complex tasks.\n\n- Decreasing server load:  \n  Less database activity, fewer requests to external services, and faster content delivery all contribute to lower overall server processing and resource utilization. This means servers can handle more users and requests without performance degradation.\n  \n- Minimizing network traffic:  \n  Caching reduces the need to repeatedly transmit the same data over a network. For example, browser caching prevents downloading the same images or scripts every time you visit a webpage. This saves bandwidth and improves network responsiveness.\n\n- Lowering external service dependency:  \n  If your application relies on external services (APIs, third-party data), caching responses from these services reduces the number of requests made to them. This is especially important for rate-limited or paid services, and it improves resilience if the external service becomes temporarily unavailable.\n  \n- Improving file system performance:  \n  Caching frequently accessed files in memory avoids repeated disk reads, which are much slower. This speeds up applications that heavily rely on file I/O.	96:00:00	2025-03-18 17:19:23.550959+00	t
362	10	Caching types	#code	Caching can be implemented at various levels in a system:\n\n### In-Memory Caching\n  This uses the computer's RAM (Random Access Memory) for extremely fast data access. Common in-memory caching systems include:\n    - Redis: Often described as an "in-memory data structure store," Redis is versatile and supports various data types beyond simple key-value pairs, making it suitable for more complex caching scenarios.\n    - Memcached: A simpler, purely in-memory key-value store designed specifically for caching.\n        \n### File System Caching\n  Here, cached data is stored on the local file system (hard drive or SSD). While slower than in-memory caching, it's more persistent and can handle larger datasets.\n    - Nginx: This popular web server can act as a reverse proxy and cache static content (images, CSS, JavaScript files) on the file system, serving them directly without needing to involve the backend application for every request.\n        \n### Browser Caching\n  Web browsers are designed to cache web page resources (HTML, CSS, JavaScript, images) locally on the user's computer. This is controlled by HTTP headers sent by the web server. These headers instruct the browser on how long to cache resources and under what conditions to re-validate them.	96:00:00	2025-03-20 03:52:49.325709+00	t
366	10	Health checks	#code	> Health checks are procedures used to verify the operational status of a service. Essentially, they are automated checks to ensure a service is running correctly.\n\nHealth checks verify service operational status, crucial for fault tolerance and high availability. They detect service crashes and unresponsiveness, triggering restarts to ensure uptime. \n\nHealth checking tools include \n- Docker, \n- Kubernetes (de-facto standard with built-in robust health checks), \n- and Docker Compose. \n\nKubernetes is a powerful container orchestration system. It's designed to manage and scale containerized applications across a cluster of machines. Kubernetes has built-in, sophisticated health check mechanisms and is considered a de-facto standard in the industry for managing containerized applications. \n\nIts health check features are a core part of its ability to ensure application resilience. If a Kubernetes health check fails for a container, Kubernetes will automatically attempt to restart that container, ensuring the application remains healthy and available.	96:00:00	2025-03-15 15:25:33.035865+00	t
249	10	Why use databases	#code	Databases play a crucial role in modern information systems and applications for several reasons:\n\n1. Efficient Data Storage and Organization:  \n    Databases offer a powerful method for storing and structuring data. By breaking information into tables, records, and fields, they simplify the organization and retrieval of data.\n    \n2. Rapid Data Access:  \n    With built-in indexing and query optimization, databases enable fast retrieval of information even from vast datasets.\n    \n3. Multi-User Access:  \n    Modern databases support concurrent access, allowing multiple users to read and modify data simultaneously without conflict.\n    \n4. Data Integrity:  \n    They provide robust mechanisms to ensure data consistency, including uniqueness checks, integrity constraints, and support for transactions that guarantee reliable operations.\n    \n5. Security:  \n    Databases incorporate authentication and authorization measures to ensure secure data access. Additionally, many offer encryption options to protect sensitive information.\n    \n6. Backup and Recovery:  \n    With features for creating backups and restoring data after failures, databases help safeguard against data loss.\n    \n7. Scalability:  \n    Many current database systems are designed to scale, allowing both the volume of data and system performance to grow in tandem with increased demand.\n    \n8. Data Management:  \n    They include comprehensive tools for managing data‚Äîwhether it‚Äôs adding, updating, deleting, or searching records.\n    \n\nWhile plain files might be adequate for storing small amounts of data, they generally lack these advanced features. In complex and large-scale systems where efficient data management, robust security, consistency, and scalability are critical, databases become an indispensable solution.	96:00:00	2025-03-18 05:37:12.156855+00	t
309	10	Query profiling tools	#sql	1. pg_stat_statements (for PostgreSQL):  \n   This is a built-in module for PostgreSQL that tracks and analyzes executed SQL queries. It provides information about the number of times a query has been executed, the execution time, indexes used, and other metrics. You can use SQL queries to retrieve statistics from this table.\n\n2. EXPLAIN (for PostgreSQL and other DBMS):  \n   The EXPLAIN command allows you to analyze the execution plan of an SQL query. It shows how the database management system (DBMS) plans to execute the query, which indexes will be used, and what operations will be performed.\n\n3. pgBadger (for PostgreSQL):  \n   This is a PostgreSQL log analysis utility that generates reports and graphs based on the logs of executed queries. It helps identify slow queries and performance issues.\n\n4. MySQL Enterprise Monitor (for MySQL):  \n   This commercial tool provides monitoring and query profiling for MySQL. It displays query execution statistics and helps identify performance bottlenecks.\n\n5. Microsoft SQL Server Profiler (for SQL Server):  \n   This tool from Microsoft is designed for monitoring and profiling queries in SQL Server. It allows you to track query execution in real-time and analyze them. Note that SQL Server Profiler is deprecated and it is recommended to use Extended Events instead.\n\n6. Query Profiler (for Oracle Database):  \n   Oracle Database includes a built-in Query Profiler, which allows you to analyze and optimize SQL queries. It provides information about execution time, indexes used, and statistics.\n\n7. New Relic (for various DBMS):  \n   New Relic is a cloud-based performance monitoring solution that can be integrated with various DBMSs. It offers a wide range of tools for query analysis and profiling.\n\n8. Datadog (for various DBMS):  \n   Datadog is another cloud-based performance monitoring service that can be integrated with different databases. It provides detailed metrics on query execution.\n\n9. Percona Toolkit (for various DBMS):  \n   Percona Toolkit is a collection of tools for database administration and optimization, including tools for query profiling.\n\n10. AppOptics (for various DBMS):  \n    AppOptics is yet another cloud-based monitoring service that provides tools for analyzing and profiling SQL queries.	96:00:00	2025-04-20 19:39:52.58965+00	t
308	10	Query optimization	#sql	Optimizing your queries is a key part of working with databases‚Äîit boosts both performance and efficiency in your application. Here are several strategies to enhance your SQL queries:\n\n- Indexing:  \n    Create appropriate indexes on columns used in WHERE clauses and JOIN conditions. Proper indexes can dramatically speed up record retrieval.\n    \n- Query Refinement:  \n    Review your SQL statements to ensure they leverage indexes effectively and retrieve only the necessary data. Avoid using the LIKE operator with leading wildcards since that can reduce performance.\n    \n- Limiting Results:  \n    Use the LIMIT keyword (or your DBMS‚Äôs equivalent) to restrict the number of records returned. This is especially useful for queries that might otherwise return a massive data set.\n    \n- Caching Results:  \n    Consider caching the output of frequent queries in memory or using a dedicated caching system like Redis or Memcached. This helps reduce the overhead of executing the same query multiple times.\n    \n- Breaking Down Complex Queries:  \n    For queries that combine data from multiple tables, think about splitting them into several simpler queries executed one after the other.\n    \n- Utilizing Views:  \n    Create indexed views for queries that are run often. This can speed up their execution by precomputing and storing complex results.\n    \n- Using Subqueries Wisely:  \n    Employ subqueries where it makes sense to improve clarity and efficiency. They can sometimes simplify the overall logic of your queries.\n    \n- Avoiding Functions in WHERE Clauses:  \n    Using functions (for example, WHERE YEAR(date_column) = 2023) can prevent the use of indexes. Instead, try to rewrite your conditions so that indexes can be utilized.\n    \n- Optimizing JOINs:  \n    When possible, use INNER JOINs rather than OUTER JOINs, and ensure that the columns used for joining tables are properly indexed.\n    \n- Leveraging Profiling Tools:  \n    Most database systems offer query profiling tools. Use these to identify slow-performing parts of your queries and target them for optimization.\n    \n- Updating Database Statistics:  \n    Regularly update your database‚Äôs statistics to help the query optimizer make more accurate decisions about execution plans.\n    \n- Implementing Additional Caching Strategies:  \n    For data that doesn‚Äôt change often, caching query results can prevent the need to re-run complex queries every time they‚Äôre requested.\n    \n- Scaling Your Database:  \n    Consider horizontal scaling (adding more servers) or vertical scaling (upgrading server resources) to provide extra capacity for processing queries.\n    \n- Analyzing Execution Plans:  \n    Regularly review the execution plans generated by your DBMS to detect inefficient operations and identify potential improvements.\n    \n- Removing Unused Indexes:  \n    Periodically check for and remove indexes that are no longer in use, as they can add unnecessary overhead to database maintenance.\n    \n- Application-Level Caching:  \n    For some types of data, implementing caching on the application side can help reduce the load on your database.	192:00:00	2025-04-28 03:34:44.664712+00	f
343	10	Remove Duplicates From Sorted List II\n\nGiven the head of a sorted linked list, delete all nodes that have duplicate numbers, leaving only distinct numbers from the original list. Return the linked list sorted as well.	#leetcode	/**\n * Definition for singly-linked list.\n * type ListNode struct {\n *     Val int\n *     Next *ListNode\n * }\n */\nfunc deleteDuplicates(head *ListNode) *ListNode {\n  dummy := &ListNode{0, head}\n\n  prev := dummy\n  curr := head\n\n  for curr != nil {\n    for curr.Next != nil && curr.Val == curr.Next.Val {\n      curr = curr.Next\n    }\n\n    if prev.Next == curr {\n      prev = prev.Next\n    } else {\n      prev.Next = curr.Next\n    }\n\n    curr = curr.Next\n  }\n\n  return dummy.Next\n}	96:00:00	2025-04-30 12:35:13.733401+00	f
79	10	ZigZag Conversion\n\nThe string "PAYPALISHIRING" is written in a zigzag pattern on a given number of rows like this: (you may want to display this pattern in a fixed font for better legibility)\n\nP   A   H   N\nA P L S I I G\nY   I   R\n\nAnd then read line by line: "PAHNAPLSIIGYIR"	#leetcode	func convert(s string, numRows int) string {\n    if numRows == 1 || numRows >= len(s) {\n        return s\n    }\n\n    mtx := make([][]rune, numRows)\n    for i := 0; i < numRows; i++ {\n        mtx[i] = make([]rune, 0)\n    }\n\n    row, d := 0, 1\n    for _, ch := range s {\n        mtx[row] = append(mtx[row], ch)\n        if row == 0 {\n            d = 1\n        } else if row == numRows-1 {\n            d = -1\n        }\n        row += d\n    }\n\n    var str strings.Builder\n    for _, row := range mtx {\n        str.WriteString(string(row))\n    }\n\n    return str.String()\n}	192:00:00	2025-04-30 03:42:12.442539+00	f
26	10	Situational awareness check	#mindfulness	Breathing, emotions, scope, inner dialogue.	24:00:00	2025-04-12 06:31:14.755342+00	t
415	10	Symmetric Tree\n\nGiven the root of a binary tree, check whether it is a mirror of itself (i.e., symmetric around its center).	#leetcode	func isSymmetric(root *TreeNode) bool {\n    if root == nil {\n        return true\n    }\n\n    return traverse(root.Left, root.Right)\n}\n\nfunc traverse(left, right *TreeNode) bool {\n    if left == nil || right == nil {\n        return left == right\n    }\n\n    if left.Val != right.Val {\n        return false\n    }\n\n    return traverse(left.Left, right.Right) && traverse(left.Right, right.Left)\n}	384:00:00	2025-05-11 15:34:27.514883+00	f
70	10	Longest common prefix	#leetcode	func longestCommonPrefix(strs []string) string {\n    if len(strs) == 0 {\n        return ""\n    }\n\n    if len(strs) == 1 {\n        return strs[0]\n    }\n\n    ref := strs[0]\n    prefix := ""\n\n    for i := 1; i <= len(ref); i++ {\n        for j := 1; j < len(strs); j++ {\n            word := strs[j]\n\n            if len(word) < i {\n                return prefix\n            }\n\n            if word[:i] != ref[:i] {\n                return prefix\n            }\n        }\n        prefix = ref[:i]\n    }\n\n    return prefix\n}	384:00:00	2025-05-08 14:53:26.217724+00	f
135	10	Longest substring without repeating characters\n\nGiven a string s, find the length of the longest substring without repeating characters.	#leetcode	func lengthOfLongestSubstring(s string) int {\n    if len(s) < 2 {\n        return len(s)\n    }    \n\n    mp := make(map[rune]int)\n    maxSub := 0\n    seen := -1\n\n    for i, ch := range s {\n        if lastSeen, ok := mp[ch]; ok {\n            seen = max(lastSeen, seen)\n        }\n\n        mp[ch] = i\n        maxSub = max(maxSub, i-seen)\n    }\n\n    return maxSub\n}	384:00:00	2025-04-29 04:07:37.527671+00	f
341	10	Reverse Linked List II\n\nGiven the head of a singly linked list and two integers left and right where left <= right, reverse the nodes of the list from position left to position right, and return the reversed list.	#leetcode	/**\n * Definition for singly-linked list.\n * type ListNode struct {\n *     Val int\n *     Next *ListNode\n * }\n */\nfunc reverseBetween(head *ListNode, left int, right int) *ListNode {\n    if head == nil || left == right {\n        return head\n    }\n\n    dummy := &ListNode{Val: 0, Next: head}\n    prev := dummy\n\n    for i := 0; i < left-1; i++ {\n        prev = prev.Next\n    }\n\n    curr := prev.Next\n    for i := 0; i < right-left; i++ {\n        next := curr.Next\n        curr.Next = next.Next\n        next.Next = prev.Next\n        prev.Next = next\n    }\n\n    return dummy.Next\n}	48:00:00	2025-04-27 16:53:27.212501+00	f
310	10	Minimum Number of Arrows to Burst Balloons\n\nThere are some spherical balloons taped onto a flat wall that represents the XY-plane. The balloons are represented as a 2D integer array points where points[i] = [xstart, xend] denotes a balloon whose horizontal diameter stretches between xstart and xend. You do not know the exact y-coordinates of the balloons.\n\nArrows can be shot up directly vertically (in the positive y-direction) from different points along the x-axis. A balloon with xstart and xend is burst by an arrow shot at x if xstart <= x <= xend. There is no limit to the number of arrows that can be shot. A shot arrow keeps traveling up infinitely, bursting any balloons in its path.\n\nGiven the array points, return the minimum number of arrows that must be shot to burst all balloons.	#leetcode	func cmpFunc(a, b []int) int {\n    // sorting by the second coordinate first\n    value := cmp.Compare(a[1], b[1])\n    if value == 0 {\n        return cmp.Compare(a[0], b[0])\n    }\n    return value\n}\n\nfunc findMinArrowShots(points [][]int) int {\n    slices.SortFunc(points, cmpFunc) \n\n    pos, res := points[0][1], 1\n    for _, pnt := range points {\n        if pnt[0] <= pos {\n            continue\n        }\n        res++\n        pos = pnt[1]\n    }\n\n    return res\n}	192:00:00	2025-05-03 14:19:31.284904+00	f
416	10	Construct Binary Tree from Preorder and Inorder Traversal\n\nGiven two integer arrays preorder and inorder where preorder is the preorder traversal of a binary tree and inorder is the inorder traversal of the same tree, construct and return the binary tree.	#leetcode	func buildTree(preorder []int, inorder []int) *TreeNode {\n    if len(preorder) == 0 {\n        return nil\n    } \n\n    idx := slices.Index(inorder, preorder[0])\n    return &TreeNode {\n        Val: preorder[0],\n        Left: buildTree(preorder[1:idx+1], inorder[:idx]),\n        Right: buildTree(preorder[idx+1:], inorder[idx+1:]),\n    }\n}	96:00:00	2025-04-30 19:08:37.016182+00	f
338	10	Kafka producer	#kafka	A Producer is usually a service that writes data directly to Apache Kafka. The Producer selects a topic‚Äîa specific channel for related messages‚Äîand begins writing information to it. For example, an advertisement service can act as a Producer. In that case, it would send events such as ‚Äúad created,‚Äù ‚Äúad updated,‚Äù ‚Äúad deleted,‚Äù etc., to corresponding topics, with each event represented as a key-value pair.\n\nBy default, if an event is sent without a key, it is distributed across the topic‚Äôs partitions in a round-robin manner (which may disrupt ordering). If a key is provided, the event is routed using a MurmurHash algorithm, ensuring that the order is preserved within that particular partition.\n\nIt‚Äôs important to note that Kafka guarantees the order of events only within a single partition. In many cases, however, this is not an issue. For instance, one can ensure that all updates for the same ad are sent to the same partition‚Äîthus maintaining the proper sequence of changes. Alternatively, a sequence number can be included in one of the event fields to preserve order.	192:00:00	2025-05-01 05:47:38.959877+00	f
391	10	Design a data structure that follows the constraints of a Least Recently Used (LRU) cache.\n\nImplement the LRUCache class:\n\n1. LRUCache(int capacity) Initialize the LRU cache with positive size capacity.\n\n2. int get(int key) Return the value of the key if the key exists, otherwise return -1.\n\n3. void put(int key, int value) Update the value of the key if the key exists. Otherwise, add the key-value pair to the cache. If the number of keys exceeds the capacity from this operation, evict the least recently used key.\n\nThe functions get and put must each run in O(1) average time complexity.	#leetcode	type LRUCache struct {\n  cap   int\n  cache map[int]*Node\n  head  *Node\n  tail  *Node\n}\n\ntype Node struct {\n  key  int\n  val  int\n  next *Node\n  prev *Node\n}\n\nfunc Constructor(capacity int) LRUCache {\n  head := new(Node)\n  tail := new(Node)\n  head.next = tail\n  tail.prev = head\n\n  return LRUCache{\n    cap:   capacity,\n    cache: make(map[int]*Node),\n    head:  head,\n    tail:  tail,\n  }\n}\n\nfunc (this *LRUCache) Get(key int) int {\n  if node, ok := this.cache[key]; ok {\n    this.remove(node)\n    this.append(node)\n    return node.val\n  }\n  return -1\n}\n\nfunc (this *LRUCache) Put(key int, value int) {\n  if node, ok := this.cache[key]; ok {\n    node.val = value\n    this.remove(node)\n    this.append(node)\n    return\n  }\n\n  if len(this.cache) == this.cap {\n    lru := this.head.next\n    this.remove(lru)\n    delete(this.cache, lru.key)\n  }\n\n  node := &Node{key: key, val: value}\n  this.cache[key] = node\n  this.append(node)\n}\n\nfunc (this *LRUCache) remove(node *Node) {\n  node.prev.next = node.next\n  node.next.prev = node.prev\n}\n\nfunc (this *LRUCache) append(node *Node) {\n  prev := this.tail.prev\n  this.tail.prev = node\n  node.next = this.tail\n  prev.next = node\n  node.prev = prev\n}	96:00:00	2025-04-27 18:23:17.276038+00	f
\.


--
-- Data for Name: users; Type: TABLE DATA; Schema: data; Owner: user
--

COPY data.users (id, telegram_id, chat_id, is_running, location, window_floor, window_ceil, is_deleted) FROM stdin;
2	1984736385	1984736385	t	Europe/Moscow	09:00:00	21:00:00	f
3	266908482	266908482	t	Europe/Moscow	08:00:00	22:00:00	f
1	138130432	138130432	t	Europe/Moscow	06:00:00	21:00:00	f
4	1074424311	1074424311	t	Europe/Moscow	09:00:00	21:00:00	f
5	520264109	520264109	t	Asia/Yekaterinburg	08:00:00	21:00:00	f
6	155080573	155080573	t	Europe/Moscow	09:00:00	22:00:00	f
7	857587199	857587199	t	Europe/Moscow	10:30:00	22:30:00	f
8	1406248325	1406248325	t	Europe/Moscow	08:00:00	22:00:00	f
9	982310189	982310189	t	Europe/Moscow	10:00:00	21:00:00	f
10	1864317949	1864317949	t	Europe/Moscow	06:00:00	21:00:00	f
11	471364601	471364601	t	Europe/Moscow	08:00:00	22:00:00	f
\.


--
-- Data for Name: goose_db_version; Type: TABLE DATA; Schema: public; Owner: user
--

COPY public.goose_db_version (id, version_id, is_applied, tstamp) FROM stdin;
1	0	t	2025-01-26 05:25:10.340573
2	1	t	2025-01-26 05:25:10.43714
3	2	t	2025-01-26 05:25:10.441858
4	3	t	2025-01-26 05:25:10.45836
\.


--
-- Name: reminders_id_seq; Type: SEQUENCE SET; Schema: data; Owner: user
--

SELECT pg_catalog.setval('data.reminders_id_seq', 510, true);


--
-- Name: users_id_seq; Type: SEQUENCE SET; Schema: data; Owner: user
--

SELECT pg_catalog.setval('data.users_id_seq', 11, true);


--
-- Name: goose_db_version_id_seq; Type: SEQUENCE SET; Schema: public; Owner: user
--

SELECT pg_catalog.setval('public.goose_db_version_id_seq', 4, true);


--
-- Name: reminders reminders_pkey; Type: CONSTRAINT; Schema: data; Owner: user
--

ALTER TABLE ONLY data.reminders
    ADD CONSTRAINT reminders_pkey PRIMARY KEY (id);


--
-- Name: users users_pkey; Type: CONSTRAINT; Schema: data; Owner: user
--

ALTER TABLE ONLY data.users
    ADD CONSTRAINT users_pkey PRIMARY KEY (id);


--
-- Name: goose_db_version goose_db_version_pkey; Type: CONSTRAINT; Schema: public; Owner: user
--

ALTER TABLE ONLY public.goose_db_version
    ADD CONSTRAINT goose_db_version_pkey PRIMARY KEY (id);


--
-- Name: reminders reminders_user_id_fkey; Type: FK CONSTRAINT; Schema: data; Owner: user
--

ALTER TABLE ONLY data.reminders
    ADD CONSTRAINT reminders_user_id_fkey FOREIGN KEY (user_id) REFERENCES data.users(id);


--
-- PostgreSQL database dump complete
--

